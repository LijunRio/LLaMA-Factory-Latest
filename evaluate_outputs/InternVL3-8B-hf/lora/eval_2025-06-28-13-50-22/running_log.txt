[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file vocab.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file merges.txt
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file tokenizer.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file added_tokens.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file special_tokens_map.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file tokenizer_config.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file chat_template.jinja
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2299 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-06-28 13:59:09] processing_utils.py:928 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|2025-06-28 13:59:09] image_processing_base.py:378 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|2025-06-28 13:59:09] image_processing_base.py:433 >> Image processor GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file vocab.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file merges.txt
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file tokenizer.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file added_tokens.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file special_tokens_map.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file tokenizer_config.json
[INFO|2025-06-28 13:59:09] tokenization_utils_base.py:2021 >> loading file chat_template.jinja
[INFO|2025-06-28 13:59:10] tokenization_utils_base.py:2299 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|2025-06-28 13:59:10] logging.py:328 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|2025-06-28 13:59:10] video_processing_utils.py:627 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|2025-06-28 13:59:10] configuration_utils.py:696 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|2025-06-28 13:59:10] configuration_utils.py:770 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|2025-06-28 13:59:10] video_processing_utils.py:627 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|2025-06-28 13:59:10] video_processing_utils.py:683 >> Video processor InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}

[INFO|2025-06-28 13:59:10] processing_utils.py:928 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|2025-06-28 13:59:10] processing_utils.py:990 >> Processor InternVLProcessor:
- image_processor: GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf', vocab_size=151643, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151666: AddedToken("</img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151667: AddedToken("<IMG_CONTEXT>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151668: AddedToken("<quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151669: AddedToken("</quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151670: AddedToken("<ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151671: AddedToken("</ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151672: AddedToken("<box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151673: AddedToken("</box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151674: AddedToken("<video>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}


{
  "image_seq_length": 256,
  "processor_class": "InternVLProcessor"
}

[INFO|2025-06-28 13:59:10] logging.py:143 >> Add <|im_end|> to stop words.
[INFO|2025-06-28 13:59:10] logging.py:143 >> Loading dataset ./0_my_dataset/coco/coco_test_format.mcml.json...
[INFO|2025-06-28 14:00:07] configuration_utils.py:696 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|2025-06-28 14:00:07] configuration_utils.py:770 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|2025-06-28 14:00:07] logging.py:143 >> KV cache is enabled for faster generation.
[INFO|2025-06-28 14:00:07] modeling_utils.py:1148 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/model.safetensors.index.json
[INFO|2025-06-28 14:00:07] modeling_utils.py:2241 >> Instantiating InternVLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|2025-06-28 14:00:07] configuration_utils.py:1135 >> Generate config GenerationConfig {}

[INFO|2025-06-28 14:00:08] modeling_utils.py:2241 >> Instantiating InternVLVisionModel model under default dtype torch.bfloat16.
[INFO|2025-06-28 14:00:08] modeling_utils.py:2241 >> Instantiating Qwen2Model model under default dtype torch.bfloat16.
[INFO|2025-06-28 14:00:15] modeling_utils.py:5131 >> All model checkpoint weights were used when initializing InternVLForConditionalGeneration.

[INFO|2025-06-28 14:00:15] modeling_utils.py:5139 >> All the weights of InternVLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLForConditionalGeneration for predictions without further training.
[INFO|2025-06-28 14:00:15] configuration_utils.py:1088 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/generation_config.json
[INFO|2025-06-28 14:00:15] configuration_utils.py:1135 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|2025-06-28 14:00:15] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-28 14:00:15] logging.py:143 >> all params: 7,944,373,760
[WARNING|2025-06-28 14:00:15] logging.py:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|2025-06-28 14:00:15] trainer.py:4327 >> 
***** Running Prediction *****
[INFO|2025-06-28 14:00:15] trainer.py:4329 >>   Num examples = 600
[INFO|2025-06-28 14:00:15] trainer.py:4332 >>   Batch size = 2
[INFO|2025-06-28 14:14:21] logging.py:143 >> Saving prediction results to saves/InternVL3-8B-hf/lora/eval_2025-06-28-13-50-22/generated_predictions.jsonl
