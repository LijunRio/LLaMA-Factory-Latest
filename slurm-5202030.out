Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Sun Jun 29 15:59:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:31:00.0 Off |                    0 |
| N/A   40C    P0              65W / 500W |      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
==============================================
Starting evaluation for model: InternVL3-8B
Model path: /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf
Template: intern_vl
Batch size: 8
==============================================
[2025-06-29 15:59:32,752] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 06-29 15:59:36 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-06-29 15:59:39] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,141 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,142 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 15:59:39,461 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:928] 2025-06-29 15:59:39,462 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|image_processing_base.py:378] 2025-06-29 15:59:39,463 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-29 15:59:39,470 >> Image processor GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 15:59:39,470 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 15:59:39,783 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-06-29 15:59:39,784 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-06-29 15:59:39,785 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-06-29 15:59:39,785 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-29 15:59:39,787 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|video_processing_utils.py:627] 2025-06-29 15:59:39,788 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-29 15:59:39,788 >> Video processor InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-06-29 15:59:39,788 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|processing_utils.py:990] 2025-06-29 15:59:40,172 >> Processor InternVLProcessor:
- image_processor: GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf', vocab_size=151643, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151666: AddedToken("</img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151667: AddedToken("<IMG_CONTEXT>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151668: AddedToken("<quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151669: AddedToken("</quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151670: AddedToken("<ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151671: AddedToken("</ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151672: AddedToken("<box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151673: AddedToken("</box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151674: AddedToken("<video>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}


{
  "image_seq_length": 256,
  "processor_class": "InternVLProcessor"
}

[INFO|2025-06-29 15:59:40] llamafactory.data.template:143 >> Using default system message: You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image..
[INFO|2025-06-29 15:59:40] llamafactory.data.template:143 >> Add <|im_end|> to stop words.
[INFO|2025-06-29 15:59:40] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/padchest_gt/padchest_input_data_qwen2_test_len_1285.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1285 examples [00:00, 24683.79 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 19/1285 [00:00<00:08, 154.65 examples/s]Converting format of dataset (num_proc=16):  68%|██████▊   | 875/1285 [00:00<00:00, 4604.89 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1285/1285 [00:00<00:00, 2898.35 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 80/1285 [01:17<19:28,  1.03 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 160/1285 [01:17<07:30,  2.50 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 241/1285 [01:18<03:52,  4.49 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 402/1285 [01:19<01:26, 10.21 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 483/1285 [01:19<00:56, 14.19 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 564/1285 [01:20<00:37, 19.14 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 644/1285 [01:20<00:24, 26.23 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 805/1285 [01:20<00:10, 46.69 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 885/1285 [01:20<00:06, 60.66 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 965/1285 [01:20<00:04, 78.01 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1125/1285 [01:21<00:01, 127.02 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1205/1285 [01:21<00:00, 151.04 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1285/1285 [01:21<00:00, 15.77 examples/s] 
[INFO|configuration_utils.py:696] 2025-06-29 16:01:03,595 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:01:03,598 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 11900, 16155, 11163, 311, 13382, 323, 23560, 6457, 2168, 6358, 13, 4615, 3476, 374, 311, 10542, 323, 94416, 11900, 5729, 5904, 315, 3151, 18808, 304, 15138, 1599, 29530, 5335, 13, 1446, 686, 387, 3897, 448, 264, 15138, 1599, 29530, 2168, 323, 264, 8457, 829, 13, 4615, 3383, 374, 311, 29257, 94416, 279, 9760, 5537, 1141, 8, 304, 279, 2168, 13, 151645, 198, 151644, 872, 198, 151665, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151666, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 81277, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image.<|im_end|>
<|im_start|>user
<img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>
Return bounding boxes of 'Pleural Thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 17, 22, 21, 345, 414, 220, 16, 19, 16, 345, 414, 220, 19, 23, 19, 345, 414, 220, 17, 16, 23, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 1153, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 20, 17, 345, 414, 220, 16, 17, 22, 345, 414, 220, 22, 21, 22, 345, 414, 220, 17, 18, 15, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      276,
      141,
      484,
      218
    ],
    "label": "Pleural Thickening"
  },
  {
    "bbox_2d": [
      552,
      127,
      767,
      230
    ],
    "label": "Pleural Thickening"
  }
]
</answer><|im_end|>

[INFO|2025-06-29 16:01:03] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1148] 2025-06-29 16:01:04,586 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-29 16:01:04,593 >> Instantiating InternVLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-29 16:01:04,598 >> Generate config GenerationConfig {}

[INFO|modeling_utils.py:2241] 2025-06-29 16:01:09,893 >> Instantiating InternVLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2241] 2025-06-29 16:01:10,097 >> Instantiating Qwen2Model model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:10,  5.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.16s/it]
[INFO|modeling_utils.py:5131] 2025-06-29 16:01:26,828 >> All model checkpoint weights were used when initializing InternVLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-06-29 16:01:26,828 >> All the weights of InternVLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-29 16:01:26,835 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-29 16:01:26,836 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|2025-06-29 16:01:26] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-29 16:01:26] llamafactory.model.loader:143 >> all params: 7,944,373,760
[WARNING|2025-06-29 16:01:26] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-06-29 16:01:26,953 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-06-29 16:01:26,953 >>   Num examples = 1285
[INFO|trainer.py:4332] 2025-06-29 16:01:26,953 >>   Batch size = 8
  0%|          | 0/161 [00:00<?, ?it/s]  1%|          | 2/161 [00:06<08:28,  3.20s/it]  2%|▏         | 3/161 [00:11<10:47,  4.10s/it]  2%|▏         | 4/161 [00:17<12:09,  4.65s/it]  3%|▎         | 5/161 [00:26<16:02,  6.17s/it]  4%|▎         | 6/161 [00:36<19:04,  7.38s/it]  4%|▍         | 7/161 [00:42<17:40,  6.89s/it]  5%|▍         | 8/161 [00:48<17:33,  6.89s/it]  6%|▌         | 9/161 [00:57<18:45,  7.40s/it]  6%|▌         | 10/161 [01:03<17:41,  7.03s/it]  7%|▋         | 11/161 [01:09<16:43,  6.69s/it]  7%|▋         | 12/161 [01:15<16:13,  6.53s/it]  8%|▊         | 13/161 [01:24<17:56,  7.28s/it]  9%|▊         | 14/161 [01:31<17:36,  7.19s/it]  9%|▉         | 15/161 [01:40<18:35,  7.64s/it] 10%|▉         | 16/161 [01:49<19:18,  7.99s/it] 11%|█         | 17/161 [02:03<23:28,  9.78s/it] 11%|█         | 18/161 [02:12<22:44,  9.54s/it] 12%|█▏        | 19/161 [02:21<22:15,  9.41s/it] 12%|█▏        | 20/161 [02:26<19:21,  8.24s/it] 13%|█▎        | 21/161 [02:33<18:00,  7.71s/it] 14%|█▎        | 22/161 [02:40<17:16,  7.46s/it] 14%|█▍        | 23/161 [02:46<16:45,  7.29s/it] 15%|█▍        | 24/161 [02:54<16:49,  7.37s/it] 16%|█▌        | 25/161 [03:01<16:29,  7.28s/it] 16%|█▌        | 26/161 [03:07<15:13,  6.77s/it] 17%|█▋        | 27/161 [03:14<15:13,  6.82s/it] 17%|█▋        | 28/161 [03:19<14:28,  6.53s/it] 18%|█▊        | 29/161 [03:26<14:12,  6.46s/it] 19%|█▊        | 30/161 [03:31<13:22,  6.13s/it] 19%|█▉        | 31/161 [03:38<13:53,  6.41s/it] 20%|█▉        | 32/161 [03:43<12:50,  5.97s/it] 20%|██        | 33/161 [03:49<12:21,  5.80s/it] 21%|██        | 34/161 [03:56<13:08,  6.21s/it] 22%|██▏       | 35/161 [04:03<13:58,  6.65s/it] 22%|██▏       | 36/161 [04:13<15:37,  7.50s/it] 23%|██▎       | 37/161 [04:18<14:16,  6.91s/it] 24%|██▎       | 38/161 [04:24<13:37,  6.65s/it] 24%|██▍       | 39/161 [04:33<14:28,  7.12s/it] 25%|██▍       | 40/161 [04:41<15:15,  7.56s/it] 25%|██▌       | 41/161 [04:47<14:12,  7.11s/it] 26%|██▌       | 42/161 [04:54<13:51,  6.99s/it] 27%|██▋       | 43/161 [05:01<13:38,  6.94s/it] 27%|██▋       | 44/161 [05:10<15:03,  7.73s/it] 28%|██▊       | 45/161 [05:16<13:46,  7.13s/it] 29%|██▊       | 46/161 [05:23<13:26,  7.02s/it] 29%|██▉       | 47/161 [05:28<12:26,  6.55s/it] 30%|██▉       | 48/161 [05:35<12:34,  6.68s/it] 30%|███       | 49/161 [05:42<12:27,  6.67s/it] 31%|███       | 50/161 [05:47<11:37,  6.28s/it] 32%|███▏      | 51/161 [05:55<12:11,  6.65s/it] 32%|███▏      | 52/161 [06:02<12:09,  6.69s/it] 33%|███▎      | 53/161 [06:14<15:06,  8.39s/it] 34%|███▎      | 54/161 [06:20<13:43,  7.70s/it] 34%|███▍      | 55/161 [06:27<13:04,  7.40s/it] 35%|███▍      | 56/161 [06:32<11:53,  6.79s/it] 35%|███▌      | 57/161 [06:38<11:02,  6.37s/it] 36%|███▌      | 58/161 [06:44<11:04,  6.45s/it] 37%|███▋      | 59/161 [06:50<10:27,  6.15s/it] 37%|███▋      | 60/161 [06:59<11:52,  7.05s/it] 38%|███▊      | 61/161 [07:06<11:41,  7.02s/it] 39%|███▊      | 62/161 [07:14<12:25,  7.53s/it] 39%|███▉      | 63/161 [07:20<11:30,  7.05s/it] 40%|███▉      | 64/161 [07:26<10:42,  6.62s/it] 40%|████      | 65/161 [07:32<10:17,  6.43s/it] 41%|████      | 66/161 [07:37<09:38,  6.09s/it] 42%|████▏     | 67/161 [07:47<11:19,  7.23s/it] 42%|████▏     | 68/161 [07:58<13:01,  8.40s/it] 43%|████▎     | 69/161 [08:06<12:30,  8.15s/it] 43%|████▎     | 70/161 [08:13<11:43,  7.73s/it] 44%|████▍     | 71/161 [08:19<10:56,  7.29s/it] 45%|████▍     | 72/161 [08:27<10:59,  7.41s/it] 45%|████▌     | 73/161 [08:35<11:12,  7.64s/it] 46%|████▌     | 74/161 [08:42<11:03,  7.63s/it] 47%|████▋     | 75/161 [08:49<10:18,  7.19s/it] 47%|████▋     | 76/161 [08:54<09:27,  6.68s/it] 48%|████▊     | 77/161 [09:05<11:04,  7.91s/it] 48%|████▊     | 78/161 [09:13<11:01,  7.97s/it] 49%|████▉     | 79/161 [09:22<11:17,  8.27s/it] 50%|████▉     | 80/161 [09:31<11:24,  8.44s/it] 50%|█████     | 81/161 [09:39<11:11,  8.40s/it] 51%|█████     | 82/161 [09:49<11:30,  8.74s/it] 52%|█████▏    | 83/161 [09:55<10:17,  7.91s/it] 52%|█████▏    | 84/161 [10:00<09:20,  7.28s/it] 53%|█████▎    | 85/161 [10:07<08:59,  7.10s/it] 53%|█████▎    | 86/161 [10:13<08:29,  6.79s/it] 54%|█████▍    | 87/161 [10:20<08:21,  6.77s/it] 55%|█████▍    | 88/161 [10:27<08:27,  6.95s/it] 55%|█████▌    | 89/161 [10:35<08:46,  7.31s/it] 56%|█████▌    | 90/161 [10:41<08:00,  6.76s/it] 57%|█████▋    | 91/161 [10:48<08:03,  6.91s/it] 57%|█████▋    | 92/161 [10:56<08:22,  7.28s/it] 58%|█████▊    | 93/161 [11:02<07:42,  6.80s/it] 58%|█████▊    | 94/161 [11:10<07:58,  7.14s/it] 59%|█████▉    | 95/161 [11:17<07:52,  7.16s/it] 60%|█████▉    | 96/161 [11:22<07:07,  6.58s/it] 60%|██████    | 97/161 [11:30<07:28,  7.00s/it] 61%|██████    | 98/161 [11:37<07:10,  6.83s/it] 61%|██████▏   | 99/161 [11:42<06:44,  6.52s/it] 62%|██████▏   | 100/161 [11:48<06:24,  6.30s/it] 63%|██████▎   | 101/161 [11:57<07:01,  7.03s/it] 63%|██████▎   | 102/161 [12:04<06:47,  6.91s/it] 64%|██████▍   | 103/161 [12:14<07:48,  8.08s/it] 65%|██████▍   | 104/161 [12:20<06:57,  7.32s/it] 65%|██████▌   | 105/161 [12:25<06:18,  6.77s/it] 66%|██████▌   | 106/161 [12:31<05:53,  6.42s/it] 66%|██████▋   | 107/161 [12:37<05:37,  6.25s/it] 67%|██████▋   | 108/161 [12:43<05:27,  6.18s/it] 68%|██████▊   | 109/161 [12:56<07:13,  8.34s/it] 68%|██████▊   | 110/161 [13:08<07:51,  9.24s/it] 69%|██████▉   | 111/161 [13:17<07:38,  9.17s/it] 70%|██████▉   | 112/161 [13:23<06:41,  8.18s/it] 70%|███████   | 113/161 [13:29<06:12,  7.76s/it] 71%|███████   | 114/161 [13:38<06:21,  8.11s/it] 71%|███████▏  | 115/161 [13:44<05:42,  7.44s/it] 72%|███████▏  | 116/161 [13:52<05:43,  7.64s/it] 73%|███████▎  | 117/161 [13:58<05:17,  7.22s/it] 73%|███████▎  | 118/161 [14:05<04:55,  6.88s/it] 74%|███████▍  | 119/161 [14:20<06:38,  9.49s/it] 75%|███████▍  | 120/161 [14:27<05:53,  8.61s/it] 75%|███████▌  | 121/161 [14:34<05:29,  8.24s/it] 76%|███████▌  | 122/161 [14:42<05:14,  8.07s/it] 76%|███████▋  | 123/161 [14:51<05:16,  8.33s/it] 77%|███████▋  | 124/161 [14:57<04:43,  7.65s/it] 78%|███████▊  | 125/161 [15:02<04:14,  7.06s/it] 78%|███████▊  | 126/161 [15:09<03:58,  6.82s/it] 79%|███████▉  | 127/161 [15:24<05:15,  9.29s/it] 80%|███████▉  | 128/161 [15:48<07:30, 13.65s/it] 80%|████████  | 129/161 [15:53<05:59, 11.25s/it] 81%|████████  | 130/161 [16:04<05:48, 11.25s/it] 81%|████████▏ | 131/161 [16:10<04:42,  9.42s/it] 82%|████████▏ | 132/161 [16:19<04:30,  9.33s/it] 83%|████████▎ | 133/161 [16:28<04:18,  9.24s/it] 83%|████████▎ | 134/161 [16:35<03:52,  8.60s/it] 84%|████████▍ | 135/161 [16:41<03:21,  7.74s/it] 84%|████████▍ | 136/161 [16:49<03:21,  8.08s/it] 85%|████████▌ | 137/161 [16:55<02:56,  7.35s/it] 86%|████████▌ | 138/161 [17:01<02:39,  6.95s/it] 86%|████████▋ | 139/161 [17:07<02:25,  6.59s/it] 87%|████████▋ | 140/161 [17:13<02:15,  6.43s/it] 88%|████████▊ | 141/161 [17:19<02:03,  6.17s/it] 88%|████████▊ | 142/161 [17:29<02:24,  7.61s/it] 89%|████████▉ | 143/161 [17:36<02:12,  7.35s/it] 89%|████████▉ | 144/161 [17:44<02:05,  7.36s/it] 90%|█████████ | 145/161 [17:50<01:53,  7.10s/it] 91%|█████████ | 146/161 [18:01<02:01,  8.10s/it] 91%|█████████▏| 147/161 [18:07<01:47,  7.70s/it] 92%|█████████▏| 148/161 [18:15<01:39,  7.66s/it] 93%|█████████▎| 149/161 [18:21<01:25,  7.12s/it] 93%|█████████▎| 150/161 [18:30<01:24,  7.72s/it] 94%|█████████▍| 151/161 [18:39<01:22,  8.21s/it] 94%|█████████▍| 152/161 [18:46<01:10,  7.86s/it] 95%|█████████▌| 153/161 [19:10<01:41, 12.72s/it] 96%|█████████▌| 154/161 [19:26<01:35, 13.64s/it] 96%|█████████▋| 155/161 [19:34<01:11, 11.90s/it] 97%|█████████▋| 156/161 [19:44<00:56, 11.24s/it] 98%|█████████▊| 157/161 [19:51<00:40, 10.19s/it] 98%|█████████▊| 158/161 [19:57<00:26,  8.73s/it] 99%|█████████▉| 159/161 [20:02<00:15,  7.84s/it] 99%|█████████▉| 160/161 [20:08<00:07,  7.22s/it]100%|██████████| 161/161 [20:13<00:00,  6.56s/it]Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.526 seconds.
Prefix dict has been built successfully.
100%|██████████| 161/161 [20:20<00:00,  7.58s/it]
***** predict metrics *****
  predict_bleu-4                 =    13.3145
  predict_model_preparation_time =     0.0128
  predict_rouge-1                =    22.8343
  predict_rouge-2                =     19.682
  predict_rouge-l                =    25.1596
  predict_runtime                = 0:20:34.88
  predict_samples_per_second     =      1.041
  predict_steps_per_second       =       0.13
[INFO|2025-06-29 16:22:01] llamafactory.train.sft.trainer:143 >> Saving prediction results to evaluate_outputs/results/padchest_gr_test/InternVL3-8B/generated_predictions.jsonl
✅ InternVL3-8B evaluation completed successfully

==============================================
Starting evaluation for model: Qwen2-VL-7B
Model path: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct
Template: qwen2_vl
Batch size: 8
==============================================
[2025-06-29 16:22:26,197] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 06-29 16:22:31 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-06-29 16:22:34] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,870 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,870 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,870 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,870 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,871 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,871 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:34,871 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:22:35,200 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-29 16:22:35,200 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-29 16:22:35,205 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-29 16:22:35,213 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:22:35,214 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:22:35,528 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-06-29 16:22:35,533 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-06-29 16:22:35,533 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-06-29 16:22:35,533 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:22:35,536 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 3584,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|video_processing_utils.py:627] 2025-06-29 16:22:35,538 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-29 16:22:35,538 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-29 16:22:36,053 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|2025-06-29 16:22:36] llamafactory.data.template:143 >> Using default system message: You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image..
[INFO|2025-06-29 16:22:36] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/padchest_gt/padchest_input_data_qwen2_test_len_1285.json...
Converting format of dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 81/1285 [00:00<00:02, 601.89 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1285/1285 [00:00<00:00, 4092.91 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 80/1285 [01:46<26:38,  1.33s/ examples]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 161/1285 [01:48<10:30,  1.78 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 242/1285 [01:48<05:17,  3.28 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 322/1285 [01:49<03:00,  5.35 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 403/1285 [01:50<01:51,  7.90 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 483/1285 [01:51<01:11, 11.29 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 644/1285 [01:52<00:30, 21.20 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 725/1285 [01:53<00:20, 27.43 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 885/1285 [01:53<00:09, 44.08 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 965/1285 [01:54<00:05, 55.62 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1045/1285 [01:54<00:03, 70.99 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1125/1285 [01:54<00:01, 90.02 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1205/1285 [01:54<00:00, 116.26 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1285/1285 [01:54<00:00, 11.20 examples/s] 
[INFO|configuration_utils.py:696] 2025-06-29 16:24:32,627 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:24:32,629 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 3584,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 11900, 16155, 11163, 311, 13382, 323, 23560, 6457, 2168, 6358, 13, 4615, 3476, 374, 311, 10542, 323, 94416, 11900, 5729, 5904, 315, 3151, 18808, 304, 15138, 1599, 29530, 5335, 13, 1446, 686, 387, 3897, 448, 264, 15138, 1599, 29530, 2168, 323, 264, 8457, 829, 13, 4615, 3383, 374, 311, 29257, 94416, 279, 9760, 5537, 1141, 8, 304, 279, 2168, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 81277, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pleural Thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 17, 22, 21, 345, 414, 220, 16, 19, 16, 345, 414, 220, 19, 23, 19, 345, 414, 220, 17, 16, 23, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 1153, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 20, 17, 345, 414, 220, 16, 17, 22, 345, 414, 220, 22, 21, 22, 345, 414, 220, 17, 18, 15, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      276,
      141,
      484,
      218
    ],
    "label": "Pleural Thickening"
  },
  {
    "bbox_2d": [
      552,
      127,
      767,
      230
    ],
    "label": "Pleural Thickening"
  }
]
</answer><|im_end|>

[INFO|2025-06-29 16:24:32] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1148] 2025-06-29 16:24:32,727 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-29 16:24:32,731 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-29 16:24:32,733 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2241] 2025-06-29 16:24:32,734 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2241] 2025-06-29 16:24:32,857 >> Instantiating Qwen2VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:15,  3.86s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:11,  3.68s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:10<00:07,  3.64s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:14<00:03,  3.61s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:15<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:15<00:00,  3.11s/it]
[INFO|modeling_utils.py:5131] 2025-06-29 16:24:48,477 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-06-29 16:24:48,477 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-29 16:24:48,482 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-29 16:24:48,482 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

[INFO|2025-06-29 16:24:48] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-29 16:24:48] llamafactory.model.loader:143 >> all params: 8,291,375,616
[WARNING|2025-06-29 16:24:48] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-06-29 16:24:48,547 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-06-29 16:24:48,547 >>   Num examples = 1285
[INFO|trainer.py:4332] 2025-06-29 16:24:48,547 >>   Batch size = 8
  0%|          | 0/161 [00:00<?, ?it/s]  1%|          | 2/161 [00:02<02:44,  1.03s/it]  2%|▏         | 3/161 [00:04<03:46,  1.43s/it]  2%|▏         | 4/161 [00:06<04:20,  1.66s/it]  3%|▎         | 5/161 [00:08<04:37,  1.78s/it]  4%|▎         | 6/161 [00:10<04:50,  1.87s/it]  4%|▍         | 7/161 [00:12<05:32,  2.16s/it]  5%|▍         | 8/161 [00:14<05:24,  2.12s/it]  6%|▌         | 9/161 [00:16<05:16,  2.08s/it]  6%|▌         | 10/161 [00:18<05:11,  2.06s/it]  7%|▋         | 11/161 [00:22<06:04,  2.43s/it]  7%|▋         | 12/161 [00:24<05:44,  2.31s/it]  8%|▊         | 13/161 [00:26<05:30,  2.23s/it]  9%|▊         | 14/161 [00:28<05:23,  2.20s/it]  9%|▉         | 15/161 [00:30<05:14,  2.15s/it] 10%|▉         | 16/161 [00:32<05:10,  2.14s/it] 11%|█         | 17/161 [00:34<05:03,  2.11s/it] 11%|█         | 18/161 [00:36<04:59,  2.09s/it] 12%|█▏        | 19/161 [00:39<05:30,  2.32s/it] 12%|█▏        | 20/161 [00:41<05:14,  2.23s/it] 13%|█▎        | 21/161 [00:43<05:04,  2.17s/it] 14%|█▎        | 22/161 [00:45<04:58,  2.15s/it] 14%|█▍        | 23/161 [00:47<04:53,  2.13s/it] 15%|█▍        | 24/161 [00:49<04:48,  2.11s/it] 16%|█▌        | 25/161 [00:51<04:45,  2.10s/it] 16%|█▌        | 26/161 [00:54<04:42,  2.09s/it] 17%|█▋        | 27/161 [00:56<05:09,  2.31s/it] 17%|█▋        | 28/161 [00:58<04:55,  2.22s/it] 18%|█▊        | 29/161 [01:00<04:47,  2.18s/it] 19%|█▊        | 30/161 [01:02<04:40,  2.14s/it] 19%|█▉        | 31/161 [01:04<04:31,  2.09s/it] 20%|█▉        | 32/161 [01:07<04:29,  2.09s/it] 20%|██        | 33/161 [01:09<04:27,  2.09s/it] 21%|██        | 34/161 [01:11<04:23,  2.08s/it] 22%|██▏       | 35/161 [01:13<04:19,  2.06s/it] 22%|██▏       | 36/161 [01:16<04:47,  2.30s/it] 23%|██▎       | 37/161 [01:18<04:36,  2.23s/it] 24%|██▎       | 38/161 [01:20<04:25,  2.16s/it] 24%|██▍       | 39/161 [01:22<04:17,  2.11s/it] 25%|██▍       | 40/161 [01:24<04:14,  2.10s/it] 25%|██▌       | 41/161 [01:26<04:12,  2.10s/it] 26%|██▌       | 42/161 [01:29<04:41,  2.37s/it] 27%|██▋       | 43/161 [01:31<04:26,  2.26s/it] 27%|██▋       | 44/161 [01:33<04:16,  2.19s/it] 28%|██▊       | 45/161 [01:35<04:11,  2.17s/it] 29%|██▊       | 46/161 [01:37<04:14,  2.21s/it] 29%|██▉       | 47/161 [01:39<04:06,  2.16s/it] 30%|██▉       | 48/161 [01:42<04:28,  2.37s/it] 30%|███       | 49/161 [01:44<04:13,  2.26s/it] 31%|███       | 50/161 [01:46<04:03,  2.20s/it] 32%|███▏      | 51/161 [01:49<04:22,  2.38s/it] 32%|███▏      | 52/161 [01:51<04:07,  2.27s/it] 33%|███▎      | 53/161 [01:53<03:58,  2.21s/it] 34%|███▎      | 54/161 [01:55<03:52,  2.17s/it] 34%|███▍      | 55/161 [01:57<03:46,  2.13s/it] 35%|███▍      | 56/161 [02:00<04:07,  2.36s/it] 35%|███▌      | 57/161 [02:02<03:55,  2.26s/it] 36%|███▌      | 58/161 [02:04<03:47,  2.21s/it] 37%|███▋      | 59/161 [02:06<03:41,  2.17s/it] 37%|███▋      | 60/161 [02:08<03:37,  2.15s/it] 38%|███▊      | 61/161 [02:10<03:30,  2.10s/it] 39%|███▊      | 62/161 [02:12<03:24,  2.07s/it] 39%|███▉      | 63/161 [02:15<03:25,  2.10s/it] 40%|███▉      | 64/161 [02:17<03:22,  2.09s/it] 40%|████      | 65/161 [02:19<03:19,  2.07s/it] 41%|████      | 66/161 [02:21<03:15,  2.06s/it] 42%|████▏     | 67/161 [02:23<03:11,  2.04s/it] 42%|████▏     | 68/161 [02:25<03:08,  2.02s/it] 43%|████▎     | 69/161 [02:27<03:07,  2.04s/it] 43%|████▎     | 70/161 [02:29<03:05,  2.04s/it] 44%|████▍     | 71/161 [02:32<03:31,  2.35s/it] 45%|████▍     | 72/161 [02:35<03:42,  2.50s/it] 45%|████▌     | 73/161 [02:37<03:27,  2.36s/it] 46%|████▌     | 74/161 [02:39<03:17,  2.27s/it] 47%|████▋     | 75/161 [02:41<03:11,  2.22s/it] 47%|████▋     | 76/161 [02:44<03:30,  2.48s/it] 48%|████▊     | 77/161 [02:46<03:17,  2.35s/it] 48%|████▊     | 78/161 [02:55<05:47,  4.19s/it] 49%|████▉     | 79/161 [02:57<04:50,  3.55s/it] 50%|████▉     | 80/161 [02:59<04:10,  3.09s/it] 50%|█████     | 81/161 [03:01<03:41,  2.77s/it] 51%|█████     | 82/161 [03:03<03:20,  2.54s/it] 52%|█████▏    | 83/161 [03:05<03:25,  2.64s/it] 52%|█████▏    | 84/161 [03:08<03:10,  2.47s/it] 53%|█████▎    | 85/161 [03:10<03:06,  2.45s/it] 53%|█████▎    | 86/161 [03:12<02:53,  2.31s/it] 54%|█████▍    | 87/161 [03:15<03:03,  2.47s/it] 55%|█████▍    | 88/161 [03:17<02:50,  2.34s/it] 55%|█████▌    | 89/161 [03:20<03:02,  2.54s/it] 56%|█████▌    | 90/161 [03:22<02:51,  2.41s/it] 57%|█████▋    | 91/161 [03:24<02:42,  2.32s/it] 57%|█████▋    | 92/161 [03:27<02:51,  2.48s/it] 58%|█████▊    | 93/161 [03:31<03:14,  2.86s/it] 58%|█████▊    | 94/161 [03:34<03:13,  2.89s/it] 59%|█████▉    | 95/161 [03:36<03:09,  2.87s/it] 60%|█████▉    | 96/161 [03:38<02:49,  2.61s/it] 60%|██████    | 97/161 [03:41<02:52,  2.69s/it] 61%|██████    | 98/161 [03:43<02:38,  2.52s/it] 61%|██████▏   | 99/161 [03:46<02:28,  2.39s/it] 62%|██████▏   | 100/161 [03:48<02:19,  2.28s/it] 63%|██████▎   | 101/161 [03:50<02:13,  2.22s/it] 63%|██████▎   | 102/161 [03:52<02:08,  2.18s/it] 64%|██████▍   | 103/161 [03:54<02:03,  2.13s/it] 65%|██████▍   | 104/161 [03:56<01:58,  2.09s/it] 65%|██████▌   | 105/161 [03:58<02:07,  2.28s/it] 66%|██████▌   | 106/161 [04:01<02:01,  2.21s/it] 66%|██████▋   | 107/161 [04:03<01:57,  2.17s/it] 67%|██████▋   | 108/161 [04:05<01:53,  2.14s/it] 68%|██████▊   | 109/161 [04:07<02:02,  2.35s/it] 68%|██████▊   | 110/161 [04:10<01:55,  2.27s/it] 69%|██████▉   | 111/161 [04:12<01:49,  2.19s/it] 70%|██████▉   | 112/161 [04:14<01:45,  2.15s/it] 70%|███████   | 113/161 [04:17<01:55,  2.40s/it] 71%|███████   | 114/161 [04:19<01:47,  2.29s/it] 71%|███████▏  | 115/161 [04:21<01:41,  2.21s/it] 72%|███████▏  | 116/161 [04:23<01:47,  2.39s/it] 73%|███████▎  | 117/161 [04:26<01:40,  2.29s/it] 73%|███████▎  | 118/161 [04:28<01:34,  2.19s/it] 74%|███████▍  | 119/161 [04:30<01:30,  2.15s/it] 75%|███████▍  | 120/161 [04:32<01:27,  2.14s/it] 75%|███████▌  | 121/161 [04:34<01:24,  2.11s/it] 76%|███████▌  | 122/161 [04:36<01:21,  2.09s/it] 76%|███████▋  | 123/161 [04:38<01:18,  2.07s/it] 77%|███████▋  | 124/161 [04:40<01:16,  2.06s/it] 78%|███████▊  | 125/161 [04:42<01:14,  2.06s/it] 78%|███████▊  | 126/161 [04:44<01:11,  2.04s/it] 79%|███████▉  | 127/161 [04:46<01:09,  2.03s/it] 80%|███████▉  | 128/161 [04:48<01:10,  2.15s/it] 80%|████████  | 129/161 [04:51<01:15,  2.35s/it] 81%|████████  | 130/161 [04:53<01:10,  2.27s/it] 81%|████████▏ | 131/161 [04:55<01:05,  2.20s/it] 82%|████████▏ | 132/161 [04:57<01:02,  2.15s/it] 83%|████████▎ | 133/161 [04:59<00:58,  2.10s/it] 83%|████████▎ | 134/161 [05:01<00:56,  2.08s/it] 84%|████████▍ | 135/161 [05:03<00:53,  2.07s/it] 84%|████████▍ | 136/161 [05:05<00:51,  2.07s/it] 85%|████████▌ | 137/161 [05:08<00:49,  2.08s/it] 86%|████████▌ | 138/161 [05:10<00:47,  2.06s/it] 86%|████████▋ | 139/161 [05:12<00:45,  2.06s/it] 87%|████████▋ | 140/161 [05:33<02:43,  7.78s/it] 88%|████████▊ | 141/161 [05:35<02:01,  6.07s/it] 88%|████████▊ | 142/161 [05:37<01:32,  4.84s/it] 89%|████████▉ | 143/161 [05:39<01:12,  4.00s/it] 89%|████████▉ | 144/161 [05:41<00:58,  3.43s/it] 90%|█████████ | 145/161 [05:43<00:48,  3.02s/it] 91%|█████████ | 146/161 [05:45<00:40,  2.71s/it] 91%|█████████▏| 147/161 [05:47<00:34,  2.50s/it] 92%|█████████▏| 148/161 [05:49<00:30,  2.38s/it] 93%|█████████▎| 149/161 [05:51<00:27,  2.27s/it] 93%|█████████▎| 150/161 [05:53<00:24,  2.21s/it] 94%|█████████▍| 151/161 [05:55<00:21,  2.17s/it] 94%|█████████▍| 152/161 [06:00<00:27,  3.02s/it] 95%|█████████▌| 153/161 [06:02<00:21,  2.73s/it] 96%|█████████▌| 154/161 [06:04<00:17,  2.53s/it] 96%|█████████▋| 155/161 [06:06<00:14,  2.39s/it] 97%|█████████▋| 156/161 [06:08<00:11,  2.28s/it] 98%|█████████▊| 157/161 [06:10<00:08,  2.20s/it] 98%|█████████▊| 158/161 [06:12<00:06,  2.15s/it] 99%|█████████▉| 159/161 [06:15<00:04,  2.12s/it] 99%|█████████▉| 160/161 [06:17<00:02,  2.08s/it]100%|██████████| 161/161 [06:18<00:00,  1.95s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.597 seconds.
Prefix dict has been built successfully.
100%|██████████| 161/161 [06:20<00:00,  2.36s/it]
***** predict metrics *****
  predict_bleu-4                 =     25.228
  predict_model_preparation_time =     0.0065
  predict_rouge-1                =    55.6295
  predict_rouge-2                =    46.5397
  predict_rouge-l                =    54.2425
  predict_runtime                = 0:06:25.77
  predict_samples_per_second     =      3.331
  predict_steps_per_second       =      0.417
[INFO|2025-06-29 16:31:14] llamafactory.train.sft.trainer:143 >> Saving prediction results to evaluate_outputs/results/padchest_gr_test/Qwen2-VL-7B/generated_predictions.jsonl
✅ Qwen2-VL-7B evaluation completed successfully

==============================================
Starting evaluation for model: InternVL3-2B
Model path: /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf
Template: intern_vl
Batch size: 16
==============================================
[2025-06-29 16:31:28,671] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 06-29 16:31:32 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-06-29 16:31:36] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,187 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:31:36,689 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:928] 2025-06-29 16:31:36,694 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/processor_config.json
[INFO|image_processing_base.py:378] 2025-06-29 16:31:36,696 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-29 16:31:36,705 >> Image processor GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:31:36,706 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:31:37,129 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-06-29 16:31:37,131 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-06-29 16:31:37,131 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-06-29 16:31:37,131 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:31:37,136 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|video_processing_utils.py:627] 2025-06-29 16:31:37,137 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-29 16:31:37,137 >> Video processor InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-06-29 16:31:37,138 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/processor_config.json
[INFO|processing_utils.py:990] 2025-06-29 16:31:37,646 >> Processor InternVLProcessor:
- image_processor: GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf', vocab_size=151643, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151666: AddedToken("</img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151667: AddedToken("<IMG_CONTEXT>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151668: AddedToken("<quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151669: AddedToken("</quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151670: AddedToken("<ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151671: AddedToken("</ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151672: AddedToken("<box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151673: AddedToken("</box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151674: AddedToken("<video>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}


{
  "image_seq_length": 256,
  "processor_class": "InternVLProcessor"
}

[INFO|2025-06-29 16:31:37] llamafactory.data.template:143 >> Using default system message: You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image..
[INFO|2025-06-29 16:31:37] llamafactory.data.template:143 >> Add <|im_end|> to stop words.
[INFO|2025-06-29 16:31:37] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/padchest_gt/padchest_input_data_qwen2_test_len_1285.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 81/1285 [01:15<18:35,  1.08 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 162/1285 [01:15<07:15,  2.58 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 242/1285 [01:17<03:47,  4.58 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 323/1285 [01:17<02:09,  7.44 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 403/1285 [01:17<01:16, 11.49 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 483/1285 [01:18<00:47, 16.90 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 563/1285 [01:18<00:29, 24.22 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 724/1285 [01:18<00:12, 45.11 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 804/1285 [01:18<00:08, 58.58 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 964/1285 [01:19<00:03, 90.96 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1045/1285 [01:19<00:02, 107.09 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1125/1285 [01:19<00:01, 136.14 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1205/1285 [01:19<00:00, 167.81 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1285/1285 [01:20<00:00, 16.04 examples/s] 
[INFO|configuration_utils.py:696] 2025-06-29 16:32:58,827 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:32:58,829 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 11900, 16155, 11163, 311, 13382, 323, 23560, 6457, 2168, 6358, 13, 4615, 3476, 374, 311, 10542, 323, 94416, 11900, 5729, 5904, 315, 3151, 18808, 304, 15138, 1599, 29530, 5335, 13, 1446, 686, 387, 3897, 448, 264, 15138, 1599, 29530, 2168, 323, 264, 8457, 829, 13, 4615, 3383, 374, 311, 29257, 94416, 279, 9760, 5537, 1141, 8, 304, 279, 2168, 13, 151645, 198, 151644, 872, 198, 151665, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151666, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 81277, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image.<|im_end|>
<|im_start|>user
<img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>
Return bounding boxes of 'Pleural Thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 17, 22, 21, 345, 414, 220, 16, 19, 16, 345, 414, 220, 19, 23, 19, 345, 414, 220, 17, 16, 23, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 1153, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 20, 17, 345, 414, 220, 16, 17, 22, 345, 414, 220, 22, 21, 22, 345, 414, 220, 17, 18, 15, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      276,
      141,
      484,
      218
    ],
    "label": "Pleural Thickening"
  },
  {
    "bbox_2d": [
      552,
      127,
      767,
      230
    ],
    "label": "Pleural Thickening"
  }
]
</answer><|im_end|>

[INFO|2025-06-29 16:32:58] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1148] 2025-06-29 16:32:58,959 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/model.safetensors
[INFO|modeling_utils.py:2241] 2025-06-29 16:32:58,970 >> Instantiating InternVLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-29 16:32:58,973 >> Generate config GenerationConfig {}

[INFO|modeling_utils.py:2241] 2025-06-29 16:32:59,570 >> Instantiating InternVLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2241] 2025-06-29 16:32:59,605 >> Instantiating Qwen2Model model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:5131] 2025-06-29 16:33:04,084 >> All model checkpoint weights were used when initializing InternVLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-06-29 16:33:04,088 >> All the weights of InternVLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-29 16:33:04,093 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-2B-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-29 16:33:04,094 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|2025-06-29 16:33:04] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-29 16:33:04] llamafactory.model.loader:143 >> all params: 2,088,957,440
[WARNING|2025-06-29 16:33:04] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-06-29 16:33:04,135 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-06-29 16:33:04,135 >>   Num examples = 1285
[INFO|trainer.py:4332] 2025-06-29 16:33:04,135 >>   Batch size = 16
  0%|          | 0/81 [00:00<?, ?it/s]  2%|▏         | 2/81 [00:01<00:57,  1.36it/s]  4%|▎         | 3/81 [00:02<01:20,  1.03s/it]  5%|▍         | 4/81 [00:04<01:31,  1.19s/it]  6%|▌         | 5/81 [00:11<04:11,  3.30s/it]  7%|▋         | 6/81 [00:13<03:20,  2.68s/it]  9%|▊         | 7/81 [00:14<02:49,  2.29s/it] 10%|▉         | 8/81 [00:15<02:28,  2.03s/it] 11%|█         | 9/81 [00:17<02:13,  1.85s/it] 12%|█▏        | 10/81 [00:18<02:03,  1.73s/it] 14%|█▎        | 11/81 [00:20<01:55,  1.65s/it] 15%|█▍        | 12/81 [00:22<02:08,  1.86s/it] 16%|█▌        | 13/81 [00:24<01:58,  1.74s/it] 17%|█▋        | 14/81 [00:25<01:50,  1.66s/it] 19%|█▊        | 15/81 [00:27<01:45,  1.59s/it] 20%|█▉        | 16/81 [00:28<01:39,  1.53s/it] 21%|██        | 17/81 [00:32<02:33,  2.40s/it] 22%|██▏       | 18/81 [00:34<02:13,  2.12s/it] 23%|██▎       | 19/81 [00:35<01:58,  1.91s/it] 25%|██▍       | 20/81 [00:37<01:47,  1.77s/it] 26%|██▌       | 21/81 [00:38<01:40,  1.68s/it] 27%|██▋       | 22/81 [00:40<01:34,  1.61s/it] 28%|██▊       | 23/81 [00:42<01:44,  1.81s/it] 30%|██▉       | 24/81 [00:43<01:36,  1.70s/it] 31%|███       | 25/81 [00:45<01:31,  1.63s/it] 32%|███▏      | 26/81 [00:46<01:26,  1.56s/it] 33%|███▎      | 27/81 [00:48<01:22,  1.53s/it] 35%|███▍      | 28/81 [00:49<01:19,  1.50s/it] 36%|███▌      | 29/81 [00:54<02:03,  2.38s/it] 37%|███▋      | 30/81 [00:55<01:47,  2.10s/it] 38%|███▊      | 31/81 [00:56<01:35,  1.91s/it] 40%|███▉      | 32/81 [00:58<01:26,  1.77s/it] 41%|████      | 33/81 [00:59<01:20,  1.67s/it] 42%|████▏     | 34/81 [01:01<01:14,  1.59s/it] 43%|████▎     | 35/81 [01:02<01:11,  1.55s/it] 44%|████▍     | 36/81 [01:04<01:08,  1.52s/it] 46%|████▌     | 37/81 [01:05<01:05,  1.50s/it] 47%|████▋     | 38/81 [01:07<01:03,  1.48s/it] 48%|████▊     | 39/81 [01:08<01:01,  1.48s/it] 49%|████▉     | 40/81 [01:09<01:00,  1.47s/it] 51%|█████     | 41/81 [01:11<00:58,  1.46s/it] 52%|█████▏    | 42/81 [01:13<01:07,  1.73s/it] 53%|█████▎    | 43/81 [01:15<01:02,  1.65s/it] 54%|█████▍    | 44/81 [01:16<00:58,  1.59s/it] 56%|█████▌    | 45/81 [01:18<00:55,  1.55s/it] 57%|█████▋    | 46/81 [01:19<00:53,  1.52s/it] 58%|█████▊    | 47/81 [01:21<00:50,  1.49s/it] 59%|█████▉    | 48/81 [01:22<00:48,  1.48s/it] 60%|██████    | 49/81 [01:23<00:47,  1.48s/it] 62%|██████▏   | 50/81 [01:25<00:45,  1.46s/it] 63%|██████▎   | 51/81 [01:26<00:43,  1.46s/it] 64%|██████▍   | 52/81 [01:28<00:42,  1.45s/it] 65%|██████▌   | 53/81 [01:29<00:40,  1.45s/it] 67%|██████▋   | 54/81 [01:31<00:39,  1.45s/it] 68%|██████▊   | 55/81 [01:33<00:45,  1.73s/it] 69%|██████▉   | 56/81 [01:34<00:41,  1.65s/it] 70%|███████   | 57/81 [01:36<00:38,  1.59s/it] 72%|███████▏  | 58/81 [01:37<00:35,  1.54s/it] 73%|███████▎  | 59/81 [01:39<00:33,  1.51s/it] 74%|███████▍  | 60/81 [01:40<00:31,  1.49s/it] 75%|███████▌  | 61/81 [01:42<00:29,  1.48s/it] 77%|███████▋  | 62/81 [01:43<00:27,  1.47s/it] 78%|███████▊  | 63/81 [01:45<00:26,  1.45s/it] 79%|███████▉  | 64/81 [01:46<00:24,  1.46s/it] 80%|████████  | 65/81 [01:48<00:23,  1.46s/it] 81%|████████▏ | 66/81 [01:49<00:21,  1.46s/it] 83%|████████▎ | 67/81 [01:50<00:20,  1.45s/it] 84%|████████▍ | 68/81 [01:52<00:18,  1.45s/it] 85%|████████▌ | 69/81 [01:53<00:17,  1.45s/it] 86%|████████▋ | 70/81 [01:55<00:16,  1.45s/it] 88%|████████▊ | 71/81 [01:56<00:14,  1.44s/it] 89%|████████▉ | 72/81 [01:58<00:12,  1.44s/it] 90%|█████████ | 73/81 [01:59<00:11,  1.45s/it] 91%|█████████▏| 74/81 [02:00<00:10,  1.44s/it] 93%|█████████▎| 75/81 [02:02<00:08,  1.45s/it] 94%|█████████▍| 76/81 [02:03<00:07,  1.46s/it] 95%|█████████▌| 77/81 [02:05<00:05,  1.46s/it] 96%|█████████▋| 78/81 [02:06<00:04,  1.46s/it] 98%|█████████▊| 79/81 [02:08<00:02,  1.45s/it] 99%|█████████▉| 80/81 [02:09<00:01,  1.43s/it]100%|██████████| 81/81 [02:10<00:00,  1.34s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.535 seconds.
Prefix dict has been built successfully.
100%|██████████| 81/81 [02:12<00:00,  1.64s/it]
***** predict metrics *****
  predict_bleu-4                 =    48.2065
  predict_model_preparation_time =     0.0064
  predict_rouge-1                =    63.5782
  predict_rouge-2                =    55.4451
  predict_rouge-l                =    63.4585
  predict_runtime                = 0:02:15.28
  predict_samples_per_second     =      9.498
  predict_steps_per_second       =      0.599
[INFO|2025-06-29 16:35:19] llamafactory.train.sft.trainer:143 >> Saving prediction results to evaluate_outputs/results/padchest_gr_test/InternVL3-2B/generated_predictions.jsonl
✅ InternVL3-2B evaluation completed successfully

==============================================
Starting evaluation for model: Qwen2-VL-2B
Model path: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct
Template: qwen2_vl
Batch size: 16
==============================================
[2025-06-29 16:35:33,617] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 06-29 16:35:37 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-06-29 16:35:41] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,041 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:35:41,286 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-29 16:35:41,286 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-29 16:35:41,290 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-29 16:35:41,297 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-29 16:35:41,297 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-29 16:35:41,517 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-06-29 16:35:41,521 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-06-29 16:35:41,521 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-06-29 16:35:41,522 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:35:41,524 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

[INFO|video_processing_utils.py:627] 2025-06-29 16:35:41,525 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-29 16:35:41,525 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-29 16:35:41,895 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|2025-06-29 16:35:41] llamafactory.data.template:143 >> Using default system message: You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image..
[INFO|2025-06-29 16:35:41] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/padchest_gt/padchest_input_data_qwen2_test_len_1285.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1285 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 80/1285 [01:43<25:52,  1.29s/ examples]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 160/1285 [01:48<10:43,  1.75 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▊        | 240/1285 [01:50<05:32,  3.14 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▍       | 320/1285 [01:50<03:06,  5.17 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 401/1285 [01:51<01:54,  7.69 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 482/1285 [01:52<01:10, 11.43 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 643/1285 [01:53<00:30, 21.08 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 724/1285 [01:53<00:20, 28.01 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 804/1285 [01:53<00:12, 37.75 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 885/1285 [01:53<00:07, 51.19 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 965/1285 [01:54<00:05, 63.71 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1045/1285 [01:54<00:02, 80.71 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1285/1285 [01:54<00:00, 168.53 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1285/1285 [01:54<00:00, 11.20 examples/s] 
[INFO|configuration_utils.py:696] 2025-06-29 16:37:37,575 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-06-29 16:37:37,582 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 11900, 16155, 11163, 311, 13382, 323, 23560, 6457, 2168, 6358, 13, 4615, 3476, 374, 311, 10542, 323, 94416, 11900, 5729, 5904, 315, 3151, 18808, 304, 15138, 1599, 29530, 5335, 13, 1446, 686, 387, 3897, 448, 264, 15138, 1599, 29530, 2168, 323, 264, 8457, 829, 13, 4615, 3383, 374, 311, 29257, 94416, 279, 9760, 5537, 1141, 8, 304, 279, 2168, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 81277, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pleural Thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 17, 22, 21, 345, 414, 220, 16, 19, 16, 345, 414, 220, 19, 23, 19, 345, 414, 220, 17, 16, 23, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 1153, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 20, 17, 345, 414, 220, 16, 17, 22, 345, 414, 220, 22, 21, 22, 345, 414, 220, 17, 18, 15, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      276,
      141,
      484,
      218
    ],
    "label": "Pleural Thickening"
  },
  {
    "bbox_2d": [
      552,
      127,
      767,
      230
    ],
    "label": "Pleural Thickening"
  }
]
</answer><|im_end|>

[INFO|2025-06-29 16:37:37] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1148] 2025-06-29 16:37:37,658 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-29 16:37:37,661 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-29 16:37:37,663 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2241] 2025-06-29 16:37:37,663 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2241] 2025-06-29 16:37:37,676 >> Instantiating Qwen2VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]
[INFO|modeling_utils.py:5131] 2025-06-29 16:37:42,150 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-06-29 16:37:42,150 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-29 16:37:42,156 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-29 16:37:42,156 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

[INFO|2025-06-29 16:37:42] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-29 16:37:42] llamafactory.model.loader:143 >> all params: 2,208,985,600
[WARNING|2025-06-29 16:37:42] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-06-29 16:37:42,196 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-06-29 16:37:42,196 >>   Num examples = 1285
[INFO|trainer.py:4332] 2025-06-29 16:37:42,196 >>   Batch size = 16
  0%|          | 0/81 [00:00<?, ?it/s]  2%|▏         | 2/81 [00:03<02:09,  1.63s/it]  4%|▎         | 3/81 [00:06<02:58,  2.28s/it]  5%|▍         | 4/81 [00:10<03:33,  2.77s/it]  6%|▌         | 5/81 [00:13<03:41,  2.92s/it]  7%|▋         | 6/81 [00:16<03:47,  3.03s/it]  9%|▊         | 7/81 [00:19<03:46,  3.07s/it] 10%|▉         | 8/81 [00:22<03:46,  3.10s/it] 11%|█         | 9/81 [00:26<03:46,  3.15s/it] 12%|█▏        | 10/81 [00:29<03:45,  3.18s/it] 14%|█▎        | 11/81 [00:32<03:42,  3.18s/it] 15%|█▍        | 12/81 [00:35<03:39,  3.19s/it] 16%|█▌        | 13/81 [00:38<03:36,  3.18s/it] 17%|█▋        | 14/81 [00:42<03:32,  3.17s/it] 19%|█▊        | 15/81 [00:45<03:29,  3.18s/it] 20%|█▉        | 16/81 [00:48<03:23,  3.14s/it] 21%|██        | 17/81 [00:51<03:21,  3.14s/it] 22%|██▏       | 18/81 [00:54<03:24,  3.25s/it] 23%|██▎       | 19/81 [00:58<03:19,  3.21s/it] 25%|██▍       | 20/81 [01:01<03:14,  3.20s/it] 26%|██▌       | 21/81 [01:04<03:12,  3.21s/it] 27%|██▋       | 22/81 [01:08<03:24,  3.46s/it] 28%|██▊       | 23/81 [01:11<03:15,  3.37s/it] 30%|██▉       | 24/81 [01:14<03:09,  3.33s/it] 31%|███       | 25/81 [01:18<03:04,  3.29s/it] 32%|███▏      | 26/81 [01:21<03:02,  3.32s/it] 33%|███▎      | 27/81 [01:24<02:57,  3.29s/it] 35%|███▍      | 28/81 [01:27<02:54,  3.29s/it] 36%|███▌      | 29/81 [01:31<02:48,  3.24s/it] 37%|███▋      | 30/81 [01:34<02:45,  3.24s/it] 38%|███▊      | 31/81 [01:37<02:45,  3.32s/it] 40%|███▉      | 32/81 [01:41<02:41,  3.30s/it] 41%|████      | 33/81 [01:44<02:37,  3.27s/it] 42%|████▏     | 34/81 [01:48<02:46,  3.55s/it] 43%|████▎     | 35/81 [01:51<02:38,  3.44s/it] 44%|████▍     | 36/81 [01:54<02:30,  3.34s/it] 46%|████▌     | 37/81 [01:57<02:25,  3.30s/it] 47%|████▋     | 38/81 [02:01<02:20,  3.27s/it] 48%|████▊     | 39/81 [02:04<02:17,  3.27s/it] 49%|████▉     | 40/81 [02:07<02:14,  3.27s/it] 51%|█████     | 41/81 [02:10<02:10,  3.27s/it] 52%|█████▏    | 42/81 [02:14<02:07,  3.26s/it] 53%|█████▎    | 43/81 [02:17<02:02,  3.23s/it] 54%|█████▍    | 44/81 [02:20<01:58,  3.21s/it] 56%|█████▌    | 45/81 [02:24<01:59,  3.33s/it] 57%|█████▋    | 46/81 [02:27<01:55,  3.29s/it] 58%|█████▊    | 47/81 [02:31<02:01,  3.59s/it] 59%|█████▉    | 48/81 [02:34<01:54,  3.46s/it] 60%|██████    | 49/81 [02:38<01:48,  3.39s/it] 62%|██████▏   | 50/81 [02:41<01:43,  3.34s/it] 63%|██████▎   | 51/81 [02:44<01:39,  3.31s/it] 64%|██████▍   | 52/81 [02:47<01:34,  3.27s/it] 65%|██████▌   | 53/81 [02:50<01:30,  3.23s/it] 67%|██████▋   | 54/81 [02:53<01:27,  3.22s/it] 68%|██████▊   | 55/81 [02:57<01:23,  3.20s/it] 69%|██████▉   | 56/81 [03:00<01:19,  3.20s/it] 70%|███████   | 57/81 [03:03<01:16,  3.20s/it] 72%|███████▏  | 58/81 [03:07<01:20,  3.50s/it] 73%|███████▎  | 59/81 [03:10<01:14,  3.41s/it] 74%|███████▍  | 60/81 [03:14<01:11,  3.43s/it] 75%|███████▌  | 61/81 [03:17<01:07,  3.35s/it] 77%|███████▋  | 62/81 [03:20<01:02,  3.29s/it] 78%|███████▊  | 63/81 [03:23<00:58,  3.26s/it] 79%|███████▉  | 64/81 [03:27<00:55,  3.24s/it] 80%|████████  | 65/81 [03:30<00:51,  3.24s/it] 81%|████████▏ | 66/81 [03:33<00:48,  3.22s/it] 83%|████████▎ | 67/81 [03:36<00:44,  3.17s/it] 84%|████████▍ | 68/81 [03:39<00:41,  3.19s/it] 85%|████████▌ | 69/81 [03:42<00:38,  3.20s/it] 86%|████████▋ | 70/81 [03:46<00:35,  3.21s/it] 88%|████████▊ | 71/81 [03:49<00:32,  3.20s/it] 89%|████████▉ | 72/81 [03:52<00:28,  3.20s/it] 90%|█████████ | 73/81 [03:55<00:25,  3.22s/it] 91%|█████████▏| 74/81 [03:59<00:22,  3.22s/it] 93%|█████████▎| 75/81 [04:02<00:20,  3.34s/it] 94%|█████████▍| 76/81 [04:05<00:16,  3.30s/it] 95%|█████████▌| 77/81 [04:09<00:13,  3.29s/it] 96%|█████████▋| 78/81 [04:14<00:11,  3.87s/it] 98%|█████████▊| 79/81 [04:17<00:07,  3.69s/it] 99%|█████████▉| 80/81 [04:20<00:03,  3.50s/it]100%|██████████| 81/81 [04:22<00:00,  2.85s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.557 seconds.
Prefix dict has been built successfully.
100%|██████████| 81/81 [04:23<00:00,  3.26s/it]
***** predict metrics *****
  predict_bleu-4                 =    17.4992
  predict_model_preparation_time =     0.0062
  predict_rouge-1                =     47.064
  predict_rouge-2                =    36.0559
  predict_rouge-l                =    43.5466
  predict_runtime                = 0:04:27.97
  predict_samples_per_second     =      4.795
  predict_steps_per_second       =      0.302
[INFO|2025-06-29 16:42:10] llamafactory.train.sft.trainer:143 >> Saving prediction results to evaluate_outputs/results/padchest_gr_test/Qwen2-VL-2B/generated_predictions.jsonl
✅ Qwen2-VL-2B evaluation completed successfully

🎉 All model evaluations completed!
