Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Fri Jul  4 21:31:52 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |
| N/A   30C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |
| N/A   31C    P0              54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |
| N/A   31C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |
| N/A   31C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
ðŸš€ Running experiment: Experiment 1:Qwen2-VL-2B-Instruct SFT Base
-----------------------------------
Model: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/
Output Dir: saves/qwen2_vl-3b/vindr_sft_def
Train Dataset: vinder_train_def
Cutoff Length: 1024
Save Steps: 63
Learning Rate: 3.0e-5
Num Train Epochs: 20.0
Logging Steps: 5
Per Device Train Batch Size: 8
Gradient Accumulation Steps: 8
-----------------------------------
[2025-07-04 21:32:04,853] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 07-04 21:32:08 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-07-04 21:32:11] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:34877
W0704 21:32:13.613000 2365248 site-packages/torch/distributed/run.py:792] 
W0704 21:32:13.613000 2365248 site-packages/torch/distributed/run.py:792] *****************************************
W0704 21:32:13.613000 2365248 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0704 21:32:13.613000 2365248 site-packages/torch/distributed/run.py:792] *****************************************
[2025-07-04 21:32:23,201] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:32:23,245] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:32:23,247] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:32:23,296] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:32:27,352] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:32:27,424] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:32:27,482] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:32:27,482] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-07-04 21:32:27,485] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-07-04 21:32:27] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:27,930 >> loading file chat_template.jinja
[INFO|2025-07-04 21:32:28] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-07-04 21:32:28] llamafactory.hparams.parser:406 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-07-04 21:32:28] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2299] 2025-07-04 21:32:28,222 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-07-04 21:32:28,222 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-07-04 21:32:28,227 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-07-04 21:32:28,234 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:32:28,235 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-07-04 21:32:28,524 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-07-04 21:32:28,526 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-07-04 21:32:28,526 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-07-04 21:32:28,526 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-07-04 21:32:28,528 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

[INFO|video_processing_utils.py:627] 2025-07-04 21:32:28,529 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-07-04 21:32:28,529 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|processing_utils.py:990] 2025-07-04 21:32:28,990 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|2025-07-04 21:32:29] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/train_def_vindr_qwen2_len_16087.json...
[rank2]:[W704 21:32:29.600665674 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W704 21:32:29.703430891 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W704 21:32:29.718102063 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 0/16087 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   2%|â–         | 369/16087 [00:00<00:04, 3299.35 examples/s]Converting format of dataset (num_proc=16):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9985/16087 [00:00<00:00, 54622.97 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16087/16087 [00:00<00:00, 42465.20 examples/s]
[rank0]:[W704 21:32:30.077198818 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/16087 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–Œ         | 1000/16087 [00:18<04:35, 54.81 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 1006/16087 [00:18<04:35, 54.82 examples/s]Running tokenizer on dataset (num_proc=16):  12%|â–ˆâ–        | 2006/16087 [00:19<01:31, 154.05 examples/s]Running tokenizer on dataset (num_proc=16):  19%|â–ˆâ–Š        | 3011/16087 [00:19<00:44, 290.88 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–       | 4017/16087 [00:19<00:26, 460.02 examples/s]Running tokenizer on dataset (num_proc=16):  31%|â–ˆâ–ˆâ–ˆ       | 5017/16087 [00:19<00:15, 709.59 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 7028/16087 [00:19<00:06, 1385.52 examples/s]Running tokenizer on dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9039/16087 [00:20<00:03, 2274.59 examples/s]Running tokenizer on dataset (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 11055/16087 [00:20<00:01, 3119.24 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 13060/16087 [00:21<00:01, 2307.01 examples/s]Running tokenizer on dataset (num_proc=16):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 14070/16087 [00:21<00:00, 2692.63 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 15076/16087 [00:22<00:00, 2743.87 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 16081/16087 [00:22<00:00, 3319.73 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16087/16087 [00:22<00:00, 721.00 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 12045, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 32440, 4176, 12045, 6019, 3363, 61597, 25755, 315, 279, 7100, 5690, 3884, 438, 264, 27950, 6193, 2163, 279, 20622, 13, 151645, 198, 151644, 77091, 198, 27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 15, 345, 414, 220, 21, 21, 21, 345, 414, 220, 16, 19, 15, 345, 414, 220, 22, 20, 20, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 12045, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pleural thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pleural thickening means Increased thickness of the pleura seen as a dense layer around the lung.<|im_end|>
<|im_start|>assistant
<answer>
[
  {
    "bbox_2d": [
      50,
      666,
      140,
      755
    ],
    "label": "Pleural thickening"
  }
]
</answer><|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 15, 345, 414, 220, 21, 21, 21, 345, 414, 220, 16, 19, 15, 345, 414, 220, 22, 20, 20, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 12045, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      50,
      666,
      140,
      755
    ],
    "label": "Pleural thickening"
  }
]
</answer><|im_end|>

[INFO|configuration_utils.py:696] 2025-07-04 21:32:55,558 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-07-04 21:32:55,560 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

[INFO|2025-07-04 21:32:55] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1148] 2025-07-04 21:32:55,626 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:3881] 2025-07-04 21:32:55,626 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-07-04 21:32:55,626] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[INFO|configuration_utils.py:1135] 2025-07-04 21:32:55,639 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|modeling_utils.py:2241] 2025-07-04 21:32:55,639 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.float32.
[2025-07-04 21:32:55,770] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:32:55,787] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:32:55,789] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[INFO|modeling_utils.py:2241] 2025-07-04 21:32:55,834 >> Instantiating Qwen2VLTextModel model under default dtype torch.float32.
[2025-07-04 21:32:55,996] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 730, num_elems = 2.44B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.70s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.70s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.69s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.58s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.58s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.58s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.69s/it]
[INFO|modeling_utils.py:5131] 2025-07-04 21:32:59,411 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-07-04 21:32:59,412 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-07-04 21:32:59,417 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/generation_config.json
[INFO|configuration_utils.py:1135] 2025-07-04 21:32:59,417 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

[INFO|2025-07-04 21:32:59] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-07-04 21:32:59] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-07-04 21:32:59] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-07-04 21:32:59] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2025-07-04 21:32:59] llamafactory.model.loader:143 >> trainable params: 2,208,985,600 || all params: 2,208,985,600 || trainable%: 100.0000
[INFO|trainer.py:756] 2025-07-04 21:32:59,459 >> Using auto half precision backend
[2025-07-04 21:33:00,188] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.6, git-hash=unknown, git-branch=unknown
[2025-07-04 21:33:00,188] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:33:00,201] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-07-04 21:33:00,202] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-07-04 21:33:00,202] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-07-04 21:33:00,237] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-07-04 21:33:00,237] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-07-04 21:33:00,237] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-07-04 21:33:00,237] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-07-04 21:33:00,469] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-07-04 21:33:00,470] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 2.33 GB         CA 1.03 GB         Max_CA 2 GB 
[2025-07-04 21:33:00,470] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.43 GB, percent = 7.8%
[2025-07-04 21:33:00,472] [INFO] [stage3.py:170:__init__] Reduce bucket size 2359296
[2025-07-04 21:33:00,472] [INFO] [stage3.py:171:__init__] Prefetch bucket size 2123366
[2025-07-04 21:33:00,704] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-07-04 21:33:00,705] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:33:00,705] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.82 GB, percent = 7.8%
Parameter Offload: Total persistent parameters: 686592 in 401 params
[2025-07-04 21:33:01,095] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-07-04 21:33:01,096] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:33:01,096] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.82 GB, percent = 7.8%
[2025-07-04 21:33:01,374] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-07-04 21:33:01,374] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:33:01,375] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.82 GB, percent = 7.8%
[2025-07-04 21:33:03,037] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[2025-07-04 21:33:03,038] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.06 GB         Max_CA 1 GB 
[2025-07-04 21:33:03,039] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.85 GB, percent = 7.9%
[2025-07-04 21:33:03,288] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-07-04 21:33:03,289] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.06 GB         Max_CA 1 GB 
[2025-07-04 21:33:03,289] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.86 GB, percent = 7.9%
[2025-07-04 21:33:04,034] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-07-04 21:33:04,035] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 4.11 GB         CA 4.15 GB         Max_CA 4 GB 
[2025-07-04 21:33:04,035] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.86 GB, percent = 7.9%
[2025-07-04 21:33:04,327] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-07-04 21:33:04,327] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 3.09 GB         CA 4.15 GB         Max_CA 4 GB 
[2025-07-04 21:33:04,328] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.86 GB, percent = 7.9%
[2025-07-04 21:33:04,615] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-07-04 21:33:04,616] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 5.14 GB         CA 6.2 GB         Max_CA 6 GB 
[2025-07-04 21:33:04,616] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.86 GB, percent = 7.9%
[2025-07-04 21:33:04,617] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-07-04 21:33:05,148] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-07-04 21:33:05,149] [INFO] [utils.py:782:see_memory_usage] MA 4.12 GB         Max_MA 4.99 GB         CA 6.2 GB         Max_CA 6 GB 
[2025-07-04 21:33:05,149] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 80.65 GB, percent = 8.0%
[2025-07-04 21:33:05,149] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-07-04 21:33:05,149] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-07-04 21:33:05,150] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-07-04 21:33:05,150] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-07-04 21:33:05,151] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-07-04 21:33:05,151] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9081eb91d0>
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-07-04 21:33:05,152] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   train_batch_size ............. 256
[2025-07-04 21:33:05,153] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2359296 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=2123366 param_persistence_threshold=15360 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-07-04 21:33:05,154] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-07-04 21:33:05,154] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.359296e+06, 
        "stage3_prefetch_bucket_size": 2.123366e+06, 
        "stage3_param_persistence_threshold": 1.536000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2409] 2025-07-04 21:33:05,156 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-07-04 21:33:05,156 >>   Num examples = 16,087
[INFO|trainer.py:2411] 2025-07-04 21:33:05,156 >>   Num Epochs = 20
[INFO|trainer.py:2412] 2025-07-04 21:33:05,156 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2415] 2025-07-04 21:33:05,156 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:2416] 2025-07-04 21:33:05,156 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2417] 2025-07-04 21:33:05,156 >>   Total optimization steps = 1,260
[INFO|trainer.py:2418] 2025-07-04 21:33:05,159 >>   Number of trainable parameters = 2,208,985,600
[INFO|integration_utils.py:832] 2025-07-04 21:33:05,161 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jun_rio (compai) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/june/Code/LLaMA-Factory-Latest/wandb/run-20250704_213305-hbyhvtgn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run saves/qwen2_vl-3b/vindr_sft_def
wandb: â­ï¸ View project at https://wandb.ai/compai/llamafactory
wandb: ðŸš€ View run at https://wandb.ai/compai/llamafactory/runs/hbyhvtgn
  0%|          | 0/1260 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-07-04 21:33:09,067 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  0%|          | 1/1260 [00:30<10:30:17, 30.04s/it]  0%|          | 2/1260 [00:51<8:48:52, 25.22s/it]   0%|          | 3/1260 [01:13<8:12:54, 23.53s/it]  0%|          | 4/1260 [01:34<7:56:06, 22.74s/it]  0%|          | 5/1260 [01:56<7:49:15, 22.43s/it]                                                  {'loss': 0.9897, 'grad_norm': 33.12212444310384, 'learning_rate': 9.523809523809523e-07, 'epoch': 0.08}
  0%|          | 5/1260 [01:56<7:49:15, 22.43s/it]  0%|          | 6/1260 [02:18<7:43:15, 22.17s/it]  1%|          | 7/1260 [02:40<7:40:13, 22.04s/it]  1%|          | 8/1260 [03:01<7:35:41, 21.84s/it]  1%|          | 9/1260 [03:23<7:37:26, 21.94s/it]  1%|          | 10/1260 [03:45<7:33:48, 21.78s/it]                                                   {'loss': 0.7838, 'grad_norm': 13.329544759908572, 'learning_rate': 2.1428571428571427e-06, 'epoch': 0.16}
  1%|          | 10/1260 [03:45<7:33:48, 21.78s/it]  1%|          | 11/1260 [04:06<7:32:29, 21.74s/it]  1%|          | 12/1260 [04:28<7:31:32, 21.71s/it]  1%|          | 13/1260 [04:50<7:30:46, 21.69s/it]  1%|          | 14/1260 [05:11<7:29:20, 21.64s/it]  1%|          | 15/1260 [05:33<7:27:27, 21.56s/it]                                                   {'loss': 0.4827, 'grad_norm': 1.6691169215682038, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.24}
  1%|          | 15/1260 [05:33<7:27:27, 21.56s/it]  1%|â–         | 16/1260 [05:54<7:26:04, 21.52s/it]  1%|â–         | 17/1260 [06:16<7:28:09, 21.63s/it]  1%|â–         | 18/1260 [06:37<7:26:43, 21.58s/it]  2%|â–         | 19/1260 [06:59<7:28:24, 21.68s/it]  2%|â–         | 20/1260 [07:21<7:28:00, 21.68s/it]                                                   {'loss': 0.4393, 'grad_norm': 1.6057056396863965, 'learning_rate': 4.5238095238095235e-06, 'epoch': 0.32}
  2%|â–         | 20/1260 [07:21<7:28:00, 21.68s/it]  2%|â–         | 21/1260 [07:42<7:25:50, 21.59s/it]  2%|â–         | 22/1260 [08:04<7:26:19, 21.63s/it]  2%|â–         | 23/1260 [08:25<7:24:44, 21.57s/it]  2%|â–         | 24/1260 [08:47<7:23:45, 21.54s/it]  2%|â–         | 25/1260 [09:08<7:23:19, 21.54s/it]                                                   {'loss': 0.4152, 'grad_norm': 2.123718119969659, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.4}
  2%|â–         | 25/1260 [09:08<7:23:19, 21.54s/it]  2%|â–         | 26/1260 [09:30<7:24:05, 21.59s/it]  2%|â–         | 27/1260 [09:52<7:22:38, 21.54s/it]  2%|â–         | 28/1260 [10:13<7:22:35, 21.55s/it]  2%|â–         | 29/1260 [10:35<7:23:06, 21.60s/it]  2%|â–         | 30/1260 [10:57<7:23:03, 21.61s/it]                                                   {'loss': 0.3855, 'grad_norm': 1.9493893397269453, 'learning_rate': 6.904761904761905e-06, 'epoch': 0.48}
  2%|â–         | 30/1260 [10:57<7:23:03, 21.61s/it]  2%|â–         | 31/1260 [11:18<7:23:00, 21.63s/it]  3%|â–Ž         | 32/1260 [11:40<7:22:38, 21.63s/it]  3%|â–Ž         | 33/1260 [12:02<7:23:50, 21.70s/it]  3%|â–Ž         | 34/1260 [12:23<7:23:31, 21.71s/it]  3%|â–Ž         | 35/1260 [12:45<7:25:09, 21.80s/it]                                                   {'loss': 0.3687, 'grad_norm': 1.4093977732986964, 'learning_rate': 8.095238095238095e-06, 'epoch': 0.56}
  3%|â–Ž         | 35/1260 [12:45<7:25:09, 21.80s/it]  3%|â–Ž         | 36/1260 [13:07<7:22:20, 21.68s/it]  3%|â–Ž         | 37/1260 [13:28<7:20:54, 21.63s/it]  3%|â–Ž         | 38/1260 [13:50<7:18:53, 21.55s/it]  3%|â–Ž         | 39/1260 [14:11<7:19:46, 21.61s/it]  3%|â–Ž         | 40/1260 [14:33<7:20:13, 21.65s/it]                                                   {'loss': 0.352, 'grad_norm': 1.6574572126649483, 'learning_rate': 9.285714285714286e-06, 'epoch': 0.64}
  3%|â–Ž         | 40/1260 [14:33<7:20:13, 21.65s/it]  3%|â–Ž         | 41/1260 [14:55<7:18:21, 21.58s/it]  3%|â–Ž         | 42/1260 [15:16<7:16:36, 21.51s/it]  3%|â–Ž         | 43/1260 [15:38<7:17:04, 21.55s/it]  3%|â–Ž         | 44/1260 [15:59<7:16:03, 21.52s/it]  4%|â–Ž         | 45/1260 [16:21<7:16:08, 21.54s/it]                                                   {'loss': 0.3403, 'grad_norm': 1.311174593660107, 'learning_rate': 1.0476190476190475e-05, 'epoch': 0.72}
  4%|â–Ž         | 45/1260 [16:21<7:16:08, 21.54s/it]  4%|â–Ž         | 46/1260 [16:43<7:18:16, 21.66s/it]  4%|â–Ž         | 47/1260 [17:04<7:16:23, 21.59s/it]  4%|â–         | 48/1260 [17:26<7:16:32, 21.61s/it]  4%|â–         | 49/1260 [17:47<7:14:42, 21.54s/it]  4%|â–         | 50/1260 [18:09<7:13:59, 21.52s/it]                                                   {'loss': 0.3296, 'grad_norm': 1.4764062687729234, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.8}
  4%|â–         | 50/1260 [18:09<7:13:59, 21.52s/it]  4%|â–         | 51/1260 [18:30<7:13:54, 21.53s/it]  4%|â–         | 52/1260 [18:52<7:14:13, 21.57s/it]  4%|â–         | 53/1260 [19:13<7:13:43, 21.56s/it]  4%|â–         | 54/1260 [19:35<7:12:32, 21.52s/it]  4%|â–         | 55/1260 [19:56<7:13:10, 21.57s/it]                                                   {'loss': 0.3244, 'grad_norm': 1.0520267821620395, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.87}
  4%|â–         | 55/1260 [19:56<7:13:10, 21.57s/it]  4%|â–         | 56/1260 [20:18<7:11:55, 21.52s/it]  5%|â–         | 57/1260 [20:39<7:12:02, 21.55s/it]  5%|â–         | 58/1260 [21:01<7:11:21, 21.53s/it]  5%|â–         | 59/1260 [21:23<7:12:16, 21.60s/it]  5%|â–         | 60/1260 [21:44<7:11:31, 21.58s/it]                                                   {'loss': 0.3187, 'grad_norm': 0.9167573423841434, 'learning_rate': 1.4047619047619048e-05, 'epoch': 0.95}
  5%|â–         | 60/1260 [21:44<7:11:31, 21.58s/it]  5%|â–         | 61/1260 [22:06<7:12:02, 21.62s/it]  5%|â–         | 62/1260 [22:27<7:10:38, 21.57s/it]  5%|â–Œ         | 63/1260 [22:47<6:56:13, 20.86s/it][INFO|trainer.py:3993] 2025-07-04 21:55:56,083 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63
[INFO|configuration_utils.py:424] 2025-07-04 21:55:56,090 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/config.json
[INFO|configuration_utils.py:904] 2025-07-04 21:55:56,091 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 21:55:59,684 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 21:55:59,686 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 21:55:59,687 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 21:55:59,688 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/special_tokens_map.json
[2025-07-04 21:55:59,944] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step62 is about to be saved!
[2025-07-04 21:55:59,956] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 21:55:59,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 21:55:59,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 21:55:59,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 21:56:04,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 21:56:04,005] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 21:56:08,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 21:56:08,424 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 21:56:08,425 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 21:56:08,425 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 21:56:08,426 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 21:56:08,577 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 21:56:08,982 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/chat_template.jinja
  5%|â–Œ         | 64/1260 [23:25<8:39:14, 26.05s/it]  5%|â–Œ         | 65/1260 [23:47<8:14:40, 24.84s/it]                                                   {'loss': 0.3045, 'grad_norm': 0.9120216549649475, 'learning_rate': 1.5238095238095238e-05, 'epoch': 1.03}
  5%|â–Œ         | 65/1260 [23:47<8:14:40, 24.84s/it]  5%|â–Œ         | 66/1260 [24:08<7:53:55, 23.82s/it]  5%|â–Œ         | 67/1260 [24:30<7:39:42, 23.12s/it]  5%|â–Œ         | 68/1260 [24:51<7:29:14, 22.61s/it]  5%|â–Œ         | 69/1260 [25:13<7:23:58, 22.37s/it]  6%|â–Œ         | 70/1260 [25:35<7:21:26, 22.26s/it]                                                   {'loss': 0.3075, 'grad_norm': 1.2694674624937468, 'learning_rate': 1.6428571428571432e-05, 'epoch': 1.11}
  6%|â–Œ         | 70/1260 [25:35<7:21:26, 22.26s/it]  6%|â–Œ         | 71/1260 [25:56<7:16:32, 22.03s/it]  6%|â–Œ         | 72/1260 [26:18<7:14:30, 21.94s/it]  6%|â–Œ         | 73/1260 [26:40<7:11:06, 21.79s/it]  6%|â–Œ         | 74/1260 [27:01<7:08:47, 21.69s/it]  6%|â–Œ         | 75/1260 [27:22<7:06:50, 21.61s/it]                                                   {'loss': 0.305, 'grad_norm': 0.8223377255439821, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.19}
  6%|â–Œ         | 75/1260 [27:22<7:06:50, 21.61s/it]  6%|â–Œ         | 76/1260 [27:44<7:08:01, 21.69s/it]  6%|â–Œ         | 77/1260 [28:06<7:07:48, 21.70s/it]  6%|â–Œ         | 78/1260 [28:28<7:06:31, 21.65s/it]  6%|â–‹         | 79/1260 [28:49<7:06:58, 21.69s/it]  6%|â–‹         | 80/1260 [29:11<7:06:46, 21.70s/it]                                                   {'loss': 0.2991, 'grad_norm': 0.9644252027964425, 'learning_rate': 1.880952380952381e-05, 'epoch': 1.27}
  6%|â–‹         | 80/1260 [29:11<7:06:46, 21.70s/it]  6%|â–‹         | 81/1260 [29:33<7:08:07, 21.79s/it]  7%|â–‹         | 82/1260 [29:55<7:05:43, 21.68s/it]  7%|â–‹         | 83/1260 [30:16<7:05:36, 21.70s/it]  7%|â–‹         | 84/1260 [30:38<7:05:53, 21.73s/it]  7%|â–‹         | 85/1260 [31:00<7:06:26, 21.78s/it]                                                   {'loss': 0.2943, 'grad_norm': 0.7652299517021033, 'learning_rate': 1.9999999999999998e-05, 'epoch': 1.35}
  7%|â–‹         | 85/1260 [31:00<7:06:26, 21.78s/it]  7%|â–‹         | 86/1260 [31:22<7:06:38, 21.80s/it]  7%|â–‹         | 87/1260 [31:43<7:04:11, 21.70s/it]  7%|â–‹         | 88/1260 [32:05<7:02:10, 21.61s/it]  7%|â–‹         | 89/1260 [32:26<7:00:39, 21.55s/it]  7%|â–‹         | 90/1260 [32:48<6:59:44, 21.53s/it]                                                   {'loss': 0.2969, 'grad_norm': 1.2087348629409627, 'learning_rate': 2.1190476190476193e-05, 'epoch': 1.43}
  7%|â–‹         | 90/1260 [32:48<6:59:44, 21.53s/it]  7%|â–‹         | 91/1260 [33:09<7:00:05, 21.56s/it]  7%|â–‹         | 92/1260 [33:31<6:59:20, 21.54s/it]  7%|â–‹         | 93/1260 [33:52<6:58:40, 21.53s/it]  7%|â–‹         | 94/1260 [34:14<6:57:32, 21.49s/it]  8%|â–Š         | 95/1260 [34:35<6:56:45, 21.46s/it]                                                   {'loss': 0.2972, 'grad_norm': 0.787876176826647, 'learning_rate': 2.238095238095238e-05, 'epoch': 1.51}
  8%|â–Š         | 95/1260 [34:35<6:56:45, 21.46s/it]  8%|â–Š         | 96/1260 [34:56<6:56:37, 21.48s/it]  8%|â–Š         | 97/1260 [35:18<6:57:41, 21.55s/it]  8%|â–Š         | 98/1260 [35:40<6:57:51, 21.58s/it]  8%|â–Š         | 99/1260 [36:02<6:59:42, 21.69s/it]  8%|â–Š         | 100/1260 [36:23<6:58:06, 21.63s/it]                                                    {'loss': 0.2958, 'grad_norm': 0.6728618250542424, 'learning_rate': 2.357142857142857e-05, 'epoch': 1.59}
  8%|â–Š         | 100/1260 [36:23<6:58:06, 21.63s/it]  8%|â–Š         | 101/1260 [36:45<6:57:39, 21.62s/it]  8%|â–Š         | 102/1260 [37:07<6:57:09, 21.61s/it]  8%|â–Š         | 103/1260 [37:28<6:56:14, 21.59s/it]  8%|â–Š         | 104/1260 [37:50<6:55:44, 21.58s/it]  8%|â–Š         | 105/1260 [38:11<6:54:25, 21.53s/it]                                                    {'loss': 0.2912, 'grad_norm': 0.7645436578783035, 'learning_rate': 2.4761904761904762e-05, 'epoch': 1.67}
  8%|â–Š         | 105/1260 [38:11<6:54:25, 21.53s/it]  8%|â–Š         | 106/1260 [38:33<6:55:06, 21.58s/it]  8%|â–Š         | 107/1260 [38:54<6:55:50, 21.64s/it]  9%|â–Š         | 108/1260 [39:16<6:54:18, 21.58s/it]  9%|â–Š         | 109/1260 [39:38<6:54:22, 21.60s/it]  9%|â–Š         | 110/1260 [39:59<6:53:14, 21.56s/it]                                                    {'loss': 0.2953, 'grad_norm': 0.7406219521505913, 'learning_rate': 2.5952380952380953e-05, 'epoch': 1.75}
  9%|â–Š         | 110/1260 [39:59<6:53:14, 21.56s/it]  9%|â–‰         | 111/1260 [40:21<6:54:23, 21.64s/it]  9%|â–‰         | 112/1260 [40:43<6:58:03, 21.85s/it]  9%|â–‰         | 113/1260 [41:05<6:56:39, 21.80s/it]  9%|â–‰         | 114/1260 [41:27<6:57:01, 21.83s/it]  9%|â–‰         | 115/1260 [41:49<6:56:39, 21.83s/it]                                                    {'loss': 0.2927, 'grad_norm': 0.7690959704001166, 'learning_rate': 2.7142857142857144e-05, 'epoch': 1.83}
  9%|â–‰         | 115/1260 [41:49<6:56:39, 21.83s/it]  9%|â–‰         | 116/1260 [42:10<6:55:35, 21.80s/it]  9%|â–‰         | 117/1260 [42:32<6:53:28, 21.70s/it]  9%|â–‰         | 118/1260 [42:54<6:54:11, 21.76s/it]  9%|â–‰         | 119/1260 [43:15<6:53:30, 21.74s/it] 10%|â–‰         | 120/1260 [43:37<6:52:33, 21.71s/it]                                                    {'loss': 0.2927, 'grad_norm': 1.049381621861928, 'learning_rate': 2.8333333333333332e-05, 'epoch': 1.91}
 10%|â–‰         | 120/1260 [43:37<6:52:33, 21.71s/it] 10%|â–‰         | 121/1260 [43:59<6:52:49, 21.75s/it] 10%|â–‰         | 122/1260 [44:20<6:50:47, 21.66s/it] 10%|â–‰         | 123/1260 [44:42<6:49:47, 21.63s/it] 10%|â–‰         | 124/1260 [45:03<6:48:42, 21.59s/it] 10%|â–‰         | 125/1260 [45:25<6:47:07, 21.52s/it]                                                    {'loss': 0.2964, 'grad_norm': 1.0208689972078566, 'learning_rate': 2.9523809523809523e-05, 'epoch': 1.99}
 10%|â–‰         | 125/1260 [45:25<6:47:07, 21.52s/it] 10%|â–ˆ         | 126/1260 [45:44<6:36:03, 20.96s/it][INFO|trainer.py:3993] 2025-07-04 22:18:53,213 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126
[INFO|configuration_utils.py:424] 2025-07-04 22:18:53,224 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/config.json
[INFO|configuration_utils.py:904] 2025-07-04 22:18:53,225 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 22:18:56,151 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:18:56,152 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:18:56,153 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:18:56,154 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/special_tokens_map.json
[2025-07-04 22:18:56,349] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step125 is about to be saved!
[2025-07-04 22:18:56,360] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 22:18:56,360] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 22:18:56,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 22:18:56,403] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 22:19:00,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 22:19:00,565] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 22:19:04,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step125 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 22:19:04,686 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:19:04,686 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:19:04,687 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:19:04,687 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 22:19:04,843 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 22:19:05,239 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/chat_template.jinja
 10%|â–ˆ         | 127/1260 [46:21<8:05:10, 25.69s/it] 10%|â–ˆ         | 128/1260 [46:43<7:42:13, 24.50s/it] 10%|â–ˆ         | 129/1260 [47:05<7:26:52, 23.71s/it] 10%|â–ˆ         | 130/1260 [47:26<7:13:46, 23.03s/it]                                                    {'loss': 0.2793, 'grad_norm': 1.0493537206643597, 'learning_rate': 2.9999481946145652e-05, 'epoch': 2.06}
 10%|â–ˆ         | 130/1260 [47:26<7:13:46, 23.03s/it] 10%|â–ˆ         | 131/1260 [47:48<7:06:45, 22.68s/it] 10%|â–ˆ         | 132/1260 [48:10<7:02:25, 22.47s/it] 11%|â–ˆ         | 133/1260 [48:32<6:57:59, 22.25s/it] 11%|â–ˆ         | 134/1260 [48:53<6:54:10, 22.07s/it] 11%|â–ˆ         | 135/1260 [49:15<6:51:06, 21.93s/it]                                                    {'loss': 0.2869, 'grad_norm': 0.78965703110192, 'learning_rate': 2.9996316191067322e-05, 'epoch': 2.14}
 11%|â–ˆ         | 135/1260 [49:15<6:51:06, 21.93s/it] 11%|â–ˆ         | 136/1260 [49:37<6:48:33, 21.81s/it] 11%|â–ˆ         | 137/1260 [49:58<6:48:19, 21.82s/it] 11%|â–ˆ         | 138/1260 [50:20<6:45:57, 21.71s/it] 11%|â–ˆ         | 139/1260 [50:41<6:44:12, 21.63s/it] 11%|â–ˆ         | 140/1260 [51:03<6:42:53, 21.58s/it]                                                    {'loss': 0.2807, 'grad_norm': 1.2148245540027585, 'learning_rate': 2.999027309528412e-05, 'epoch': 2.22}
 11%|â–ˆ         | 140/1260 [51:03<6:42:53, 21.58s/it] 11%|â–ˆ         | 141/1260 [51:24<6:42:53, 21.60s/it] 11%|â–ˆâ–        | 142/1260 [51:46<6:42:57, 21.63s/it] 11%|â–ˆâ–        | 143/1260 [52:08<6:42:10, 21.60s/it] 11%|â–ˆâ–        | 144/1260 [52:30<6:45:50, 21.82s/it] 12%|â–ˆâ–        | 145/1260 [52:52<6:46:42, 21.89s/it]                                                    {'loss': 0.2872, 'grad_norm': 0.6886127588663299, 'learning_rate': 2.9981353818283835e-05, 'epoch': 2.3}
 12%|â–ˆâ–        | 145/1260 [52:52<6:46:42, 21.89s/it] 12%|â–ˆâ–        | 146/1260 [53:13<6:44:00, 21.76s/it] 12%|â–ˆâ–        | 147/1260 [53:35<6:41:31, 21.65s/it] 12%|â–ˆâ–        | 148/1260 [53:56<6:39:31, 21.56s/it] 12%|â–ˆâ–        | 149/1260 [54:18<6:39:19, 21.57s/it] 12%|â–ˆâ–        | 150/1260 [54:39<6:39:07, 21.57s/it]                                                    {'loss': 0.2831, 'grad_norm': 0.8935968116417802, 'learning_rate': 2.996956007140667e-05, 'epoch': 2.38}
 12%|â–ˆâ–        | 150/1260 [54:39<6:39:07, 21.57s/it] 12%|â–ˆâ–        | 151/1260 [55:01<6:37:54, 21.53s/it] 12%|â–ˆâ–        | 152/1260 [55:22<6:36:35, 21.48s/it] 12%|â–ˆâ–        | 153/1260 [55:44<6:36:45, 21.50s/it] 12%|â–ˆâ–        | 154/1260 [56:06<6:38:32, 21.62s/it] 12%|â–ˆâ–        | 155/1260 [56:27<6:36:55, 21.55s/it]                                                    {'loss': 0.2822, 'grad_norm': 0.9573224976882189, 'learning_rate': 2.995489411751688e-05, 'epoch': 2.46}
 12%|â–ˆâ–        | 155/1260 [56:27<6:36:55, 21.55s/it] 12%|â–ˆâ–        | 156/1260 [56:49<6:37:19, 21.59s/it] 12%|â–ˆâ–        | 157/1260 [57:10<6:36:08, 21.55s/it] 13%|â–ˆâ–Ž        | 158/1260 [57:32<6:35:19, 21.52s/it] 13%|â–ˆâ–Ž        | 159/1260 [57:53<6:34:20, 21.49s/it] 13%|â–ˆâ–Ž        | 160/1260 [58:15<6:34:32, 21.52s/it]                                                    {'loss': 0.2814, 'grad_norm': 0.7060491520381491, 'learning_rate': 2.9937358770568617e-05, 'epoch': 2.54}
 13%|â–ˆâ–Ž        | 160/1260 [58:15<6:34:32, 21.52s/it] 13%|â–ˆâ–Ž        | 161/1260 [58:36<6:35:03, 21.57s/it] 13%|â–ˆâ–Ž        | 162/1260 [58:58<6:34:05, 21.54s/it] 13%|â–ˆâ–Ž        | 163/1260 [59:19<6:33:09, 21.50s/it] 13%|â–ˆâ–Ž        | 164/1260 [59:41<6:32:42, 21.50s/it] 13%|â–ˆâ–Ž        | 165/1260 [1:00:02<6:32:11, 21.49s/it]                                                      {'loss': 0.2794, 'grad_norm': 0.7487247863061326, 'learning_rate': 2.9916957395065996e-05, 'epoch': 2.62}
 13%|â–ˆâ–Ž        | 165/1260 [1:00:02<6:32:11, 21.49s/it] 13%|â–ˆâ–Ž        | 166/1260 [1:00:24<6:31:34, 21.48s/it] 13%|â–ˆâ–Ž        | 167/1260 [1:00:45<6:31:17, 21.48s/it] 13%|â–ˆâ–Ž        | 168/1260 [1:01:06<6:30:26, 21.45s/it] 13%|â–ˆâ–Ž        | 169/1260 [1:01:28<6:30:48, 21.49s/it] 13%|â–ˆâ–Ž        | 170/1260 [1:01:50<6:31:48, 21.57s/it]                                                      {'loss': 0.2789, 'grad_norm': 1.127460001998081, 'learning_rate': 2.9893693905417548e-05, 'epoch': 2.7}
 13%|â–ˆâ–Ž        | 170/1260 [1:01:50<6:31:48, 21.57s/it] 14%|â–ˆâ–Ž        | 171/1260 [1:02:11<6:31:10, 21.55s/it] 14%|â–ˆâ–Ž        | 172/1260 [1:02:33<6:29:37, 21.49s/it] 14%|â–ˆâ–Ž        | 173/1260 [1:02:55<6:32:02, 21.64s/it] 14%|â–ˆâ–        | 174/1260 [1:03:16<6:30:28, 21.57s/it] 14%|â–ˆâ–        | 175/1260 [1:03:38<6:30:34, 21.60s/it]                                                      {'loss': 0.2764, 'grad_norm': 0.6625605228009044, 'learning_rate': 2.9867572765185192e-05, 'epoch': 2.78}
 14%|â–ˆâ–        | 175/1260 [1:03:38<6:30:34, 21.60s/it] 14%|â–ˆâ–        | 176/1260 [1:03:59<6:29:26, 21.56s/it] 14%|â–ˆâ–        | 177/1260 [1:04:21<6:28:17, 21.51s/it] 14%|â–ˆâ–        | 178/1260 [1:04:42<6:30:08, 21.63s/it] 14%|â–ˆâ–        | 179/1260 [1:05:04<6:28:26, 21.56s/it] 14%|â–ˆâ–        | 180/1260 [1:05:25<6:27:02, 21.50s/it]                                                      {'loss': 0.2775, 'grad_norm': 0.880113640400063, 'learning_rate': 2.9838598986227776e-05, 'epoch': 2.86}
 14%|â–ˆâ–        | 180/1260 [1:05:25<6:27:02, 21.50s/it] 14%|â–ˆâ–        | 181/1260 [1:05:47<6:26:30, 21.49s/it] 14%|â–ˆâ–        | 182/1260 [1:06:08<6:25:43, 21.47s/it] 15%|â–ˆâ–        | 183/1260 [1:06:30<6:26:35, 21.54s/it] 15%|â–ˆâ–        | 184/1260 [1:06:51<6:26:33, 21.55s/it] 15%|â–ˆâ–        | 185/1260 [1:07:13<6:25:27, 21.51s/it]                                                      {'loss': 0.275, 'grad_norm': 0.9473798627823503, 'learning_rate': 2.9806778127739467e-05, 'epoch': 2.94}
 15%|â–ˆâ–        | 185/1260 [1:07:13<6:25:27, 21.51s/it] 15%|â–ˆâ–        | 186/1260 [1:07:35<6:27:07, 21.63s/it] 15%|â–ˆâ–        | 187/1260 [1:07:56<6:25:51, 21.58s/it] 15%|â–ˆâ–        | 188/1260 [1:08:18<6:24:23, 21.51s/it] 15%|â–ˆâ–Œ        | 189/1260 [1:08:37<6:14:22, 20.97s/it][INFO|trainer.py:3993] 2025-07-04 22:41:46,376 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189
[INFO|configuration_utils.py:424] 2025-07-04 22:41:46,382 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/config.json
[INFO|configuration_utils.py:904] 2025-07-04 22:41:46,383 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 22:41:49,376 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:41:49,378 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:41:49,379 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:41:49,380 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/special_tokens_map.json
[2025-07-04 22:41:49,569] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step188 is about to be saved!
[2025-07-04 22:41:49,580] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 22:41:49,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 22:41:49,615] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 22:41:49,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 22:41:55,848] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 22:41:55,850] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 22:41:57,531] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step188 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 22:41:57,539 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:41:57,540 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:41:57,540 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:41:57,541 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 22:41:57,700 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 22:41:58,117 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/chat_template.jinja
 15%|â–ˆâ–Œ        | 190/1260 [1:09:14<7:38:32, 25.71s/it]                                                      {'loss': 0.2616, 'grad_norm': 0.679941789676124, 'learning_rate': 2.977211629518312e-05, 'epoch': 3.02}
 15%|â–ˆâ–Œ        | 190/1260 [1:09:14<7:38:32, 25.71s/it] 15%|â–ˆâ–Œ        | 191/1260 [1:09:36<7:17:23, 24.55s/it] 15%|â–ˆâ–Œ        | 192/1260 [1:09:58<7:02:45, 23.75s/it] 15%|â–ˆâ–Œ        | 193/1260 [1:10:19<6:49:45, 23.04s/it] 15%|â–ˆâ–Œ        | 194/1260 [1:10:41<6:40:38, 22.55s/it] 15%|â–ˆâ–Œ        | 195/1260 [1:11:02<6:35:08, 22.26s/it]                                                      {'loss': 0.2636, 'grad_norm': 0.8497277074597733, 'learning_rate': 2.9734620139118812e-05, 'epoch': 3.1}
 15%|â–ˆâ–Œ        | 195/1260 [1:11:02<6:35:08, 22.26s/it] 16%|â–ˆâ–Œ        | 196/1260 [1:11:24<6:32:58, 22.16s/it] 16%|â–ˆâ–Œ        | 197/1260 [1:11:45<6:28:49, 21.95s/it] 16%|â–ˆâ–Œ        | 198/1260 [1:12:07<6:26:04, 21.81s/it] 16%|â–ˆâ–Œ        | 199/1260 [1:12:28<6:23:21, 21.68s/it] 16%|â–ˆâ–Œ        | 200/1260 [1:12:50<6:22:55, 21.68s/it]                                                      {'loss': 0.2619, 'grad_norm': 0.8059330148051864, 'learning_rate': 2.9694296853927792e-05, 'epoch': 3.17}
 16%|â–ˆâ–Œ        | 200/1260 [1:12:50<6:22:55, 21.68s/it] 16%|â–ˆâ–Œ        | 201/1260 [1:13:12<6:22:12, 21.66s/it] 16%|â–ˆâ–Œ        | 202/1260 [1:13:33<6:22:54, 21.71s/it] 16%|â–ˆâ–Œ        | 203/1260 [1:13:55<6:21:40, 21.67s/it] 16%|â–ˆâ–Œ        | 204/1260 [1:14:17<6:23:53, 21.81s/it] 16%|â–ˆâ–‹        | 205/1260 [1:14:39<6:21:55, 21.72s/it]                                                      {'loss': 0.262, 'grad_norm': 0.9517459409354162, 'learning_rate': 2.965115417643212e-05, 'epoch': 3.25}
 16%|â–ˆâ–‹        | 205/1260 [1:14:39<6:21:55, 21.72s/it] 16%|â–ˆâ–‹        | 206/1260 [1:15:00<6:19:53, 21.63s/it] 16%|â–ˆâ–‹        | 207/1260 [1:15:22<6:20:17, 21.67s/it] 17%|â–ˆâ–‹        | 208/1260 [1:15:43<6:19:06, 21.62s/it] 17%|â–ˆâ–‹        | 209/1260 [1:16:05<6:18:49, 21.63s/it] 17%|â–ˆâ–‹        | 210/1260 [1:16:26<6:16:59, 21.54s/it]                                                      {'loss': 0.2599, 'grad_norm': 1.2524679740650577, 'learning_rate': 2.960520038441018e-05, 'epoch': 3.33}
 17%|â–ˆâ–‹        | 210/1260 [1:16:26<6:16:59, 21.54s/it] 17%|â–ˆâ–‹        | 211/1260 [1:16:48<6:16:15, 21.52s/it] 17%|â–ˆâ–‹        | 212/1260 [1:17:09<6:16:22, 21.55s/it] 17%|â–ˆâ–‹        | 213/1260 [1:17:31<6:16:30, 21.58s/it] 17%|â–ˆâ–‹        | 214/1260 [1:17:53<6:15:24, 21.53s/it] 17%|â–ˆâ–‹        | 215/1260 [1:18:14<6:14:25, 21.50s/it]                                                      {'loss': 0.2624, 'grad_norm': 0.9039549243520806, 'learning_rate': 2.9556444295008444e-05, 'epoch': 3.41}
 17%|â–ˆâ–‹        | 215/1260 [1:18:14<6:14:25, 21.50s/it] 17%|â–ˆâ–‹        | 216/1260 [1:18:36<6:16:36, 21.64s/it] 17%|â–ˆâ–‹        | 217/1260 [1:18:57<6:15:21, 21.59s/it] 17%|â–ˆâ–‹        | 218/1260 [1:19:19<6:14:31, 21.57s/it] 17%|â–ˆâ–‹        | 219/1260 [1:19:40<6:13:56, 21.55s/it] 17%|â–ˆâ–‹        | 220/1260 [1:20:03<6:17:11, 21.76s/it]                                                      {'loss': 0.2623, 'grad_norm': 0.722210687455425, 'learning_rate': 2.950489526304971e-05, 'epoch': 3.49}
 17%|â–ˆâ–‹        | 220/1260 [1:20:03<6:17:11, 21.76s/it] 18%|â–ˆâ–Š        | 221/1260 [1:20:24<6:16:59, 21.77s/it] 18%|â–ˆâ–Š        | 222/1260 [1:20:46<6:14:58, 21.68s/it] 18%|â–ˆâ–Š        | 223/1260 [1:21:07<6:13:47, 21.63s/it] 18%|â–ˆâ–Š        | 224/1260 [1:21:29<6:13:48, 21.65s/it] 18%|â–ˆâ–Š        | 225/1260 [1:21:51<6:12:25, 21.59s/it]                                                      {'loss': 0.2616, 'grad_norm': 0.6954058487618087, 'learning_rate': 2.9450563179238207e-05, 'epoch': 3.57}
 18%|â–ˆâ–Š        | 225/1260 [1:21:51<6:12:25, 21.59s/it] 18%|â–ˆâ–Š        | 226/1260 [1:22:12<6:12:00, 21.59s/it] 18%|â–ˆâ–Š        | 227/1260 [1:22:34<6:10:30, 21.52s/it] 18%|â–ˆâ–Š        | 228/1260 [1:22:55<6:09:55, 21.51s/it] 18%|â–ˆâ–Š        | 229/1260 [1:23:17<6:09:39, 21.51s/it] 18%|â–ˆâ–Š        | 230/1260 [1:23:38<6:08:50, 21.49s/it]                                                      {'loss': 0.265, 'grad_norm': 0.7933492058673592, 'learning_rate': 2.939345846826186e-05, 'epoch': 3.65}
 18%|â–ˆâ–Š        | 230/1260 [1:23:38<6:08:50, 21.49s/it] 18%|â–ˆâ–Š        | 231/1260 [1:24:00<6:11:05, 21.64s/it] 18%|â–ˆâ–Š        | 232/1260 [1:24:22<6:11:28, 21.68s/it] 18%|â–ˆâ–Š        | 233/1260 [1:24:44<6:12:34, 21.77s/it] 19%|â–ˆâ–Š        | 234/1260 [1:25:05<6:10:20, 21.66s/it] 19%|â–ˆâ–Š        | 235/1260 [1:25:27<6:10:43, 21.70s/it]                                                      {'loss': 0.2642, 'grad_norm': 0.722740209503702, 'learning_rate': 2.9333592086792113e-05, 'epoch': 3.73}
 19%|â–ˆâ–Š        | 235/1260 [1:25:27<6:10:43, 21.70s/it] 19%|â–ˆâ–Š        | 236/1260 [1:25:49<6:10:48, 21.73s/it] 19%|â–ˆâ–‰        | 237/1260 [1:26:10<6:09:47, 21.69s/it] 19%|â–ˆâ–‰        | 238/1260 [1:26:32<6:10:14, 21.74s/it] 19%|â–ˆâ–‰        | 239/1260 [1:26:54<6:08:13, 21.64s/it] 19%|â–ˆâ–‰        | 240/1260 [1:27:15<6:07:04, 21.59s/it]                                                      {'loss': 0.2634, 'grad_norm': 0.7708739749407736, 'learning_rate': 2.927097552138166e-05, 'epoch': 3.81}
 19%|â–ˆâ–‰        | 240/1260 [1:27:15<6:07:04, 21.59s/it] 19%|â–ˆâ–‰        | 241/1260 [1:27:37<6:06:07, 21.56s/it] 19%|â–ˆâ–‰        | 242/1260 [1:27:58<6:06:17, 21.59s/it] 19%|â–ˆâ–‰        | 243/1260 [1:28:20<6:04:57, 21.53s/it] 19%|â–ˆâ–‰        | 244/1260 [1:28:41<6:06:15, 21.63s/it] 19%|â–ˆâ–‰        | 245/1260 [1:29:03<6:05:03, 21.58s/it]                                                      {'loss': 0.2659, 'grad_norm': 0.7619385823986534, 'learning_rate': 2.920562078626055e-05, 'epoch': 3.89}
 19%|â–ˆâ–‰        | 245/1260 [1:29:03<6:05:03, 21.58s/it] 20%|â–ˆâ–‰        | 246/1260 [1:29:25<6:05:04, 21.60s/it] 20%|â–ˆâ–‰        | 247/1260 [1:29:46<6:04:59, 21.62s/it] 20%|â–ˆâ–‰        | 248/1260 [1:30:08<6:04:27, 21.61s/it] 20%|â–ˆâ–‰        | 249/1260 [1:30:29<6:03:28, 21.57s/it] 20%|â–ˆâ–‰        | 250/1260 [1:30:51<6:02:14, 21.52s/it]                                                      {'loss': 0.2673, 'grad_norm': 1.0456368676354992, 'learning_rate': 2.9137540421030987e-05, 'epoch': 3.97}
 20%|â–ˆâ–‰        | 250/1260 [1:30:51<6:02:14, 21.52s/it] 20%|â–ˆâ–‰        | 251/1260 [1:31:12<6:01:40, 21.51s/it] 20%|â–ˆâ–ˆ        | 252/1260 [1:31:32<5:51:13, 20.91s/it][INFO|trainer.py:3993] 2025-07-04 23:04:40,597 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252
[INFO|configuration_utils.py:424] 2025-07-04 23:04:40,603 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:04:40,604 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:04:43,286 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:04:43,288 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:04:43,289 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:04:43,289 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/special_tokens_map.json
[2025-07-04 23:04:43,486] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step251 is about to be saved!
[2025-07-04 23:04:43,497] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:04:43,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:04:43,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:04:43,532] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:04:47,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:04:47,649] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:04:51,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:04:51,481 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:04:51,482 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:04:51,482 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:04:51,482 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:04:51,640 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:04:52,046 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/chat_template.jinja
 20%|â–ˆâ–ˆ        | 253/1260 [1:32:08<7:08:00, 25.50s/it] 20%|â–ˆâ–ˆ        | 254/1260 [1:32:29<6:47:52, 24.33s/it] 20%|â–ˆâ–ˆ        | 255/1260 [1:32:51<6:33:03, 23.47s/it]                                                      {'loss': 0.245, 'grad_norm': 1.3018315855692388, 'learning_rate': 2.9066747488261378e-05, 'epoch': 4.05}
 20%|â–ˆâ–ˆ        | 255/1260 [1:32:51<6:33:03, 23.47s/it] 20%|â–ˆâ–ˆ        | 256/1260 [1:33:13<6:24:05, 22.95s/it] 20%|â–ˆâ–ˆ        | 257/1260 [1:33:34<6:16:53, 22.55s/it] 20%|â–ˆâ–ˆ        | 258/1260 [1:33:56<6:11:05, 22.22s/it] 21%|â–ˆâ–ˆ        | 259/1260 [1:34:17<6:07:42, 22.04s/it] 21%|â–ˆâ–ˆ        | 260/1260 [1:34:40<6:07:55, 22.08s/it]                                                      {'loss': 0.248, 'grad_norm': 1.2550648335625023, 'learning_rate': 2.899325557098001e-05, 'epoch': 4.13}
 21%|â–ˆâ–ˆ        | 260/1260 [1:34:40<6:07:55, 22.08s/it] 21%|â–ˆâ–ˆ        | 261/1260 [1:35:01<6:05:26, 21.95s/it] 21%|â–ˆâ–ˆ        | 262/1260 [1:35:22<6:01:57, 21.76s/it] 21%|â–ˆâ–ˆ        | 263/1260 [1:35:45<6:02:55, 21.84s/it] 21%|â–ˆâ–ˆ        | 264/1260 [1:36:06<6:00:56, 21.74s/it] 21%|â–ˆâ–ˆ        | 265/1260 [1:36:28<5:59:21, 21.67s/it]                                                      {'loss': 0.2473, 'grad_norm': 0.8447402394266893, 'learning_rate': 2.8917078770068882e-05, 'epoch': 4.21}
 21%|â–ˆâ–ˆ        | 265/1260 [1:36:28<5:59:21, 21.67s/it] 21%|â–ˆâ–ˆ        | 266/1260 [1:36:49<5:57:24, 21.57s/it] 21%|â–ˆâ–ˆ        | 267/1260 [1:37:10<5:57:12, 21.58s/it] 21%|â–ˆâ–ˆâ–       | 268/1260 [1:37:32<5:55:53, 21.53s/it] 21%|â–ˆâ–ˆâ–       | 269/1260 [1:37:53<5:55:26, 21.52s/it] 21%|â–ˆâ–ˆâ–       | 270/1260 [1:38:15<5:54:48, 21.50s/it]                                                      {'loss': 0.2474, 'grad_norm': 1.1877250052890442, 'learning_rate': 2.8838231701558182e-05, 'epoch': 4.29}
 21%|â–ˆâ–ˆâ–       | 270/1260 [1:38:15<5:54:48, 21.50s/it] 22%|â–ˆâ–ˆâ–       | 271/1260 [1:38:37<5:55:13, 21.55s/it] 22%|â–ˆâ–ˆâ–       | 272/1260 [1:38:58<5:55:41, 21.60s/it] 22%|â–ˆâ–ˆâ–       | 273/1260 [1:39:20<5:55:33, 21.61s/it] 22%|â–ˆâ–ˆâ–       | 274/1260 [1:39:42<5:55:37, 21.64s/it] 22%|â–ˆâ–ˆâ–       | 275/1260 [1:40:03<5:54:09, 21.57s/it]                                                      {'loss': 0.2474, 'grad_norm': 0.8973411903838817, 'learning_rate': 2.8756729493821883e-05, 'epoch': 4.37}
 22%|â–ˆâ–ˆâ–       | 275/1260 [1:40:03<5:54:09, 21.57s/it] 22%|â–ˆâ–ˆâ–       | 276/1260 [1:40:24<5:52:53, 21.52s/it] 22%|â–ˆâ–ˆâ–       | 277/1260 [1:40:46<5:52:32, 21.52s/it] 22%|â–ˆâ–ˆâ–       | 278/1260 [1:41:07<5:52:10, 21.52s/it] 22%|â–ˆâ–ˆâ–       | 279/1260 [1:41:29<5:50:53, 21.46s/it] 22%|â–ˆâ–ˆâ–       | 280/1260 [1:41:50<5:51:34, 21.52s/it]                                                      {'loss': 0.2461, 'grad_norm': 0.991855794159526, 'learning_rate': 2.8672587784675098e-05, 'epoch': 4.45}
 22%|â–ˆâ–ˆâ–       | 280/1260 [1:41:50<5:51:34, 21.52s/it] 22%|â–ˆâ–ˆâ–       | 281/1260 [1:42:12<5:50:55, 21.51s/it] 22%|â–ˆâ–ˆâ–       | 282/1260 [1:42:33<5:50:03, 21.48s/it] 22%|â–ˆâ–ˆâ–       | 283/1260 [1:42:55<5:49:09, 21.44s/it] 23%|â–ˆâ–ˆâ–Ž       | 284/1260 [1:43:16<5:48:33, 21.43s/it] 23%|â–ˆâ–ˆâ–Ž       | 285/1260 [1:43:38<5:48:36, 21.45s/it]                                                      {'loss': 0.2487, 'grad_norm': 1.2419245115694533, 'learning_rate': 2.8585822718373623e-05, 'epoch': 4.52}
 23%|â–ˆâ–ˆâ–Ž       | 285/1260 [1:43:38<5:48:36, 21.45s/it] 23%|â–ˆâ–ˆâ–Ž       | 286/1260 [1:43:59<5:50:05, 21.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 287/1260 [1:44:21<5:49:06, 21.53s/it] 23%|â–ˆâ–ˆâ–Ž       | 288/1260 [1:44:43<5:50:48, 21.65s/it] 23%|â–ˆâ–ˆâ–Ž       | 289/1260 [1:45:04<5:48:56, 21.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 290/1260 [1:45:26<5:49:41, 21.63s/it]                                                      {'loss': 0.255, 'grad_norm': 1.1476869097510016, 'learning_rate': 2.8496450942516373e-05, 'epoch': 4.6}
 23%|â–ˆâ–ˆâ–Ž       | 290/1260 [1:45:26<5:49:41, 21.63s/it] 23%|â–ˆâ–ˆâ–Ž       | 291/1260 [1:45:47<5:48:25, 21.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 292/1260 [1:46:09<5:47:48, 21.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 293/1260 [1:46:31<5:48:40, 21.63s/it] 23%|â–ˆâ–ˆâ–Ž       | 294/1260 [1:46:52<5:48:09, 21.62s/it] 23%|â–ˆâ–ˆâ–Ž       | 295/1260 [1:47:14<5:46:49, 21.56s/it]                                                      {'loss': 0.2485, 'grad_norm': 0.7109875117846354, 'learning_rate': 2.8404489604851186e-05, 'epoch': 4.68}
 23%|â–ˆâ–ˆâ–Ž       | 295/1260 [1:47:14<5:46:49, 21.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 296/1260 [1:47:36<5:47:38, 21.64s/it] 24%|â–ˆâ–ˆâ–Ž       | 297/1260 [1:47:57<5:45:47, 21.54s/it] 24%|â–ˆâ–ˆâ–Ž       | 298/1260 [1:48:18<5:44:22, 21.48s/it] 24%|â–ˆâ–ˆâ–Ž       | 299/1260 [1:48:40<5:44:48, 21.53s/it] 24%|â–ˆâ–ˆâ–       | 300/1260 [1:49:01<5:44:59, 21.56s/it]                                                      {'loss': 0.2482, 'grad_norm': 0.8626430230207355, 'learning_rate': 2.8309956349984684e-05, 'epoch': 4.76}
 24%|â–ˆâ–ˆâ–       | 300/1260 [1:49:01<5:44:59, 21.56s/it] 24%|â–ˆâ–ˆâ–       | 301/1260 [1:49:23<5:43:31, 21.49s/it] 24%|â–ˆâ–ˆâ–       | 302/1260 [1:49:44<5:42:28, 21.45s/it] 24%|â–ˆâ–ˆâ–       | 303/1260 [1:50:06<5:43:15, 21.52s/it] 24%|â–ˆâ–ˆâ–       | 304/1260 [1:50:27<5:43:06, 21.53s/it] 24%|â–ˆâ–ˆâ–       | 305/1260 [1:50:49<5:42:33, 21.52s/it]                                                      {'loss': 0.2585, 'grad_norm': 1.3087160709636732, 'learning_rate': 2.821286931599684e-05, 'epoch': 4.84}
 24%|â–ˆâ–ˆâ–       | 305/1260 [1:50:49<5:42:33, 21.52s/it] 24%|â–ˆâ–ˆâ–       | 306/1260 [1:51:10<5:41:53, 21.50s/it] 24%|â–ˆâ–ˆâ–       | 307/1260 [1:51:33<5:44:44, 21.71s/it] 24%|â–ˆâ–ˆâ–       | 308/1260 [1:51:54<5:43:05, 21.62s/it] 25%|â–ˆâ–ˆâ–       | 309/1260 [1:52:15<5:41:44, 21.56s/it] 25%|â–ˆâ–ˆâ–       | 310/1260 [1:52:37<5:41:04, 21.54s/it]                                                      {'loss': 0.2501, 'grad_norm': 0.973244124863102, 'learning_rate': 2.8113247130960783e-05, 'epoch': 4.92}
 25%|â–ˆâ–ˆâ–       | 310/1260 [1:52:37<5:41:04, 21.54s/it] 25%|â–ˆâ–ˆâ–       | 311/1260 [1:52:58<5:40:00, 21.50s/it] 25%|â–ˆâ–ˆâ–       | 312/1260 [1:53:20<5:39:18, 21.48s/it] 25%|â–ˆâ–ˆâ–       | 313/1260 [1:53:42<5:40:48, 21.59s/it] 25%|â–ˆâ–ˆâ–       | 314/1260 [1:54:03<5:40:42, 21.61s/it] 25%|â–ˆâ–ˆâ–Œ       | 315/1260 [1:54:23<5:29:32, 20.92s/it]                                                      {'loss': 0.2422, 'grad_norm': 1.2770022255763824, 'learning_rate': 2.801110890936867e-05, 'epoch': 5.0}
 25%|â–ˆâ–ˆâ–Œ       | 315/1260 [1:54:23<5:29:32, 20.92s/it][INFO|trainer.py:3993] 2025-07-04 23:27:31,543 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315
[INFO|configuration_utils.py:424] 2025-07-04 23:27:31,549 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:27:31,550 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:27:34,380 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:27:34,381 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:27:34,382 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:27:34,383 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/special_tokens_map.json
[2025-07-04 23:27:34,576] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step314 is about to be saved!
[2025-07-04 23:27:34,587] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:27:34,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:27:34,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:27:34,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:27:38,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:27:38,559] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:27:42,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step314 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:27:42,569 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:27:42,569 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:27:42,570 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:27:42,570 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:27:42,726 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:27:43,126 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/chat_template.jinja
 25%|â–ˆâ–ˆâ–Œ       | 316/1260 [1:54:59<6:42:01, 25.55s/it] 25%|â–ˆâ–ˆâ–Œ       | 317/1260 [1:55:21<6:23:56, 24.43s/it] 25%|â–ˆâ–ˆâ–Œ       | 318/1260 [1:55:42<6:10:36, 23.61s/it] 25%|â–ˆâ–ˆâ–Œ       | 319/1260 [1:56:04<6:01:29, 23.05s/it] 25%|â–ˆâ–ˆâ–Œ       | 320/1260 [1:56:25<5:53:05, 22.54s/it]                                                      {'loss': 0.2334, 'grad_norm': 1.1583649785947527, 'learning_rate': 2.790647424846417e-05, 'epoch': 5.08}
 25%|â–ˆâ–ˆâ–Œ       | 320/1260 [1:56:25<5:53:05, 22.54s/it] 25%|â–ˆâ–ˆâ–Œ       | 321/1260 [1:56:47<5:48:41, 22.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 322/1260 [1:57:09<5:44:52, 22.06s/it] 26%|â–ˆâ–ˆâ–Œ       | 323/1260 [1:57:30<5:41:04, 21.84s/it] 26%|â–ˆâ–ˆâ–Œ       | 324/1260 [1:57:52<5:39:08, 21.74s/it] 26%|â–ˆâ–ˆâ–Œ       | 325/1260 [1:58:13<5:39:17, 21.77s/it]                                                      {'loss': 0.2336, 'grad_norm': 0.8627413698104995, 'learning_rate': 2.7799363224482334e-05, 'epoch': 5.16}
 26%|â–ˆâ–ˆâ–Œ       | 325/1260 [1:58:13<5:39:17, 21.77s/it] 26%|â–ˆâ–ˆâ–Œ       | 326/1260 [1:58:35<5:37:50, 21.70s/it] 26%|â–ˆâ–ˆâ–Œ       | 327/1260 [1:58:56<5:36:53, 21.67s/it] 26%|â–ˆâ–ˆâ–Œ       | 328/1260 [1:59:18<5:35:58, 21.63s/it] 26%|â–ˆâ–ˆâ–Œ       | 329/1260 [1:59:39<5:34:30, 21.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 330/1260 [2:00:01<5:34:07, 21.56s/it]                                                      {'loss': 0.2333, 'grad_norm': 1.3904474771871402, 'learning_rate': 2.7689796388797616e-05, 'epoch': 5.24}
 26%|â–ˆâ–ˆâ–Œ       | 330/1260 [2:00:01<5:34:07, 21.56s/it] 26%|â–ˆâ–ˆâ–‹       | 331/1260 [2:00:22<5:33:25, 21.53s/it] 26%|â–ˆâ–ˆâ–‹       | 332/1260 [2:00:44<5:34:46, 21.65s/it] 26%|â–ˆâ–ˆâ–‹       | 333/1260 [2:01:06<5:33:46, 21.60s/it] 27%|â–ˆâ–ˆâ–‹       | 334/1260 [2:01:28<5:33:44, 21.63s/it] 27%|â–ˆâ–ˆâ–‹       | 335/1260 [2:01:49<5:34:49, 21.72s/it]                                                      {'loss': 0.2328, 'grad_norm': 0.8392310400998947, 'learning_rate': 2.7577794763980634e-05, 'epoch': 5.32}
 27%|â–ˆâ–ˆâ–‹       | 335/1260 [2:01:49<5:34:49, 21.72s/it] 27%|â–ˆâ–ˆâ–‹       | 336/1260 [2:02:11<5:33:28, 21.65s/it] 27%|â–ˆâ–ˆâ–‹       | 337/1260 [2:02:32<5:31:37, 21.56s/it] 27%|â–ˆâ–ˆâ–‹       | 338/1260 [2:02:54<5:32:05, 21.61s/it] 27%|â–ˆâ–ˆâ–‹       | 339/1260 [2:03:16<5:32:02, 21.63s/it] 27%|â–ˆâ–ˆâ–‹       | 340/1260 [2:03:37<5:30:34, 21.56s/it]                                                      {'loss': 0.231, 'grad_norm': 0.7507553954520342, 'learning_rate': 2.7463379839764596e-05, 'epoch': 5.4}
 27%|â–ˆâ–ˆâ–‹       | 340/1260 [2:03:37<5:30:34, 21.56s/it] 27%|â–ˆâ–ˆâ–‹       | 341/1260 [2:03:59<5:30:59, 21.61s/it] 27%|â–ˆâ–ˆâ–‹       | 342/1260 [2:04:20<5:30:19, 21.59s/it] 27%|â–ˆâ–ˆâ–‹       | 343/1260 [2:04:42<5:28:47, 21.51s/it] 27%|â–ˆâ–ˆâ–‹       | 344/1260 [2:05:03<5:27:59, 21.48s/it] 27%|â–ˆâ–ˆâ–‹       | 345/1260 [2:05:25<5:29:38, 21.62s/it]                                                      {'loss': 0.2322, 'grad_norm': 1.1382721695480924, 'learning_rate': 2.734657356892208e-05, 'epoch': 5.48}
 27%|â–ˆâ–ˆâ–‹       | 345/1260 [2:05:25<5:29:38, 21.62s/it] 27%|â–ˆâ–ˆâ–‹       | 346/1260 [2:05:47<5:29:40, 21.64s/it] 28%|â–ˆâ–ˆâ–Š       | 347/1260 [2:06:08<5:28:23, 21.58s/it] 28%|â–ˆâ–ˆâ–Š       | 348/1260 [2:06:30<5:27:07, 21.52s/it] 28%|â–ˆâ–ˆâ–Š       | 349/1260 [2:06:51<5:26:16, 21.49s/it] 28%|â–ˆâ–ˆâ–Š       | 350/1260 [2:07:12<5:25:38, 21.47s/it]                                                      {'loss': 0.236, 'grad_norm': 1.161713898557078, 'learning_rate': 2.7227398363052918e-05, 'epoch': 5.56}
 28%|â–ˆâ–ˆâ–Š       | 350/1260 [2:07:12<5:25:38, 21.47s/it] 28%|â–ˆâ–ˆâ–Š       | 351/1260 [2:07:34<5:25:09, 21.46s/it] 28%|â–ˆâ–ˆâ–Š       | 352/1260 [2:07:56<5:26:15, 21.56s/it] 28%|â–ˆâ–ˆâ–Š       | 353/1260 [2:08:17<5:26:55, 21.63s/it] 28%|â–ˆâ–ˆâ–Š       | 354/1260 [2:08:39<5:26:40, 21.63s/it] 28%|â–ˆâ–ˆâ–Š       | 355/1260 [2:09:01<5:27:18, 21.70s/it]                                                      {'loss': 0.2342, 'grad_norm': 1.239437879835769, 'learning_rate': 2.710587708828414e-05, 'epoch': 5.64}
 28%|â–ˆâ–ˆâ–Š       | 355/1260 [2:09:01<5:27:18, 21.70s/it] 28%|â–ˆâ–ˆâ–Š       | 356/1260 [2:09:23<5:26:41, 21.68s/it] 28%|â–ˆâ–ˆâ–Š       | 357/1260 [2:09:45<5:27:20, 21.75s/it] 28%|â–ˆâ–ˆâ–Š       | 358/1260 [2:10:06<5:27:16, 21.77s/it] 28%|â–ˆâ–ˆâ–Š       | 359/1260 [2:10:28<5:27:35, 21.82s/it] 29%|â–ˆâ–ˆâ–Š       | 360/1260 [2:10:50<5:26:31, 21.77s/it]                                                      {'loss': 0.2359, 'grad_norm': 0.9863099781319665, 'learning_rate': 2.698203306088262e-05, 'epoch': 5.72}
 29%|â–ˆâ–ˆâ–Š       | 360/1260 [2:10:50<5:26:31, 21.77s/it] 29%|â–ˆâ–ˆâ–Š       | 361/1260 [2:11:11<5:24:58, 21.69s/it] 29%|â–ˆâ–ˆâ–Š       | 362/1260 [2:11:33<5:23:21, 21.61s/it] 29%|â–ˆâ–ˆâ–‰       | 363/1260 [2:11:54<5:21:51, 21.53s/it] 29%|â–ˆâ–ˆâ–‰       | 364/1260 [2:12:16<5:21:10, 21.51s/it] 29%|â–ˆâ–ˆâ–‰       | 365/1260 [2:12:37<5:21:32, 21.56s/it]                                                      {'loss': 0.235, 'grad_norm': 1.4297024950969144, 'learning_rate': 2.685589004278139e-05, 'epoch': 5.8}
 29%|â–ˆâ–ˆâ–‰       | 365/1260 [2:12:37<5:21:32, 21.56s/it] 29%|â–ˆâ–ˆâ–‰       | 366/1260 [2:12:59<5:22:13, 21.63s/it] 29%|â–ˆâ–ˆâ–‰       | 367/1260 [2:13:21<5:22:08, 21.64s/it] 29%|â–ˆâ–ˆâ–‰       | 368/1260 [2:13:42<5:21:47, 21.65s/it] 29%|â–ˆâ–ˆâ–‰       | 369/1260 [2:14:04<5:21:44, 21.67s/it] 29%|â–ˆâ–ˆâ–‰       | 370/1260 [2:14:26<5:20:15, 21.59s/it]                                                      {'loss': 0.2407, 'grad_norm': 0.7512784098627535, 'learning_rate': 2.672747223702045e-05, 'epoch': 5.87}
 29%|â–ˆâ–ˆâ–‰       | 370/1260 [2:14:26<5:20:15, 21.59s/it] 29%|â–ˆâ–ˆâ–‰       | 371/1260 [2:14:47<5:20:02, 21.60s/it] 30%|â–ˆâ–ˆâ–‰       | 372/1260 [2:15:09<5:19:29, 21.59s/it] 30%|â–ˆâ–ˆâ–‰       | 373/1260 [2:15:30<5:18:41, 21.56s/it] 30%|â–ˆâ–ˆâ–‰       | 374/1260 [2:15:52<5:18:14, 21.55s/it] 30%|â–ˆâ–ˆâ–‰       | 375/1260 [2:16:13<5:18:24, 21.59s/it]                                                      {'loss': 0.2369, 'grad_norm': 0.8136884241256329, 'learning_rate': 2.6596804283102928e-05, 'epoch': 5.95}
 30%|â–ˆâ–ˆâ–‰       | 375/1260 [2:16:13<5:18:24, 21.59s/it] 30%|â–ˆâ–ˆâ–‰       | 376/1260 [2:16:35<5:17:35, 21.56s/it] 30%|â–ˆâ–ˆâ–‰       | 377/1260 [2:16:57<5:17:49, 21.60s/it] 30%|â–ˆâ–ˆâ–ˆ       | 378/1260 [2:17:16<5:07:16, 20.90s/it][INFO|trainer.py:3993] 2025-07-04 23:50:25,036 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378
[INFO|configuration_utils.py:424] 2025-07-04 23:50:25,042 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:50:25,043 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:50:27,950 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:50:27,952 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:50:27,953 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:50:27,953 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/special_tokens_map.json
[2025-07-04 23:50:28,137] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step377 is about to be saved!
[2025-07-04 23:50:28,148] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:50:28,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:50:28,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:50:28,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:50:34,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:50:34,532] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:50:36,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step377 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:50:36,111 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:50:36,111 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:50:36,112 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:50:36,113 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:50:36,270 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:50:36,678 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/chat_template.jinja
 30%|â–ˆâ–ˆâ–ˆ       | 379/1260 [2:17:53<6:16:30, 25.64s/it] 30%|â–ˆâ–ˆâ–ˆ       | 380/1260 [2:18:14<5:57:21, 24.37s/it]                                                      {'loss': 0.2233, 'grad_norm': 0.8067261983413969, 'learning_rate': 2.646391125226751e-05, 'epoch': 6.03}
 30%|â–ˆâ–ˆâ–ˆ       | 380/1260 [2:18:14<5:57:21, 24.37s/it] 30%|â–ˆâ–ˆâ–ˆ       | 381/1260 [2:18:35<5:44:15, 23.50s/it] 30%|â–ˆâ–ˆâ–ˆ       | 382/1260 [2:18:57<5:34:55, 22.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 383/1260 [2:19:18<5:28:49, 22.50s/it] 30%|â–ˆâ–ˆâ–ˆ       | 384/1260 [2:19:40<5:23:50, 22.18s/it] 31%|â–ˆâ–ˆâ–ˆ       | 385/1260 [2:20:01<5:20:12, 21.96s/it]                                                      {'loss': 0.2203, 'grad_norm': 1.686697771505321, 'learning_rate': 2.6328818642678026e-05, 'epoch': 6.11}
 31%|â–ˆâ–ˆâ–ˆ       | 385/1260 [2:20:01<5:20:12, 21.96s/it] 31%|â–ˆâ–ˆâ–ˆ       | 386/1260 [2:20:23<5:17:34, 21.80s/it] 31%|â–ˆâ–ˆâ–ˆ       | 387/1260 [2:20:44<5:16:17, 21.74s/it] 31%|â–ˆâ–ˆâ–ˆ       | 388/1260 [2:21:06<5:14:13, 21.62s/it] 31%|â–ˆâ–ˆâ–ˆ       | 389/1260 [2:21:27<5:12:56, 21.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 390/1260 [2:21:49<5:12:04, 21.52s/it]                                                      {'loss': 0.2208, 'grad_norm': 1.382076402919942, 'learning_rate': 2.6191552374531105e-05, 'epoch': 6.19}
 31%|â–ˆâ–ˆâ–ˆ       | 390/1260 [2:21:49<5:12:04, 21.52s/it] 31%|â–ˆâ–ˆâ–ˆ       | 391/1260 [2:22:10<5:11:48, 21.53s/it] 31%|â–ˆâ–ˆâ–ˆ       | 392/1260 [2:22:32<5:12:12, 21.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 393/1260 [2:22:54<5:12:28, 21.62s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 394/1260 [2:23:15<5:11:44, 21.60s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 395/1260 [2:23:36<5:10:27, 21.54s/it]                                                      {'loss': 0.2169, 'grad_norm': 1.551360876408413, 'learning_rate': 2.6052138785082897e-05, 'epoch': 6.27}
 31%|â–ˆâ–ˆâ–ˆâ–      | 395/1260 [2:23:36<5:10:27, 21.54s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 396/1260 [2:23:58<5:10:04, 21.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 397/1260 [2:24:19<5:09:07, 21.49s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 398/1260 [2:24:41<5:08:33, 21.48s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 399/1260 [2:25:02<5:08:42, 21.51s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 400/1260 [2:25:24<5:10:08, 21.64s/it]                                                      {'loss': 0.213, 'grad_norm': 1.0839945603953907, 'learning_rate': 2.5910604623595732e-05, 'epoch': 6.35}
 32%|â–ˆâ–ˆâ–ˆâ–      | 400/1260 [2:25:24<5:10:08, 21.64s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 401/1260 [2:25:46<5:09:31, 21.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 402/1260 [2:26:07<5:08:32, 21.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 403/1260 [2:26:29<5:08:21, 21.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 404/1260 [2:26:50<5:07:09, 21.53s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 405/1260 [2:27:12<5:06:57, 21.54s/it]                                                      {'loss': 0.2156, 'grad_norm': 1.745805525382732, 'learning_rate': 2.5766977046205735e-05, 'epoch': 6.43}
 32%|â–ˆâ–ˆâ–ˆâ–      | 405/1260 [2:27:12<5:06:57, 21.54s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 406/1260 [2:27:33<5:06:19, 21.52s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 407/1260 [2:27:55<5:07:55, 21.66s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 408/1260 [2:28:17<5:08:39, 21.74s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 409/1260 [2:28:39<5:08:25, 21.75s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 410/1260 [2:29:01<5:07:32, 21.71s/it]                                                      {'loss': 0.2185, 'grad_norm': 1.2358303642958441, 'learning_rate': 2.5621283610712407e-05, 'epoch': 6.51}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 410/1260 [2:29:01<5:07:32, 21.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 411/1260 [2:29:22<5:07:11, 21.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 412/1260 [2:29:44<5:05:34, 21.62s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 413/1260 [2:30:05<5:04:31, 21.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 414/1260 [2:30:27<5:04:17, 21.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 415/1260 [2:30:48<5:03:33, 21.55s/it]                                                      {'loss': 0.2133, 'grad_norm': 1.8703356888305178, 'learning_rate': 2.5473552271291092e-05, 'epoch': 6.59}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 415/1260 [2:30:48<5:03:33, 21.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 416/1260 [2:31:10<5:02:54, 21.53s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 417/1260 [2:31:31<5:02:20, 21.52s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 418/1260 [2:31:53<5:01:16, 21.47s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 419/1260 [2:32:15<5:02:01, 21.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 420/1260 [2:32:36<5:02:00, 21.57s/it]                                                      {'loss': 0.2177, 'grad_norm': 1.2813998283712622, 'learning_rate': 2.5323811373129434e-05, 'epoch': 6.67}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 420/1260 [2:32:36<5:02:00, 21.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 421/1260 [2:32:58<5:01:55, 21.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 422/1260 [2:33:20<5:02:23, 21.65s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 423/1260 [2:33:41<5:01:56, 21.64s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 424/1260 [2:34:03<5:02:17, 21.70s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 425/1260 [2:34:24<5:00:46, 21.61s/it]                                                      {'loss': 0.2129, 'grad_norm': 1.6654179310338113, 'learning_rate': 2.5172089646988765e-05, 'epoch': 6.75}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 425/1260 [2:34:24<5:00:46, 21.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 426/1260 [2:34:46<5:00:23, 21.61s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 427/1260 [2:35:08<5:00:33, 21.65s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 428/1260 [2:35:29<4:59:03, 21.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 429/1260 [2:35:51<4:59:41, 21.64s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 430/1260 [2:36:12<4:58:06, 21.55s/it]                                                      {'loss': 0.2153, 'grad_norm': 1.1531679539375663, 'learning_rate': 2.501841620369156e-05, 'epoch': 6.83}
 34%|â–ˆâ–ˆâ–ˆâ–      | 430/1260 [2:36:12<4:58:06, 21.55s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 431/1260 [2:36:34<4:57:14, 21.51s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 432/1260 [2:36:55<4:56:13, 21.47s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 433/1260 [2:37:17<4:55:59, 21.47s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 434/1260 [2:37:38<4:56:47, 21.56s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 435/1260 [2:38:00<4:55:56, 21.52s/it]                                                      {'loss': 0.2114, 'grad_norm': 0.9510523414299642, 'learning_rate': 2.4862820528535955e-05, 'epoch': 6.91}
 35%|â–ˆâ–ˆâ–ˆâ–      | 435/1260 [2:38:00<4:55:56, 21.52s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 436/1260 [2:38:22<4:57:24, 21.66s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 437/1260 [2:38:43<4:57:15, 21.67s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 438/1260 [2:39:05<4:56:27, 21.64s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 439/1260 [2:39:26<4:54:49, 21.55s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 440/1260 [2:39:48<4:54:43, 21.57s/it]                                                      {'loss': 0.2151, 'grad_norm': 1.2040888010683606, 'learning_rate': 2.470533247563837e-05, 'epoch': 6.99}
 35%|â–ˆâ–ˆâ–ˆâ–      | 440/1260 [2:39:48<4:54:43, 21.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 441/1260 [2:40:08<4:46:58, 21.02s/it][INFO|trainer.py:3993] 2025-07-05 00:13:16,561 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441
[INFO|configuration_utils.py:424] 2025-07-05 00:13:16,567 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:13:16,568 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:13:20,186 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:13:20,188 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:13:20,189 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:13:20,190 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/special_tokens_map.json
[2025-07-05 00:13:20,381] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is about to be saved!
[2025-07-05 00:13:20,392] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:13:20,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:13:20,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:13:20,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:13:27,986] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:13:27,987] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:13:28,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step440 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:13:28,387 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:13:28,387 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:13:28,388 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:13:28,388 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:13:28,545 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:13:28,952 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/chat_template.jinja
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 442/1260 [2:40:46<5:55:19, 26.06s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 443/1260 [2:41:07<5:37:09, 24.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 444/1260 [2:41:29<5:24:08, 23.83s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 445/1260 [2:41:51<5:14:47, 23.18s/it]                                                      {'loss': 0.1882, 'grad_norm': 1.1594014798163437, 'learning_rate': 2.4545982262205455e-05, 'epoch': 7.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 445/1260 [2:41:51<5:14:47, 23.18s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 446/1260 [2:42:12<5:07:25, 22.66s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 447/1260 [2:42:34<5:03:36, 22.41s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 448/1260 [2:42:56<5:01:34, 22.28s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 449/1260 [2:43:18<4:59:39, 22.17s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 450/1260 [2:43:39<4:56:17, 21.95s/it]                                                      {'loss': 0.1844, 'grad_norm': 1.163016633447776, 'learning_rate': 2.4384800462736265e-05, 'epoch': 7.14}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 450/1260 [2:43:39<4:56:17, 21.95s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 451/1260 [2:44:01<4:55:00, 21.88s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 452/1260 [2:44:22<4:52:35, 21.73s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 453/1260 [2:44:44<4:51:10, 21.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 454/1260 [2:45:05<4:49:36, 21.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 455/1260 [2:45:27<4:50:27, 21.65s/it]                                                      {'loss': 0.1969, 'grad_norm': 2.1048728438265916, 'learning_rate': 2.422181800315599e-05, 'epoch': 7.22}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 455/1260 [2:45:27<4:50:27, 21.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 456/1260 [2:45:48<4:49:32, 21.61s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 457/1260 [2:46:10<4:49:09, 21.61s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 458/1260 [2:46:32<4:48:14, 21.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 459/1260 [2:46:53<4:48:28, 21.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 460/1260 [2:47:15<4:47:27, 21.56s/it]                                                      {'loss': 0.194, 'grad_norm': 2.1526804645409716, 'learning_rate': 2.405706615488216e-05, 'epoch': 7.3}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 460/1260 [2:47:15<4:47:27, 21.56s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 461/1260 [2:47:36<4:46:34, 21.52s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 462/1260 [2:47:58<4:46:49, 21.57s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 463/1260 [2:48:19<4:45:58, 21.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 464/1260 [2:48:41<4:45:37, 21.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 465/1260 [2:49:02<4:45:23, 21.54s/it]                                                      {'loss': 0.1903, 'grad_norm': 1.2082684163916826, 'learning_rate': 2.3890576528824637e-05, 'epoch': 7.38}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 465/1260 [2:49:02<4:45:23, 21.54s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 466/1260 [2:49:24<4:44:21, 21.49s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 467/1260 [2:49:45<4:45:19, 21.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 468/1260 [2:50:07<4:44:27, 21.55s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 469/1260 [2:50:29<4:44:48, 21.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 470/1260 [2:50:50<4:44:37, 21.62s/it]                                                      {'loss': 0.1866, 'grad_norm': 1.3494436864244765, 'learning_rate': 2.37223810693204e-05, 'epoch': 7.46}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 470/1260 [2:50:50<4:44:37, 21.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 471/1260 [2:51:12<4:43:54, 21.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 472/1260 [2:51:34<4:44:10, 21.64s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 473/1260 [2:51:55<4:43:44, 21.63s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 474/1260 [2:52:17<4:44:10, 21.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 475/1260 [2:52:38<4:42:42, 21.61s/it]                                                      {'loss': 0.1927, 'grad_norm': 1.5420683444577554, 'learning_rate': 2.3552512048004428e-05, 'epoch': 7.54}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 475/1260 [2:52:38<4:42:42, 21.61s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 476/1260 [2:53:00<4:42:58, 21.66s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 477/1260 [2:53:22<4:43:00, 21.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 478/1260 [2:53:44<4:42:48, 21.70s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 479/1260 [2:54:06<4:44:38, 21.87s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 480/1260 [2:54:27<4:42:38, 21.74s/it]                                                      {'loss': 0.1914, 'grad_norm': 1.3013548611889, 'learning_rate': 2.3381002057617706e-05, 'epoch': 7.62}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 480/1260 [2:54:27<4:42:38, 21.74s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 481/1260 [2:54:49<4:41:55, 21.71s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 482/1260 [2:55:10<4:40:22, 21.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 483/1260 [2:55:32<4:41:17, 21.72s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 484/1260 [2:55:54<4:39:53, 21.64s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 485/1260 [2:56:16<4:40:28, 21.71s/it]                                                      {'loss': 0.193, 'grad_norm': 1.3154091582137104, 'learning_rate': 2.3207884005753707e-05, 'epoch': 7.7}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 485/1260 [2:56:16<4:40:28, 21.71s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 486/1260 [2:56:37<4:40:03, 21.71s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 487/1260 [2:56:59<4:39:16, 21.68s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 488/1260 [2:57:21<4:38:46, 21.67s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 489/1260 [2:57:42<4:38:14, 21.65s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 490/1260 [2:58:04<4:37:24, 21.62s/it]                                                      {'loss': 0.1957, 'grad_norm': 1.1302383908031444, 'learning_rate': 2.303319110854438e-05, 'epoch': 7.78}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 490/1260 [2:58:04<4:37:24, 21.62s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 491/1260 [2:58:25<4:36:31, 21.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 492/1260 [2:58:47<4:36:00, 21.56s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 493/1260 [2:59:08<4:35:46, 21.57s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 494/1260 [2:59:30<4:35:01, 21.54s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 495/1260 [2:59:51<4:34:09, 21.50s/it]                                                      {'loss': 0.1946, 'grad_norm': 1.090336498723518, 'learning_rate': 2.2856956884286986e-05, 'epoch': 7.86}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 495/1260 [2:59:51<4:34:09, 21.50s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 496/1260 [3:00:13<4:33:34, 21.48s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 497/1260 [3:00:34<4:33:30, 21.51s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 498/1260 [3:00:56<4:33:03, 21.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 499/1260 [3:01:18<4:34:09, 21.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 500/1260 [3:01:39<4:33:31, 21.59s/it]                                                      {'loss': 0.2052, 'grad_norm': 1.5949145755002545, 'learning_rate': 2.2679215147012955e-05, 'epoch': 7.94}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 500/1260 [3:01:39<4:33:31, 21.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 501/1260 [3:02:01<4:32:49, 21.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 502/1260 [3:02:22<4:32:31, 21.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 503/1260 [3:02:44<4:32:04, 21.56s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 504/1260 [3:03:03<4:23:15, 20.89s/it][INFO|trainer.py:3993] 2025-07-05 00:36:12,563 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504
[INFO|configuration_utils.py:424] 2025-07-05 00:36:12,569 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:36:12,570 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:36:15,738 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:36:15,739 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:36:15,740 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:36:15,741 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/special_tokens_map.json
[2025-07-05 00:36:15,938] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step503 is about to be saved!
[2025-07-05 00:36:15,948] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:36:15,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:36:15,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:36:15,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:36:20,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:36:20,378] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:36:23,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step503 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:36:23,915 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:36:23,916 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:36:23,917 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:36:23,918 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:36:24,074 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:36:24,482 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/chat_template.jinja
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 505/1260 [3:03:40<5:24:25, 25.78s/it]                                                      {'loss': 0.1915, 'grad_norm': 1.536331529441199, 'learning_rate': 2.25e-05, 'epoch': 8.02}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 505/1260 [3:03:40<5:24:25, 25.78s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 506/1260 [3:04:02<5:08:15, 24.53s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 507/1260 [3:04:24<4:56:23, 23.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 508/1260 [3:04:45<4:47:36, 22.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 509/1260 [3:05:07<4:42:26, 22.57s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 510/1260 [3:05:28<4:37:56, 22.24s/it]                                                      {'loss': 0.166, 'grad_norm': 1.9747485613853988, 'learning_rate': 2.2319345829228706e-05, 'epoch': 8.1}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 510/1260 [3:05:28<4:37:56, 22.24s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 511/1260 [3:05:50<4:35:23, 22.06s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 512/1260 [3:06:11<4:32:59, 21.90s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 513/1260 [3:06:33<4:31:13, 21.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 514/1260 [3:06:54<4:30:38, 21.77s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 515/1260 [3:07:16<4:28:52, 21.65s/it]                                                      {'loss': 0.1632, 'grad_norm': 1.3278516185443798, 'learning_rate': 2.213728729678491e-05, 'epoch': 8.17}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 515/1260 [3:07:16<4:28:52, 21.65s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 516/1260 [3:07:37<4:27:27, 21.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 517/1260 [3:07:59<4:28:08, 21.65s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 518/1260 [3:08:21<4:27:33, 21.64s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 519/1260 [3:08:42<4:26:34, 21.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 520/1260 [3:09:04<4:25:45, 21.55s/it]                                                      {'loss': 0.16, 'grad_norm': 0.9480374586908686, 'learning_rate': 2.1953859334209085e-05, 'epoch': 8.25}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 520/1260 [3:09:04<4:25:45, 21.55s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 521/1260 [3:09:25<4:25:12, 21.53s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 522/1260 [3:09:47<4:24:27, 21.50s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 523/1260 [3:10:08<4:24:01, 21.49s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 524/1260 [3:10:30<4:24:01, 21.52s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 525/1260 [3:10:51<4:23:13, 21.49s/it]                                                      {'loss': 0.1639, 'grad_norm': 1.6706254749076082, 'learning_rate': 2.1769097135794052e-05, 'epoch': 8.33}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 525/1260 [3:10:51<4:23:13, 21.49s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 526/1260 [3:11:13<4:23:34, 21.55s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 527/1260 [3:11:34<4:23:54, 21.60s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 528/1260 [3:11:56<4:24:51, 21.71s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 529/1260 [3:12:18<4:23:21, 21.62s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 530/1260 [3:12:39<4:22:16, 21.56s/it]                                                      {'loss': 0.1634, 'grad_norm': 1.3204616912321079, 'learning_rate': 2.158303615183223e-05, 'epoch': 8.41}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 530/1260 [3:12:39<4:22:16, 21.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 531/1260 [3:13:01<4:21:54, 21.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 532/1260 [3:13:23<4:24:10, 21.77s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 533/1260 [3:13:45<4:23:00, 21.71s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 534/1260 [3:14:06<4:21:49, 21.64s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 535/1260 [3:14:28<4:21:21, 21.63s/it]                                                      {'loss': 0.1641, 'grad_norm': 1.5983575907757053, 'learning_rate': 2.139571208181381e-05, 'epoch': 8.49}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 535/1260 [3:14:28<4:21:21, 21.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 536/1260 [3:14:49<4:20:17, 21.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 537/1260 [3:15:11<4:19:58, 21.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 538/1260 [3:15:33<4:21:08, 21.70s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 539/1260 [3:15:54<4:20:27, 21.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 540/1260 [3:16:16<4:20:03, 21.67s/it]                                                      {'loss': 0.1552, 'grad_norm': 1.142564214019326, 'learning_rate': 2.1207160867577087e-05, 'epoch': 8.57}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 540/1260 [3:16:16<4:20:03, 21.67s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 541/1260 [3:16:38<4:20:07, 21.71s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 542/1260 [3:17:00<4:20:56, 21.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 543/1260 [3:17:21<4:19:31, 21.72s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 544/1260 [3:17:43<4:18:10, 21.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 545/1260 [3:18:04<4:17:31, 21.61s/it]                                                      {'loss': 0.1595, 'grad_norm': 1.3558653013722137, 'learning_rate': 2.101741868641233e-05, 'epoch': 8.65}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 545/1260 [3:18:04<4:17:31, 21.61s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 546/1260 [3:18:26<4:16:15, 21.53s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 547/1260 [3:18:47<4:15:36, 21.51s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 548/1260 [3:19:09<4:14:58, 21.49s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 549/1260 [3:19:30<4:14:19, 21.46s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 550/1260 [3:19:51<4:13:51, 21.45s/it]                                                      {'loss': 0.1637, 'grad_norm': 1.0286433913992328, 'learning_rate': 2.082652194412042e-05, 'epoch': 8.73}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 550/1260 [3:19:51<4:13:51, 21.45s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 551/1260 [3:20:13<4:13:33, 21.46s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 552/1260 [3:20:35<4:14:36, 21.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 553/1260 [3:20:56<4:13:48, 21.54s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 554/1260 [3:21:18<4:13:01, 21.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 555/1260 [3:21:39<4:13:02, 21.53s/it]                                                      {'loss': 0.1639, 'grad_norm': 1.094132514227969, 'learning_rate': 2.0634507268027702e-05, 'epoch': 8.81}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 555/1260 [3:21:39<4:13:02, 21.53s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 556/1260 [3:22:01<4:13:06, 21.57s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 557/1260 [3:22:22<4:12:55, 21.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 558/1260 [3:22:44<4:12:51, 21.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 559/1260 [3:23:06<4:11:56, 21.56s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 560/1260 [3:23:27<4:11:35, 21.56s/it]                                                      {'loss': 0.1695, 'grad_norm': 1.9453775874702577, 'learning_rate': 2.0441411499958287e-05, 'epoch': 8.89}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 560/1260 [3:23:27<4:11:35, 21.56s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 561/1260 [3:23:49<4:11:10, 21.56s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 562/1260 [3:24:10<4:10:31, 21.53s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 563/1260 [3:24:32<4:09:40, 21.49s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 564/1260 [3:24:53<4:10:30, 21.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 565/1260 [3:25:15<4:09:17, 21.52s/it]                                                      {'loss': 0.1703, 'grad_norm': 1.7069424150580634, 'learning_rate': 2.0247271689165226e-05, 'epoch': 8.97}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 565/1260 [3:25:15<4:09:17, 21.52s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 566/1260 [3:25:37<4:10:43, 21.68s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 567/1260 [3:25:56<4:02:53, 21.03s/it][INFO|trainer.py:3993] 2025-07-05 00:59:05,044 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567
[INFO|configuration_utils.py:424] 2025-07-05 00:59:05,050 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:59:05,051 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:59:08,220 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:59:08,222 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:59:08,223 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:59:08,224 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/special_tokens_map.json
[2025-07-05 00:59:08,417] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step565 is about to be saved!
[2025-07-05 00:59:08,428] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:59:08,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:59:08,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:59:08,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:59:12,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:59:12,956] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:59:16,373] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step565 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:59:16,382 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:59:16,382 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:59:16,383 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:59:16,383 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:59:16,546 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:59:16,948 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/chat_template.jinja
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 568/1260 [3:26:33<4:56:48, 25.73s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 569/1260 [3:26:55<4:43:37, 24.63s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 570/1260 [3:27:16<4:32:09, 23.67s/it]                                                      {'loss': 0.1429, 'grad_norm': 1.8935790246701274, 'learning_rate': 2.0052125085221868e-05, 'epoch': 9.05}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 570/1260 [3:27:16<4:32:09, 23.67s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 571/1260 [3:27:38<4:24:24, 23.02s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 572/1260 [3:28:00<4:18:59, 22.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 573/1260 [3:28:21<4:14:37, 22.24s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 574/1260 [3:28:43<4:11:43, 22.02s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 575/1260 [3:29:04<4:09:15, 21.83s/it]                                                      {'loss': 0.1279, 'grad_norm': 1.6717600026988035, 'learning_rate': 1.985600913087482e-05, 'epoch': 9.13}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 575/1260 [3:29:04<4:09:15, 21.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 576/1260 [3:29:25<4:07:48, 21.74s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 577/1260 [3:29:47<4:07:49, 21.77s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 578/1260 [3:30:09<4:07:01, 21.73s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 579/1260 [3:30:30<4:05:32, 21.63s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 580/1260 [3:30:52<4:04:07, 21.54s/it]                                                      {'loss': 0.125, 'grad_norm': 1.5083747818922892, 'learning_rate': 1.9658961454859758e-05, 'epoch': 9.21}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 580/1260 [3:30:52<4:04:07, 21.54s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 581/1260 [3:31:13<4:04:06, 21.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 582/1260 [3:31:35<4:03:06, 21.51s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 583/1260 [3:31:56<4:03:18, 21.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 584/1260 [3:32:18<4:03:44, 21.63s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 585/1260 [3:32:40<4:02:53, 21.59s/it]                                                      {'loss': 0.1269, 'grad_norm': 1.5290020166110139, 'learning_rate': 1.946101986468167e-05, 'epoch': 9.29}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 585/1260 [3:32:40<4:02:53, 21.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 586/1260 [3:33:01<4:02:53, 21.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 587/1260 [3:33:23<4:01:57, 21.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 588/1260 [3:33:44<4:01:25, 21.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 589/1260 [3:34:06<4:00:35, 21.51s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 590/1260 [3:34:27<4:00:03, 21.50s/it]                                                      {'loss': 0.1264, 'grad_norm': 1.892046044470987, 'learning_rate': 1.9262222339360684e-05, 'epoch': 9.37}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 590/1260 [3:34:27<4:00:03, 21.50s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 591/1260 [3:34:49<4:00:05, 21.53s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 592/1260 [3:35:10<3:59:37, 21.52s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 593/1260 [3:35:32<3:58:43, 21.47s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 594/1260 [3:35:53<3:59:15, 21.55s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 595/1260 [3:36:15<3:59:36, 21.62s/it]                                                      {'loss': 0.1247, 'grad_norm': 1.9766700908444432, 'learning_rate': 1.906260702214508e-05, 'epoch': 9.45}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 595/1260 [3:36:15<3:59:36, 21.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 596/1260 [3:36:37<3:58:32, 21.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 597/1260 [3:36:59<3:59:30, 21.68s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 598/1260 [3:37:20<3:58:04, 21.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 599/1260 [3:37:42<3:58:29, 21.65s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 600/1260 [3:38:03<3:58:12, 21.65s/it]                                                      {'loss': 0.1285, 'grad_norm': 1.3908185879834143, 'learning_rate': 1.8862212213192718e-05, 'epoch': 9.52}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 600/1260 [3:38:03<3:58:12, 21.65s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 601/1260 [3:38:25<3:58:21, 21.70s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 602/1260 [3:38:47<3:58:38, 21.76s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 603/1260 [3:39:09<3:58:32, 21.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 604/1260 [3:39:30<3:57:13, 21.70s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 605/1260 [3:39:52<3:57:04, 21.72s/it]                                                      {'loss': 0.1318, 'grad_norm': 1.545198331345673, 'learning_rate': 1.866107636222242e-05, 'epoch': 9.6}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 605/1260 [3:39:52<3:57:04, 21.72s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 606/1260 [3:40:14<3:56:28, 21.69s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 607/1260 [3:40:35<3:55:46, 21.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 608/1260 [3:40:57<3:54:36, 21.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 609/1260 [3:41:18<3:53:32, 21.52s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 610/1260 [3:41:40<3:53:39, 21.57s/it]                                                      {'loss': 0.1319, 'grad_norm': 1.4399440282937868, 'learning_rate': 1.8459238061136604e-05, 'epoch': 9.68}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 610/1260 [3:41:40<3:53:39, 21.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 611/1260 [3:42:02<3:54:47, 21.71s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 612/1260 [3:42:23<3:53:33, 21.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 613/1260 [3:42:45<3:53:16, 21.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 614/1260 [3:43:07<3:52:44, 21.62s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 615/1260 [3:43:28<3:51:46, 21.56s/it]                                                      {'loss': 0.1301, 'grad_norm': 1.0187998595374557, 'learning_rate': 1.82567360366167e-05, 'epoch': 9.76}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 615/1260 [3:43:28<3:51:46, 21.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 616/1260 [3:43:50<3:51:37, 21.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 617/1260 [3:44:11<3:51:38, 21.61s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 618/1260 [3:44:33<3:52:22, 21.72s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 619/1260 [3:44:55<3:51:05, 21.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 620/1260 [3:45:16<3:49:51, 21.55s/it]                                                      {'loss': 0.1293, 'grad_norm': 2.0482483959051, 'learning_rate': 1.8053609142692608e-05, 'epoch': 9.84}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 620/1260 [3:45:16<3:49:51, 21.55s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 621/1260 [3:45:37<3:49:08, 21.52s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 622/1260 [3:45:59<3:48:47, 21.52s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 623/1260 [3:46:21<3:48:49, 21.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 624/1260 [3:46:42<3:48:13, 21.53s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 625/1260 [3:47:04<3:47:53, 21.53s/it]                                                      {'loss': 0.1314, 'grad_norm': 2.164356927405989, 'learning_rate': 1.7849896353287853e-05, 'epoch': 9.92}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 625/1260 [3:47:04<3:47:53, 21.53s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 626/1260 [3:47:25<3:46:57, 21.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 627/1260 [3:47:47<3:46:47, 21.50s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 628/1260 [3:48:08<3:46:53, 21.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 629/1260 [3:48:30<3:45:52, 21.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 630/1260 [3:48:49<3:39:58, 20.95s/it]                                                      {'loss': 0.1273, 'grad_norm': 1.380580907792332, 'learning_rate': 1.7645636754741604e-05, 'epoch': 10.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 630/1260 [3:48:49<3:39:58, 20.95s/it][INFO|trainer.py:3993] 2025-07-05 01:21:58,584 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630
[INFO|configuration_utils.py:424] 2025-07-05 01:21:58,590 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/config.json
[INFO|configuration_utils.py:904] 2025-07-05 01:21:58,591 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 01:22:01,381 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:22:01,383 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:22:01,384 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:22:01,384 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/special_tokens_map.json
[2025-07-05 01:22:01,580] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step628 is about to be saved!
[2025-07-05 01:22:01,591] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 01:22:01,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 01:22:01,626] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 01:22:01,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 01:22:06,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 01:22:06,250] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 01:22:09,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step628 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 01:22:09,527 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:22:09,527 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:22:09,528 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:22:09,528 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 01:22:09,706 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 01:22:10,116 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/chat_template.jinja
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 631/1260 [3:49:26<4:29:50, 25.74s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 632/1260 [3:49:48<4:16:33, 24.51s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 633/1260 [3:50:09<4:07:15, 23.66s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 634/1260 [3:50:31<4:00:18, 23.03s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 635/1260 [3:50:53<3:55:05, 22.57s/it]                                                      {'loss': 0.0926, 'grad_norm': 1.3544839279414589, 'learning_rate': 1.744086953830922e-05, 'epoch': 10.08}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 635/1260 [3:50:53<3:55:05, 22.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 636/1260 [3:51:14<3:51:13, 22.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 637/1260 [3:51:36<3:49:21, 22.09s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 638/1260 [3:51:57<3:47:45, 21.97s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 639/1260 [3:52:19<3:45:46, 21.81s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 640/1260 [3:52:40<3:44:10, 21.69s/it]                                                      {'loss': 0.0923, 'grad_norm': 1.8726973588590692, 'learning_rate': 1.7235633992642615e-05, 'epoch': 10.16}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 640/1260 [3:52:40<3:44:10, 21.69s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 641/1260 [3:53:02<3:45:15, 21.83s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 642/1260 [3:53:24<3:44:46, 21.82s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 643/1260 [3:53:46<3:44:10, 21.80s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 644/1260 [3:54:08<3:42:56, 21.72s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 645/1260 [3:54:29<3:42:03, 21.67s/it]                                                      {'loss': 0.0944, 'grad_norm': 1.9913408787989686, 'learning_rate': 1.702996949625197e-05, 'epoch': 10.24}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 645/1260 [3:54:29<3:42:03, 21.67s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 646/1260 [3:54:51<3:41:03, 21.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 647/1260 [3:55:13<3:41:53, 21.72s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 648/1260 [3:55:34<3:40:32, 21.62s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 649/1260 [3:55:55<3:39:49, 21.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 650/1260 [3:56:17<3:40:03, 21.64s/it]                                                      {'loss': 0.098, 'grad_norm': 1.7129598752452946, 'learning_rate': 1.682391550995014e-05, 'epoch': 10.32}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 650/1260 [3:56:17<3:40:03, 21.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 651/1260 [3:56:39<3:39:06, 21.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 652/1260 [3:57:00<3:38:20, 21.55s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 653/1260 [3:57:22<3:38:31, 21.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 654/1260 [3:57:43<3:38:07, 21.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 655/1260 [3:58:05<3:38:03, 21.63s/it]                                                      {'loss': 0.0967, 'grad_norm': 1.8490807377440697, 'learning_rate': 1.6617511569281382e-05, 'epoch': 10.4}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 655/1260 [3:58:05<3:38:03, 21.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 656/1260 [3:58:27<3:38:38, 21.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 657/1260 [3:58:49<3:37:44, 21.67s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 658/1260 [3:59:10<3:37:45, 21.70s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 659/1260 [3:59:32<3:36:41, 21.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 660/1260 [3:59:54<3:37:09, 21.72s/it]                                                      {'loss': 0.0966, 'grad_norm': 2.0453001721534148, 'learning_rate': 1.641079727693561e-05, 'epoch': 10.48}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 660/1260 [3:59:54<3:37:09, 21.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 661/1260 [4:00:15<3:36:24, 21.68s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 662/1260 [4:00:37<3:36:07, 21.68s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 663/1260 [4:00:59<3:35:24, 21.65s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 664/1260 [4:01:20<3:34:45, 21.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 665/1260 [4:01:42<3:34:27, 21.63s/it]                                                      {'loss': 0.0953, 'grad_norm': 1.3308505018920638, 'learning_rate': 1.6203812295149876e-05, 'epoch': 10.56}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 665/1260 [4:01:42<3:34:27, 21.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 666/1260 [4:02:03<3:34:16, 21.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 667/1260 [4:02:25<3:33:38, 21.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 668/1260 [4:02:46<3:32:47, 21.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 669/1260 [4:03:08<3:32:45, 21.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 670/1260 [4:03:30<3:32:09, 21.58s/it]                                                      {'loss': 0.0941, 'grad_norm': 1.7862107928001811, 'learning_rate': 1.5996596338098365e-05, 'epoch': 10.64}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 670/1260 [4:03:30<3:32:09, 21.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 671/1260 [4:03:52<3:32:38, 21.66s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 672/1260 [4:04:13<3:32:59, 21.73s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 673/1260 [4:04:35<3:32:14, 21.69s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 674/1260 [4:04:57<3:31:19, 21.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 675/1260 [4:05:18<3:30:54, 21.63s/it]                                                      {'loss': 0.0963, 'grad_norm': 1.5972145860830425, 'learning_rate': 1.5789189164272456e-05, 'epoch': 10.72}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 675/1260 [4:05:18<3:30:54, 21.63s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 676/1260 [4:05:40<3:30:29, 21.63s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 677/1260 [4:06:01<3:29:31, 21.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 678/1260 [4:06:23<3:29:53, 21.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 679/1260 [4:06:45<3:29:30, 21.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 680/1260 [4:07:06<3:28:48, 21.60s/it]                                                      {'loss': 0.0943, 'grad_norm': 1.1636142296514866, 'learning_rate': 1.5581630568852252e-05, 'epoch': 10.8}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 680/1260 [4:07:06<3:28:48, 21.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 681/1260 [4:07:28<3:28:49, 21.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 682/1260 [4:07:50<3:28:36, 21.65s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 683/1260 [4:08:11<3:27:34, 21.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 684/1260 [4:08:32<3:26:55, 21.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 685/1260 [4:08:54<3:26:31, 21.55s/it]                                                      {'loss': 0.0979, 'grad_norm': 1.9620364521158544, 'learning_rate': 1.5373960376071095e-05, 'epoch': 10.87}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 685/1260 [4:08:54<3:26:31, 21.55s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 686/1260 [4:09:16<3:26:29, 21.59s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 687/1260 [4:09:37<3:25:46, 21.55s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 688/1260 [4:09:59<3:25:57, 21.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 689/1260 [4:10:20<3:25:16, 21.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 690/1260 [4:10:42<3:24:54, 21.57s/it]                                                      {'loss': 0.0932, 'grad_norm': 1.8054478989352858, 'learning_rate': 1.516621843157449e-05, 'epoch': 10.95}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 690/1260 [4:10:42<3:24:54, 21.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 691/1260 [4:11:03<3:24:03, 21.52s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 692/1260 [4:11:25<3:23:23, 21.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 693/1260 [4:11:44<3:17:18, 20.88s/it][INFO|trainer.py:3993] 2025-07-05 01:44:52,953 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693
[INFO|configuration_utils.py:424] 2025-07-05 01:44:52,958 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/config.json
[INFO|configuration_utils.py:904] 2025-07-05 01:44:52,959 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 01:44:56,182 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:44:56,183 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:44:56,184 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:44:56,185 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/special_tokens_map.json
[2025-07-05 01:44:56,371] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step691 is about to be saved!
[2025-07-05 01:44:56,381] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 01:44:56,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 01:44:56,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 01:44:56,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 01:45:01,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 01:45:01,316] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 01:45:04,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 01:45:04,344 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:45:04,344 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:45:04,345 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:45:04,345 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 01:45:04,503 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 01:45:04,903 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-693/chat_template.jinja
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 694/1260 [4:12:21<4:01:32, 25.60s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 695/1260 [4:12:42<3:49:07, 24.33s/it]                                                      {'loss': 0.0809, 'grad_norm': 1.603573729477561, 'learning_rate': 1.495844459477494e-05, 'epoch': 11.03}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 695/1260 [4:12:42<3:49:07, 24.33s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 696/1260 [4:13:04<3:40:34, 23.47s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 697/1260 [4:13:25<3:34:49, 22.89s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 698/1260 [4:13:47<3:30:15, 22.45s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 699/1260 [4:14:08<3:27:07, 22.15s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 700/1260 [4:14:30<3:25:09, 21.98s/it]                                                      {'loss': 0.0623, 'grad_norm': 2.0375262071002673, 'learning_rate': 1.4750678731204108e-05, 'epoch': 11.11}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 700/1260 [4:14:30<3:25:09, 21.98s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 701/1260 [4:14:51<3:23:31, 21.85s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 702/1260 [4:15:13<3:22:59, 21.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 703/1260 [4:15:35<3:21:57, 21.75s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 704/1260 [4:15:56<3:20:39, 21.65s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 705/1260 [4:16:17<3:19:27, 21.56s/it]                                                      {'loss': 0.061, 'grad_norm': 1.4265702476491378, 'learning_rate': 1.4542960704863842e-05, 'epoch': 11.19}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 705/1260 [4:16:17<3:19:27, 21.56s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 706/1260 [4:16:39<3:19:15, 21.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 707/1260 [4:17:01<3:19:00, 21.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 708/1260 [4:17:22<3:18:51, 21.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 709/1260 [4:17:44<3:18:37, 21.63s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 710/1260 [4:18:06<3:18:34, 21.66s/it]                                                      {'loss': 0.0624, 'grad_norm': 1.5939702272599028, 'learning_rate': 1.4335330370577475e-05, 'epoch': 11.27}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 710/1260 [4:18:06<3:18:34, 21.66s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 711/1260 [4:18:27<3:18:13, 21.66s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 712/1260 [4:18:49<3:17:08, 21.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 713/1260 [4:19:10<3:16:33, 21.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 714/1260 [4:19:32<3:15:48, 21.52s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 715/1260 [4:19:53<3:15:26, 21.52s/it]                                                      {'loss': 0.0639, 'grad_norm': 1.601780065502264, 'learning_rate': 1.4127827566342864e-05, 'epoch': 11.35}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 715/1260 [4:19:53<3:15:26, 21.52s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 716/1260 [4:20:15<3:15:23, 21.55s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 717/1260 [4:20:36<3:14:41, 21.51s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 718/1260 [4:20:58<3:15:18, 21.62s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 719/1260 [4:21:20<3:14:33, 21.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 720/1260 [4:21:41<3:14:50, 21.65s/it]                                                      {'loss': 0.0597, 'grad_norm': 0.9818393911151659, 'learning_rate': 1.3920492105688703e-05, 'epoch': 11.43}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 720/1260 [4:21:41<3:14:50, 21.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 721/1260 [4:22:03<3:14:31, 21.65s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 722/1260 [4:22:25<3:14:37, 21.70s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 723/1260 [4:22:47<3:15:32, 21.85s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 724/1260 [4:23:09<3:14:37, 21.79s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 725/1260 [4:23:31<3:15:59, 21.98s/it]                                                      {'loss': 0.0583, 'grad_norm': 1.0562814166942662, 'learning_rate': 1.371336377003551e-05, 'epoch': 11.51}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 725/1260 [4:23:31<3:15:59, 21.98s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 726/1260 [4:23:53<3:14:07, 21.81s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 727/1260 [4:24:14<3:12:42, 21.69s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 728/1260 [4:24:36<3:12:15, 21.68s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 729/1260 [4:24:57<3:11:28, 21.63s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 730/1260 [4:25:19<3:11:58, 21.73s/it]                                                      {'loss': 0.061, 'grad_norm': 1.2482752349382704, 'learning_rate': 1.3506482301062753e-05, 'epoch': 11.59}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 730/1260 [4:25:19<3:11:58, 21.73s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 731/1260 [4:25:41<3:11:37, 21.73s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 732/1260 [4:26:03<3:11:09, 21.72s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 733/1260 [4:26:24<3:10:30, 21.69s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 734/1260 [4:26:46<3:09:28, 21.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 735/1260 [4:27:07<3:09:12, 21.62s/it]                                                      {'loss': 0.0641, 'grad_norm': 2.1867153547536153, 'learning_rate': 1.3299887393083629e-05, 'epoch': 11.67}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 735/1260 [4:27:07<3:09:12, 21.62s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 736/1260 [4:27:29<3:08:23, 21.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 737/1260 [4:27:50<3:07:47, 21.54s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 738/1260 [4:28:12<3:07:10, 21.51s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 739/1260 [4:28:33<3:07:05, 21.55s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 740/1260 [4:28:55<3:07:13, 21.60s/it]                                                      {'loss': 0.0665, 'grad_norm': 2.2944474288665413, 'learning_rate': 1.309361868542893e-05, 'epoch': 11.75}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 740/1260 [4:28:55<3:07:13, 21.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 741/1260 [4:29:17<3:06:56, 21.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 742/1260 [4:29:39<3:07:26, 21.71s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 743/1260 [4:30:00<3:06:48, 21.68s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 744/1260 [4:30:22<3:06:36, 21.70s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 745/1260 [4:30:44<3:06:12, 21.69s/it]                                                      {'loss': 0.072, 'grad_norm': 3.3997681403897575, 'learning_rate': 1.288771575484145e-05, 'epoch': 11.83}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 745/1260 [4:30:44<3:06:12, 21.69s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 746/1260 [4:31:05<3:05:45, 21.68s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 747/1260 [4:31:27<3:04:45, 21.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 748/1260 [4:31:48<3:04:04, 21.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 749/1260 [4:32:10<3:04:00, 21.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 750/1260 [4:32:31<3:03:01, 21.53s/it]                                                      {'loss': 0.0778, 'grad_norm': 2.4136654624578306, 'learning_rate': 1.2682218107882393e-05, 'epoch': 11.91}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 750/1260 [4:32:31<3:03:01, 21.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 751/1260 [4:32:53<3:02:25, 21.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 752/1260 [4:33:14<3:01:53, 21.48s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 753/1260 [4:33:36<3:02:01, 21.54s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 754/1260 [4:33:57<3:01:34, 21.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 755/1260 [4:34:19<3:00:59, 21.50s/it]                                                      {'loss': 0.0713, 'grad_norm': 1.5182204944891997, 'learning_rate': 1.2477165173351256e-05, 'epoch': 11.99}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 755/1260 [4:34:19<3:00:59, 21.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 756/1260 [4:34:39<2:56:26, 21.01s/it][INFO|trainer.py:3993] 2025-07-05 02:07:47,404 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756
[INFO|configuration_utils.py:424] 2025-07-05 02:07:47,410 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:07:47,411 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:07:50,439 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:07:50,441 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:07:50,442 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:07:50,442 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/special_tokens_map.json
[2025-07-05 02:07:50,634] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step754 is about to be saved!
[2025-07-05 02:07:50,644] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:07:50,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:07:50,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:07:50,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:07:55,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:07:55,211] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:07:58,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step754 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:07:58,548 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:07:58,549 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:07:58,550 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:07:58,550 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:07:58,706 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:07:59,111 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-756/chat_template.jinja
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 757/1260 [4:35:15<3:36:12, 25.79s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 758/1260 [4:35:37<3:24:59, 24.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 759/1260 [4:35:59<3:17:08, 23.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 760/1260 [4:36:20<3:11:44, 23.01s/it]                                                      {'loss': 0.0502, 'grad_norm': 1.3916856689758301, 'learning_rate': 1.227259629472064e-05, 'epoch': 12.06}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 760/1260 [4:36:20<3:11:44, 23.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 761/1260 [4:36:42<3:07:43, 22.57s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 762/1260 [4:37:03<3:05:19, 22.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 763/1260 [4:37:25<3:03:07, 22.11s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 764/1260 [4:37:47<3:01:37, 21.97s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 765/1260 [4:38:08<2:59:54, 21.81s/it]                                                      {'loss': 0.0459, 'grad_norm': 1.189483367216191, 'learning_rate': 1.206855072258742e-05, 'epoch': 12.14}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 765/1260 [4:38:08<2:59:54, 21.81s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 766/1260 [4:38:30<2:58:51, 21.72s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 767/1260 [4:38:51<2:58:40, 21.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 768/1260 [4:39:13<2:58:22, 21.75s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 769/1260 [4:39:35<2:57:13, 21.66s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 770/1260 [4:39:56<2:56:21, 21.59s/it]                                                      {'loss': 0.0449, 'grad_norm': 1.255380564704539, 'learning_rate': 1.1865067607141743e-05, 'epoch': 12.22}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 770/1260 [4:39:56<2:56:21, 21.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 771/1260 [4:40:18<2:55:55, 21.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 772/1260 [4:40:39<2:55:30, 21.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 773/1260 [4:41:01<2:54:59, 21.56s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 774/1260 [4:41:22<2:54:56, 21.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 775/1260 [4:41:44<2:55:30, 21.71s/it]                                                      {'loss': 0.043, 'grad_norm': 1.1861423299300005, 'learning_rate': 1.1662185990655285e-05, 'epoch': 12.3}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 775/1260 [4:41:44<2:55:30, 21.71s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 776/1260 [4:42:06<2:54:28, 21.63s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 777/1260 [4:42:28<2:54:19, 21.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 778/1260 [4:42:49<2:53:28, 21.59s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 779/1260 [4:43:10<2:52:48, 21.56s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 780/1260 [4:43:32<2:53:23, 21.67s/it]                                                      {'loss': 0.0438, 'grad_norm': 1.8333473237631133, 'learning_rate': 1.1459944799990203e-05, 'epoch': 12.38}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 780/1260 [4:43:32<2:53:23, 21.67s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 781/1260 [4:43:54<2:52:29, 21.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 782/1260 [4:44:15<2:51:39, 21.55s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 783/1260 [4:44:37<2:52:02, 21.64s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 784/1260 [4:44:59<2:51:15, 21.59s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 785/1260 [4:45:20<2:51:02, 21.60s/it]                                                      {'loss': 0.0451, 'grad_norm': 1.272541596284852, 'learning_rate': 1.1258382839130282e-05, 'epoch': 12.46}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 785/1260 [4:45:20<2:51:02, 21.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 786/1260 [4:45:42<2:50:09, 21.54s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 787/1260 [4:46:03<2:49:48, 21.54s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 788/1260 [4:46:25<2:49:44, 21.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 789/1260 [4:46:46<2:49:05, 21.54s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 790/1260 [4:47:08<2:49:03, 21.58s/it]                                                      {'loss': 0.0465, 'grad_norm': 1.6040267309368823, 'learning_rate': 1.1057538781735571e-05, 'epoch': 12.54}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 790/1260 [4:47:08<2:49:03, 21.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 791/1260 [4:47:29<2:48:20, 21.54s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 792/1260 [4:47:51<2:47:42, 21.50s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 793/1260 [4:48:13<2:48:09, 21.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 794/1260 [4:48:34<2:48:21, 21.68s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 795/1260 [4:48:56<2:48:38, 21.76s/it]                                                      {'loss': 0.0452, 'grad_norm': 1.383153423788851, 'learning_rate': 1.0857451163722119e-05, 'epoch': 12.62}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 795/1260 [4:48:56<2:48:38, 21.76s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 796/1260 [4:49:18<2:47:47, 21.70s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 797/1260 [4:49:39<2:46:54, 21.63s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 798/1260 [4:50:01<2:45:55, 21.55s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 799/1260 [4:50:22<2:45:40, 21.56s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 800/1260 [4:50:44<2:45:25, 21.58s/it]                                                      {'loss': 0.0447, 'grad_norm': 2.880697201305092, 'learning_rate': 1.0658158375868056e-05, 'epoch': 12.7}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 800/1260 [4:50:44<2:45:25, 21.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 801/1260 [4:51:06<2:45:28, 21.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 802/1260 [4:51:27<2:44:48, 21.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 803/1260 [4:51:49<2:44:34, 21.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 804/1260 [4:52:11<2:44:17, 21.62s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 805/1260 [4:52:32<2:44:22, 21.68s/it]                                                      {'loss': 0.0429, 'grad_norm': 1.1253819851342852, 'learning_rate': 1.0459698656447612e-05, 'epoch': 12.78}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 805/1260 [4:52:32<2:44:22, 21.68s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 806/1260 [4:52:54<2:43:44, 21.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 807/1260 [4:53:15<2:43:09, 21.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 808/1260 [4:53:37<2:42:44, 21.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 809/1260 [4:53:59<2:42:05, 21.56s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 810/1260 [4:54:20<2:41:47, 21.57s/it]                                                      {'loss': 0.0425, 'grad_norm': 1.1055802358560014, 'learning_rate': 1.0262110083894285e-05, 'epoch': 12.86}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 810/1260 [4:54:20<2:41:47, 21.57s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 811/1260 [4:54:42<2:41:07, 21.53s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 812/1260 [4:55:03<2:40:37, 21.51s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 813/1260 [4:55:25<2:40:19, 21.52s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 814/1260 [4:55:46<2:40:17, 21.56s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 815/1260 [4:56:08<2:40:13, 21.60s/it]                                                      {'loss': 0.0429, 'grad_norm': 1.3064767573469864, 'learning_rate': 1.0065430569494785e-05, 'epoch': 12.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 815/1260 [4:56:08<2:40:13, 21.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 816/1260 [4:56:30<2:39:59, 21.62s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 817/1260 [4:56:51<2:40:08, 21.69s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 818/1260 [4:57:13<2:39:12, 21.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 819/1260 [4:57:32<2:34:28, 21.02s/it][INFO|trainer.py:3993] 2025-07-05 02:30:41,234 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819
[INFO|configuration_utils.py:424] 2025-07-05 02:30:41,239 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:30:41,240 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:30:44,459 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:30:44,461 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:30:44,461 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:30:44,462 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/special_tokens_map.json
[2025-07-05 02:30:44,643] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step817 is about to be saved!
[2025-07-05 02:30:44,654] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:30:44,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:30:44,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:30:44,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:30:49,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:30:49,583] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:30:52,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step817 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:30:52,648 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:30:52,649 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:30:52,649 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:30:52,650 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:30:52,807 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:30:53,222 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-819/chat_template.jinja
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 820/1260 [4:58:09<3:08:54, 25.76s/it]                                                      {'loss': 0.0405, 'grad_norm': 1.557292058849046, 'learning_rate': 9.86969785011497e-06, 'epoch': 13.02}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 820/1260 [4:58:09<3:08:54, 25.76s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 821/1260 [4:58:31<2:59:00, 24.46s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 822/1260 [4:58:53<2:53:11, 23.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 823/1260 [4:59:14<2:48:01, 23.07s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 824/1260 [4:59:36<2:43:50, 22.55s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 825/1260 [4:59:57<2:40:57, 22.20s/it]                                                      {'loss': 0.029, 'grad_norm': 1.1221133971102797, 'learning_rate': 9.67494948095931e-06, 'epoch': 13.1}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 825/1260 [4:59:57<2:40:57, 22.20s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 826/1260 [5:00:18<2:38:52, 21.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 827/1260 [5:00:40<2:37:28, 21.82s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 828/1260 [5:01:01<2:36:32, 21.74s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 829/1260 [5:01:24<2:37:08, 21.88s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 830/1260 [5:01:45<2:35:58, 21.76s/it]                                                      {'loss': 0.0307, 'grad_norm': 1.1764694853009012, 'learning_rate': 9.481222828365151e-06, 'epoch': 13.17}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 830/1260 [5:01:45<2:35:58, 21.76s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 831/1260 [5:02:07<2:35:02, 21.68s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 832/1260 [5:02:28<2:34:20, 21.64s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 833/1260 [5:02:50<2:33:33, 21.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 834/1260 [5:03:11<2:33:20, 21.60s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 835/1260 [5:03:33<2:32:37, 21.55s/it]                                                      {'loss': 0.0291, 'grad_norm': 1.1638634366540677, 'learning_rate': 9.288555062633258e-06, 'epoch': 13.25}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 835/1260 [5:03:33<2:32:37, 21.55s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 836/1260 [5:03:55<2:32:59, 21.65s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 837/1260 [5:04:16<2:32:11, 21.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 838/1260 [5:04:37<2:31:31, 21.54s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 839/1260 [5:04:59<2:31:03, 21.53s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 840/1260 [5:05:21<2:30:54, 21.56s/it]                                                      {'loss': 0.0287, 'grad_norm': 1.2140584630800284, 'learning_rate': 9.096983150895936e-06, 'epoch': 13.33}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 840/1260 [5:05:21<2:30:54, 21.56s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 841/1260 [5:05:43<2:31:35, 21.71s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 842/1260 [5:06:04<2:31:04, 21.69s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 843/1260 [5:06:26<2:31:10, 21.75s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 844/1260 [5:06:48<2:31:22, 21.83s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 845/1260 [5:07:10<2:30:41, 21.79s/it]                                                      {'loss': 0.0286, 'grad_norm': 1.0372574550607336, 'learning_rate': 8.906543850024186e-06, 'epoch': 13.41}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 845/1260 [5:07:10<2:30:41, 21.79s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 846/1260 [5:07:32<2:30:36, 21.83s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 847/1260 [5:07:53<2:29:46, 21.76s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 848/1260 [5:08:15<2:28:48, 21.67s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 849/1260 [5:08:37<2:28:34, 21.69s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 850/1260 [5:08:58<2:27:48, 21.63s/it]                                                      {'loss': 0.0311, 'grad_norm': 1.7367468313122025, 'learning_rate': 8.71727369957513e-06, 'epoch': 13.49}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 850/1260 [5:08:58<2:27:48, 21.63s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 851/1260 [5:09:20<2:27:22, 21.62s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 852/1260 [5:09:41<2:26:50, 21.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 853/1260 [5:10:03<2:26:02, 21.53s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 854/1260 [5:10:24<2:26:00, 21.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 855/1260 [5:10:46<2:25:29, 21.56s/it]                                                      {'loss': 0.032, 'grad_norm': 1.8703971301581237, 'learning_rate': 8.529209014781202e-06, 'epoch': 13.57}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 855/1260 [5:10:46<2:25:29, 21.56s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 856/1260 [5:11:07<2:25:20, 21.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 857/1260 [5:11:29<2:25:07, 21.61s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 858/1260 [5:11:51<2:24:41, 21.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 859/1260 [5:12:12<2:24:21, 21.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 860/1260 [5:12:34<2:24:09, 21.62s/it]                                                      {'loss': 0.0311, 'grad_norm': 1.0031179102234193, 'learning_rate': 8.342385879582332e-06, 'epoch': 13.65}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 860/1260 [5:12:34<2:24:09, 21.62s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 861/1260 [5:12:56<2:23:50, 21.63s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 862/1260 [5:13:17<2:23:33, 21.64s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 863/1260 [5:13:39<2:23:58, 21.76s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 864/1260 [5:14:01<2:23:26, 21.73s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 865/1260 [5:14:22<2:22:31, 21.65s/it]                                                      {'loss': 0.0306, 'grad_norm': 1.635456236039372, 'learning_rate': 8.156840139702554e-06, 'epoch': 13.73}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 865/1260 [5:14:22<2:22:31, 21.65s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 866/1260 [5:14:44<2:21:46, 21.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 867/1260 [5:15:05<2:21:13, 21.56s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 868/1260 [5:15:27<2:21:43, 21.69s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 869/1260 [5:15:49<2:21:23, 21.70s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 870/1260 [5:16:11<2:20:42, 21.65s/it]                                                      {'loss': 0.0308, 'grad_norm': 1.5591937356750123, 'learning_rate': 7.972607395772256e-06, 'epoch': 13.81}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 870/1260 [5:16:11<2:20:42, 21.65s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 871/1260 [5:16:32<2:19:52, 21.57s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 872/1260 [5:16:54<2:19:36, 21.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 873/1260 [5:17:15<2:19:02, 21.56s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 874/1260 [5:17:37<2:18:24, 21.51s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 875/1260 [5:17:58<2:17:59, 21.51s/it]                                                      {'loss': 0.0285, 'grad_norm': 0.9052758510529079, 'learning_rate': 7.789722996497514e-06, 'epoch': 13.89}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 875/1260 [5:17:58<2:17:59, 21.51s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 876/1260 [5:18:20<2:18:01, 21.57s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 877/1260 [5:18:42<2:18:20, 21.67s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 878/1260 [5:19:03<2:17:36, 21.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 879/1260 [5:19:25<2:17:33, 21.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 880/1260 [5:19:46<2:16:44, 21.59s/it]                                                      {'loss': 0.0294, 'grad_norm': 1.782255579585005, 'learning_rate': 7.608222031877732e-06, 'epoch': 13.97}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 880/1260 [5:19:46<2:16:44, 21.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 881/1260 [5:20:08<2:16:12, 21.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 882/1260 [5:20:27<2:12:00, 20.95s/it][INFO|trainer.py:3993] 2025-07-05 02:53:36,951 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882
[INFO|configuration_utils.py:424] 2025-07-05 02:53:36,957 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:53:36,958 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:53:39,974 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:53:39,976 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:53:39,976 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:53:39,977 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/special_tokens_map.json
[2025-07-05 02:53:40,170] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step880 is about to be saved!
[2025-07-05 02:53:40,182] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:53:40,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:53:40,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:53:40,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:53:44,620] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:53:44,622] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:53:48,108] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step880 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:53:48,118 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:53:48,118 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:53:48,119 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:53:48,119 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:53:48,281 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:53:48,687 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-882/chat_template.jinja
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 883/1260 [5:21:05<2:42:30, 25.86s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 884/1260 [5:21:26<2:33:55, 24.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 885/1260 [5:21:48<2:28:13, 23.72s/it]                                                      {'loss': 0.0224, 'grad_norm': 1.270430829258417, 'learning_rate': 7.4281393264729584e-06, 'epoch': 14.05}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 885/1260 [5:21:48<2:28:13, 23.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 886/1260 [5:22:10<2:24:35, 23.20s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 887/1260 [5:22:31<2:21:04, 22.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 888/1260 [5:22:53<2:19:02, 22.43s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 889/1260 [5:23:15<2:16:48, 22.12s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 890/1260 [5:23:36<2:15:22, 21.95s/it]                                                      {'loss': 0.0184, 'grad_norm': 1.2302497413103282, 'learning_rate': 7.249509432722056e-06, 'epoch': 14.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 890/1260 [5:23:36<2:15:22, 21.95s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 891/1260 [5:23:58<2:14:26, 21.86s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 892/1260 [5:24:19<2:13:09, 21.71s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 893/1260 [5:24:41<2:12:13, 21.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 894/1260 [5:25:02<2:11:58, 21.63s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 895/1260 [5:25:24<2:11:44, 21.66s/it]                                                      {'loss': 0.0188, 'grad_norm': 1.3959834952371295, 'learning_rate': 7.072366624313169e-06, 'epoch': 14.21}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 895/1260 [5:25:24<2:11:44, 21.66s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 896/1260 [5:25:46<2:11:02, 21.60s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 897/1260 [5:26:07<2:10:57, 21.64s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 898/1260 [5:26:29<2:10:15, 21.59s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 899/1260 [5:26:50<2:09:55, 21.59s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 900/1260 [5:27:12<2:09:19, 21.55s/it]                                                      {'loss': 0.0173, 'grad_norm': 1.236813833051355, 'learning_rate': 6.896744889607612e-06, 'epoch': 14.29}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 900/1260 [5:27:12<2:09:19, 21.55s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 901/1260 [5:27:33<2:08:49, 21.53s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 902/1260 [5:27:55<2:08:56, 21.61s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 903/1260 [5:28:17<2:08:44, 21.64s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 904/1260 [5:28:38<2:07:55, 21.56s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 905/1260 [5:29:00<2:08:06, 21.65s/it]                                                      {'loss': 0.0178, 'grad_norm': 0.9939384202402752, 'learning_rate': 6.722677925118561e-06, 'epoch': 14.37}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 905/1260 [5:29:00<2:08:06, 21.65s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 906/1260 [5:29:21<2:07:20, 21.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 907/1260 [5:29:43<2:07:43, 21.71s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 908/1260 [5:30:05<2:06:44, 21.60s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 909/1260 [5:30:26<2:06:18, 21.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 910/1260 [5:30:48<2:06:03, 21.61s/it]                                                      {'loss': 0.0173, 'grad_norm': 0.910726788804039, 'learning_rate': 6.55019912904567e-06, 'epoch': 14.45}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 910/1260 [5:30:48<2:06:03, 21.61s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 911/1260 [5:31:10<2:05:36, 21.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 912/1260 [5:31:31<2:04:55, 21.54s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 913/1260 [5:31:52<2:04:24, 21.51s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 914/1260 [5:32:14<2:04:04, 21.52s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 915/1260 [5:32:35<2:03:30, 21.48s/it]                                                      {'loss': 0.0184, 'grad_norm': 1.6192393730922157, 'learning_rate': 6.379341594866983e-06, 'epoch': 14.52}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 915/1260 [5:32:35<2:03:30, 21.48s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 916/1260 [5:32:57<2:02:57, 21.45s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 917/1260 [5:33:18<2:02:35, 21.44s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 918/1260 [5:33:40<2:02:27, 21.48s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 919/1260 [5:34:02<2:02:55, 21.63s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 920/1260 [5:34:24<2:02:56, 21.70s/it]                                                      {'loss': 0.0202, 'grad_norm': 0.991317370191773, 'learning_rate': 6.2101381049893e-06, 'epoch': 14.6}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 920/1260 [5:34:24<2:02:56, 21.70s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 921/1260 [5:34:45<2:02:10, 21.62s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 922/1260 [5:35:06<2:01:33, 21.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 923/1260 [5:35:28<2:01:09, 21.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 924/1260 [5:35:49<2:00:32, 21.53s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 925/1260 [5:36:11<2:00:27, 21.57s/it]                                                      {'loss': 0.0182, 'grad_norm': 1.4490939314576856, 'learning_rate': 6.0426211244582105e-06, 'epoch': 14.68}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 925/1260 [5:36:11<2:00:27, 21.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 926/1260 [5:36:33<2:00:01, 21.56s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 927/1260 [5:36:54<2:00:01, 21.63s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 928/1260 [5:37:16<1:59:14, 21.55s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 929/1260 [5:37:37<1:58:49, 21.54s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 930/1260 [5:37:59<1:58:10, 21.49s/it]                                                      {'loss': 0.0173, 'grad_norm': 1.7066974093928184, 'learning_rate': 5.876822794729035e-06, 'epoch': 14.76}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 930/1260 [5:37:59<1:58:10, 21.49s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 931/1260 [5:38:20<1:57:57, 21.51s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 932/1260 [5:38:42<1:57:49, 21.55s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 933/1260 [5:39:04<1:57:34, 21.57s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 934/1260 [5:39:25<1:56:51, 21.51s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 935/1260 [5:39:46<1:56:16, 21.47s/it]                                                      {'loss': 0.017, 'grad_norm': 0.7605435066899506, 'learning_rate': 5.712774927499851e-06, 'epoch': 14.84}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 935/1260 [5:39:46<1:56:16, 21.47s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 936/1260 [5:40:08<1:55:50, 21.45s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 937/1260 [5:40:29<1:55:31, 21.46s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 938/1260 [5:40:51<1:55:12, 21.47s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 939/1260 [5:41:12<1:54:54, 21.48s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 940/1260 [5:41:34<1:54:29, 21.47s/it]                                                      {'loss': 0.0168, 'grad_norm': 0.91392758939628, 'learning_rate': 5.55050899860778e-06, 'epoch': 14.92}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 940/1260 [5:41:34<1:54:29, 21.47s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 941/1260 [5:41:55<1:54:46, 21.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 942/1260 [5:42:18<1:55:09, 21.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 943/1260 [5:42:39<1:54:19, 21.64s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 944/1260 [5:43:00<1:53:33, 21.56s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 945/1260 [5:43:20<1:50:09, 20.98s/it]                                                      {'loss': 0.0165, 'grad_norm': 0.7849509905969044, 'learning_rate': 5.390056141989745e-06, 'epoch': 15.0}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 945/1260 [5:43:20<1:50:09, 20.98s/it][INFO|trainer.py:3993] 2025-07-05 03:16:29,289 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945
[INFO|configuration_utils.py:424] 2025-07-05 03:16:29,295 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/config.json
[INFO|configuration_utils.py:904] 2025-07-05 03:16:29,296 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 03:16:32,471 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:16:32,473 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:16:32,474 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:16:32,475 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/special_tokens_map.json
[2025-07-05 03:16:32,666] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step943 is about to be saved!
[2025-07-05 03:16:32,677] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 03:16:32,677] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 03:16:32,711] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 03:16:32,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 03:16:37,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 03:16:37,071] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 03:16:40,603] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step943 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 03:16:40,615 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:16:40,617 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:16:40,617 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:16:40,618 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 03:16:40,783 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 03:16:41,188 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-945/chat_template.jinja
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 946/1260 [5:43:57<2:15:25, 25.88s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 947/1260 [5:44:19<2:08:48, 24.69s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 948/1260 [5:44:41<2:03:22, 23.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 949/1260 [5:45:02<1:59:42, 23.09s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 950/1260 [5:45:24<1:56:45, 22.60s/it]                                                      {'loss': 0.0111, 'grad_norm': 1.6779241416705852, 'learning_rate': 5.231447143708774e-06, 'epoch': 15.08}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 950/1260 [5:45:24<1:56:45, 22.60s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 951/1260 [5:45:45<1:54:50, 22.30s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 952/1260 [5:46:07<1:53:05, 22.03s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 953/1260 [5:46:28<1:51:48, 21.85s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 954/1260 [5:46:50<1:50:51, 21.74s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 955/1260 [5:47:11<1:50:28, 21.73s/it]                                                      {'loss': 0.0115, 'grad_norm': 1.744630295779852, 'learning_rate': 5.0747124360471125e-06, 'epoch': 15.16}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 955/1260 [5:47:11<1:50:28, 21.73s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 956/1260 [5:47:33<1:49:37, 21.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 957/1260 [5:47:54<1:48:53, 21.56s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 958/1260 [5:48:16<1:48:30, 21.56s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 959/1260 [5:48:37<1:47:53, 21.51s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 960/1260 [5:48:59<1:47:24, 21.48s/it]                                                      {'loss': 0.0108, 'grad_norm': 1.8164698523976797, 'learning_rate': 4.9198820916671634e-06, 'epoch': 15.24}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 960/1260 [5:48:59<1:47:24, 21.48s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 961/1260 [5:49:20<1:47:15, 21.52s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 962/1260 [5:49:42<1:47:36, 21.67s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 963/1260 [5:50:04<1:47:26, 21.71s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 964/1260 [5:50:26<1:46:53, 21.67s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 965/1260 [5:50:47<1:46:49, 21.73s/it]                                                      {'loss': 0.011, 'grad_norm': 1.746920467395347, 'learning_rate': 4.766985817841482e-06, 'epoch': 15.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 965/1260 [5:50:47<1:46:49, 21.73s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 966/1260 [5:51:09<1:46:03, 21.65s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 967/1260 [5:51:31<1:45:55, 21.69s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 968/1260 [5:51:52<1:45:45, 21.73s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 969/1260 [5:52:14<1:45:11, 21.69s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 970/1260 [5:52:36<1:44:56, 21.71s/it]                                                      {'loss': 0.0106, 'grad_norm': 0.988002571899811, 'learning_rate': 4.616052950752807e-06, 'epoch': 15.4}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 970/1260 [5:52:36<1:44:56, 21.71s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 971/1260 [5:52:58<1:44:50, 21.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 972/1260 [5:53:19<1:44:24, 21.75s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 973/1260 [5:53:41<1:43:33, 21.65s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 974/1260 [5:54:02<1:42:46, 21.56s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 975/1260 [5:54:24<1:42:13, 21.52s/it]                                                      {'loss': 0.0092, 'grad_norm': 0.6555102119760801, 'learning_rate': 4.4671124498653624e-06, 'epoch': 15.48}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 975/1260 [5:54:24<1:42:13, 21.52s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 976/1260 [5:54:45<1:42:16, 21.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 977/1260 [5:55:07<1:41:53, 21.60s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 978/1260 [5:55:29<1:41:38, 21.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 979/1260 [5:55:50<1:41:00, 21.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 980/1260 [5:56:12<1:40:22, 21.51s/it]                                                      {'loss': 0.0112, 'grad_norm': 1.4258101524813807, 'learning_rate': 4.320192892368389e-06, 'epoch': 15.56}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 980/1260 [5:56:12<1:40:22, 21.51s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 981/1260 [5:56:33<1:40:22, 21.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 982/1260 [5:56:55<1:39:39, 21.51s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 983/1260 [5:57:16<1:39:09, 21.48s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 984/1260 [5:57:38<1:39:14, 21.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 985/1260 [5:57:59<1:38:56, 21.59s/it]                                                      {'loss': 0.0096, 'grad_norm': 1.0099876716342968, 'learning_rate': 4.175322467693068e-06, 'epoch': 15.64}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 985/1260 [5:57:59<1:38:56, 21.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 986/1260 [5:58:21<1:39:09, 21.71s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 987/1260 [5:58:43<1:38:28, 21.64s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 988/1260 [5:59:05<1:38:15, 21.68s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 989/1260 [5:59:26<1:37:33, 21.60s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 990/1260 [5:59:48<1:36:59, 21.55s/it]                                                      {'loss': 0.01, 'grad_norm': 0.9504438035344999, 'learning_rate': 4.032528972103797e-06, 'epoch': 15.72}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 990/1260 [5:59:48<1:36:59, 21.55s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 991/1260 [6:00:09<1:36:25, 21.51s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 992/1260 [6:00:31<1:36:10, 21.53s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 993/1260 [6:00:52<1:35:40, 21.50s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 994/1260 [6:01:13<1:35:18, 21.50s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 995/1260 [6:01:35<1:34:51, 21.48s/it]                                                      {'loss': 0.0092, 'grad_norm': 0.7695124541416163, 'learning_rate': 3.891839803364934e-06, 'epoch': 15.8}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 995/1260 [6:01:35<1:34:51, 21.48s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 996/1260 [6:01:56<1:34:23, 21.45s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 997/1260 [6:02:18<1:34:06, 21.47s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 998/1260 [6:02:39<1:33:46, 21.48s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 999/1260 [6:03:01<1:33:42, 21.54s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1260 [6:03:23<1:33:39, 21.61s/it]                                                       {'loss': 0.0095, 'grad_norm': 0.5554632425863063, 'learning_rate': 3.7532819554839853e-06, 'epoch': 15.87}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1260 [6:03:23<1:33:39, 21.61s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1001/1260 [6:03:44<1:33:03, 21.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1002/1260 [6:04:06<1:32:51, 21.60s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1003/1260 [6:04:27<1:32:24, 21.57s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1004/1260 [6:04:49<1:32:22, 21.65s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1005/1260 [6:05:11<1:31:44, 21.59s/it]                                                       {'loss': 0.0094, 'grad_norm': 0.7412211622758849, 'learning_rate': 3.6168820135322987e-06, 'epoch': 15.95}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1005/1260 [6:05:11<1:31:44, 21.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1006/1260 [6:05:32<1:31:24, 21.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1007/1260 [6:05:54<1:30:48, 21.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1008/1260 [6:06:13<1:27:56, 20.94s/it][INFO|trainer.py:3993] 2025-07-05 03:39:22,060 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008
[INFO|configuration_utils.py:424] 2025-07-05 03:39:22,066 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/config.json
[INFO|configuration_utils.py:904] 2025-07-05 03:39:22,067 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 03:39:25,084 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:39:25,086 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:39:25,087 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:39:25,087 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/special_tokens_map.json
[2025-07-05 03:39:25,279] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1006 is about to be saved!
[2025-07-05 03:39:25,290] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 03:39:25,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 03:39:25,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 03:39:25,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 03:39:29,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 03:39:29,813] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 03:39:33,237] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1006 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 03:39:33,249 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:39:33,249 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:39:33,250 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:39:33,250 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 03:39:33,406 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 03:39:33,824 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1008/chat_template.jinja
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1009/1260 [6:06:50<1:47:28, 25.69s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1010/1260 [6:07:12<1:41:51, 24.45s/it]                                                       {'loss': 0.0075, 'grad_norm': 0.4718383284844298, 'learning_rate': 3.482666148544146e-06, 'epoch': 16.03}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1010/1260 [6:07:12<1:41:51, 24.45s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1011/1260 [6:07:33<1:37:44, 23.55s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1012/1260 [6:07:54<1:34:34, 22.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1013/1260 [6:08:16<1:32:47, 22.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1014/1260 [6:08:37<1:31:02, 22.21s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1015/1260 [6:08:59<1:30:03, 22.06s/it]                                                       {'loss': 0.0061, 'grad_norm': 0.7234080245630818, 'learning_rate': 3.3506601124953246e-06, 'epoch': 16.11}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1015/1260 [6:08:59<1:30:03, 22.06s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1016/1260 [6:09:21<1:28:56, 21.87s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1017/1260 [6:09:42<1:28:30, 21.85s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1018/1260 [6:10:04<1:27:33, 21.71s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1019/1260 [6:10:26<1:27:24, 21.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1020/1260 [6:10:47<1:26:38, 21.66s/it]                                                       {'loss': 0.0048, 'grad_norm': 0.5438404783696259, 'learning_rate': 3.220889233362113e-06, 'epoch': 16.19}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1020/1260 [6:10:47<1:26:38, 21.66s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1021/1260 [6:11:09<1:26:16, 21.66s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1022/1260 [6:11:31<1:26:18, 21.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1023/1260 [6:11:52<1:25:30, 21.65s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1024/1260 [6:12:14<1:25:38, 21.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1025/1260 [6:12:36<1:24:55, 21.68s/it]                                                       {'loss': 0.0057, 'grad_norm': 0.591847443020294, 'learning_rate': 3.0933784102616147e-06, 'epoch': 16.27}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1025/1260 [6:12:36<1:24:55, 21.68s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1026/1260 [6:12:57<1:24:14, 21.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1027/1260 [6:13:19<1:24:09, 21.67s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1028/1260 [6:13:41<1:23:42, 21.65s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1029/1260 [6:14:02<1:22:59, 21.56s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1030/1260 [6:14:23<1:22:29, 21.52s/it]                                                       {'loss': 0.0051, 'grad_norm': 0.4620038994083959, 'learning_rate': 2.9681521086743426e-06, 'epoch': 16.35}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1030/1260 [6:14:23<1:22:29, 21.52s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1031/1260 [6:14:45<1:22:00, 21.49s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1032/1260 [6:15:06<1:21:33, 21.46s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1033/1260 [6:15:27<1:21:04, 21.43s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1034/1260 [6:15:49<1:20:41, 21.42s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1035/1260 [6:16:10<1:20:22, 21.43s/it]                                                       {'loss': 0.0052, 'grad_norm': 0.7406884010394875, 'learning_rate': 2.845234355750051e-06, 'epoch': 16.43}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1035/1260 [6:16:10<1:20:22, 21.43s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1036/1260 [6:16:32<1:20:02, 21.44s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1037/1260 [6:16:54<1:20:00, 21.53s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1038/1260 [6:17:15<1:19:39, 21.53s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1039/1260 [6:17:36<1:19:07, 21.48s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1040/1260 [6:17:58<1:19:14, 21.61s/it]                                                       {'loss': 0.005, 'grad_norm': 0.9544187996169812, 'learning_rate': 2.7246487356976447e-06, 'epoch': 16.51}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1040/1260 [6:17:58<1:19:14, 21.61s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1041/1260 [6:18:20<1:18:49, 21.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1042/1260 [6:18:42<1:18:33, 21.62s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1043/1260 [6:19:03<1:18:13, 21.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1044/1260 [6:19:25<1:17:53, 21.64s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1045/1260 [6:19:46<1:17:19, 21.58s/it]                                                       {'loss': 0.005, 'grad_norm': 1.2065130889206246, 'learning_rate': 2.60641838526008e-06, 'epoch': 16.59}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1045/1260 [6:19:46<1:17:19, 21.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1046/1260 [6:20:08<1:17:11, 21.64s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1047/1260 [6:20:30<1:16:39, 21.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1048/1260 [6:20:51<1:16:18, 21.60s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1049/1260 [6:21:13<1:15:43, 21.53s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1050/1260 [6:21:34<1:15:31, 21.58s/it]                                                       {'loss': 0.0048, 'grad_norm': 0.6630049966893697, 'learning_rate': 2.490565989275118e-06, 'epoch': 16.67}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1050/1260 [6:21:34<1:15:31, 21.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1051/1260 [6:21:56<1:14:55, 21.51s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1052/1260 [6:22:17<1:14:50, 21.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1053/1260 [6:22:39<1:14:25, 21.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1054/1260 [6:23:00<1:13:52, 21.52s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1055/1260 [6:23:22<1:13:26, 21.49s/it]                                                       {'loss': 0.0043, 'grad_norm': 0.607632350633441, 'learning_rate': 2.3771137763228014e-06, 'epoch': 16.75}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1055/1260 [6:23:22<1:13:26, 21.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1056/1260 [6:23:44<1:13:27, 21.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1057/1260 [6:24:06<1:13:23, 21.69s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1058/1260 [6:24:27<1:12:48, 21.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1059/1260 [6:24:49<1:12:27, 21.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1060/1260 [6:25:10<1:12:02, 21.61s/it]                                                       {'loss': 0.0048, 'grad_norm': 0.6333626755040347, 'learning_rate': 2.2660835144604407e-06, 'epoch': 16.83}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1060/1260 [6:25:10<1:12:02, 21.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1061/1260 [6:25:32<1:11:25, 21.53s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1062/1260 [6:25:53<1:11:10, 21.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1063/1260 [6:26:15<1:10:42, 21.54s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1064/1260 [6:26:36<1:10:10, 21.48s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1065/1260 [6:26:58<1:09:56, 21.52s/it]                                                       {'loss': 0.0048, 'grad_norm': 0.4902127560373529, 'learning_rate': 2.1574965070460047e-06, 'epoch': 16.91}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1065/1260 [6:26:58<1:09:56, 21.52s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1066/1260 [6:27:19<1:09:45, 21.57s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1067/1260 [6:27:41<1:09:12, 21.51s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1068/1260 [6:28:02<1:08:41, 21.47s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1069/1260 [6:28:23<1:08:15, 21.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1070/1260 [6:28:45<1:07:53, 21.44s/it]                                                       {'loss': 0.0047, 'grad_norm': 0.6522367098512403, 'learning_rate': 2.0513735886506258e-06, 'epoch': 16.99}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1070/1260 [6:28:45<1:07:53, 21.44s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1071/1260 [6:29:04<1:05:34, 20.82s/it][INFO|trainer.py:3993] 2025-07-05 04:02:12,998 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071
[INFO|configuration_utils.py:424] 2025-07-05 04:02:13,003 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:02:13,004 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:02:16,244 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:02:16,246 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:02:16,246 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:02:16,247 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/special_tokens_map.json
[2025-07-05 04:02:16,432] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1068 is about to be saved!
[2025-07-05 04:02:16,445] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:02:16,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:02:16,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:02:16,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:02:21,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:02:21,417] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:02:24,443] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1068 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:02:24,455 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:02:24,456 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:02:24,456 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:02:24,456 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:02:24,611 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:02:25,015 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1071/chat_template.jinja
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1072/1260 [6:29:41<1:20:28, 25.69s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1073/1260 [6:30:03<1:16:29, 24.54s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1074/1260 [6:30:25<1:13:40, 23.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1075/1260 [6:30:47<1:11:20, 23.14s/it]                                                       {'loss': 0.0028, 'grad_norm': 0.28194873700085754, 'learning_rate': 1.947735121061088e-06, 'epoch': 17.06}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1075/1260 [6:30:47<1:11:20, 23.14s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1076/1260 [6:31:08<1:09:24, 22.64s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1077/1260 [6:31:30<1:07:55, 22.27s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1078/1260 [6:31:51<1:06:42, 21.99s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1079/1260 [6:32:13<1:05:54, 21.85s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1080/1260 [6:32:34<1:05:08, 21.71s/it]                                                       {'loss': 0.0027, 'grad_norm': 0.6115569015486748, 'learning_rate': 1.8466009893730057e-06, 'epoch': 17.14}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1080/1260 [6:32:34<1:05:08, 21.71s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1081/1260 [6:32:56<1:04:54, 21.76s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1082/1260 [6:33:17<1:04:12, 21.65s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1083/1260 [6:33:39<1:03:47, 21.63s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1084/1260 [6:34:01<1:03:44, 21.73s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1085/1260 [6:34:23<1:03:32, 21.79s/it]                                                       {'loss': 0.0023, 'grad_norm': 0.2079421642455597, 'learning_rate': 1.7479905981754917e-06, 'epoch': 17.22}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1085/1260 [6:34:23<1:03:32, 21.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1086/1260 [6:34:44<1:02:53, 21.69s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1087/1260 [6:35:05<1:02:15, 21.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1088/1260 [6:35:27<1:01:46, 21.55s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1089/1260 [6:35:49<1:01:37, 21.62s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1090/1260 [6:36:10<1:01:02, 21.55s/it]                                                       {'loss': 0.0027, 'grad_norm': 0.37134791032836917, 'learning_rate': 1.6519228678279718e-06, 'epoch': 17.3}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1090/1260 [6:36:10<1:01:02, 21.55s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1091/1260 [6:36:32<1:00:39, 21.53s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1092/1260 [6:36:53<1:00:15, 21.52s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1093/1260 [6:37:14<59:45, 21.47s/it]   87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1094/1260 [6:37:36<59:37, 21.55s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1095/1260 [6:37:58<59:35, 21.67s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.3050031554047848, 'learning_rate': 1.5584162308299675e-06, 'epoch': 17.38}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1095/1260 [6:37:58<59:35, 21.67s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1096/1260 [6:38:20<59:24, 21.74s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1097/1260 [6:38:41<58:48, 21.65s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1098/1260 [6:39:03<58:13, 21.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1099/1260 [6:39:25<58:10, 21.68s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1100/1260 [6:39:46<57:47, 21.67s/it]                                                     {'loss': 0.0025, 'grad_norm': 0.4778763182024837, 'learning_rate': 1.467488628284434e-06, 'epoch': 17.46}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1100/1260 [6:39:46<57:47, 21.67s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1101/1260 [6:40:08<57:14, 21.60s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1102/1260 [6:40:29<56:54, 21.61s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1103/1260 [6:40:51<56:20, 21.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1104/1260 [6:41:12<55:58, 21.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1105/1260 [6:41:34<55:32, 21.50s/it]                                                     {'loss': 0.0021, 'grad_norm': 0.3841202054926856, 'learning_rate': 1.3791575064554262e-06, 'epoch': 17.54}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1105/1260 [6:41:34<55:32, 21.50s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1106/1260 [6:41:55<55:04, 21.46s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1107/1260 [6:42:17<54:51, 21.51s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1108/1260 [6:42:38<54:37, 21.56s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1109/1260 [6:43:00<54:07, 21.50s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1110/1260 [6:43:21<53:42, 21.48s/it]                                                     {'loss': 0.0024, 'grad_norm': 0.30512205699319306, 'learning_rate': 1.2934398134206604e-06, 'epoch': 17.62}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1110/1260 [6:43:21<53:42, 21.48s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1111/1260 [6:43:43<53:20, 21.48s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1112/1260 [6:44:04<53:08, 21.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1113/1260 [6:44:26<52:52, 21.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1114/1260 [6:44:48<52:46, 21.69s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1115/1260 [6:45:10<52:16, 21.63s/it]                                                     {'loss': 0.0021, 'grad_norm': 0.6259812050801297, 'learning_rate': 1.2103519958197084e-06, 'epoch': 17.7}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1115/1260 [6:45:10<52:16, 21.63s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1116/1260 [6:45:31<52:05, 21.70s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1117/1260 [6:45:53<51:30, 21.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1118/1260 [6:46:14<51:02, 21.56s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1119/1260 [6:46:36<50:41, 21.57s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1120/1260 [6:46:58<50:28, 21.63s/it]                                                     {'loss': 0.0023, 'grad_norm': 0.39954569617387775, 'learning_rate': 1.129909995698377e-06, 'epoch': 17.78}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1120/1260 [6:46:58<50:28, 21.63s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1121/1260 [6:47:19<49:59, 21.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1122/1260 [6:47:41<49:40, 21.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1123/1260 [6:48:02<49:17, 21.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1124/1260 [6:48:24<48:47, 21.53s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1125/1260 [6:48:45<48:30, 21.56s/it]                                                     {'loss': 0.0024, 'grad_norm': 0.39058225122163226, 'learning_rate': 1.052129247449915e-06, 'epoch': 17.86}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1125/1260 [6:48:45<48:30, 21.56s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1126/1260 [6:49:07<48:05, 21.53s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1127/1260 [6:49:28<47:44, 21.54s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1128/1260 [6:49:50<47:33, 21.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1129/1260 [6:50:12<47:16, 21.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1130/1260 [6:50:33<46:47, 21.60s/it]                                                     {'loss': 0.0021, 'grad_norm': 0.30367740045535674, 'learning_rate': 9.77024674853611e-07, 'epoch': 17.94}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1130/1260 [6:50:33<46:47, 21.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1131/1260 [6:50:55<46:46, 21.76s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1132/1260 [6:51:17<46:14, 21.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1133/1260 [6:51:39<45:52, 21.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1134/1260 [6:51:58<44:09, 21.03s/it][INFO|trainer.py:3993] 2025-07-05 04:25:07,603 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134
[INFO|configuration_utils.py:424] 2025-07-05 04:25:07,609 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:25:07,610 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:25:10,484 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:25:10,486 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:25:10,487 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:25:10,488 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/special_tokens_map.json
[2025-07-05 04:25:10,683] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1131 is about to be saved!
[2025-07-05 04:25:10,695] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:25:10,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:25:10,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:25:10,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:25:15,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:25:15,103] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:25:18,723] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1131 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:25:18,734 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:25:18,735 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:25:18,736 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:25:18,736 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:25:18,898 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:25:19,303 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1134/chat_template.jinja
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1135/1260 [6:52:36<54:28, 26.15s/it]                                                     {'loss': 0.0023, 'grad_norm': 0.4270014511619987, 'learning_rate': 9.046106882113753e-07, 'epoch': 18.02}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1135/1260 [6:52:36<54:28, 26.15s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1136/1260 [6:52:58<51:27, 24.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1137/1260 [6:53:20<48:59, 23.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1138/1260 [6:53:41<47:06, 23.17s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1139/1260 [6:54:03<45:40, 22.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1140/1260 [6:54:24<44:41, 22.34s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.24669652673672546, 'learning_rate': 8.349011815828322e-07, 'epoch': 18.1}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1140/1260 [6:54:24<44:41, 22.34s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1141/1260 [6:54:46<43:50, 22.10s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1142/1260 [6:55:08<43:19, 22.03s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1143/1260 [6:55:29<42:39, 21.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1144/1260 [6:55:51<42:06, 21.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1145/1260 [6:56:12<41:33, 21.68s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.24065298621468262, 'learning_rate': 7.679095301194849e-07, 'epoch': 18.17}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1145/1260 [6:56:12<41:33, 21.68s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1146/1260 [6:56:34<41:09, 21.66s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1147/1260 [6:56:56<40:57, 21.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1148/1260 [6:57:18<40:33, 21.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1149/1260 [6:57:39<40:18, 21.79s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1150/1260 [6:58:01<39:47, 21.70s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.27535170023381805, 'learning_rate': 7.036485874984044e-07, 'epoch': 18.25}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1150/1260 [6:58:01<39:47, 21.70s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1151/1260 [6:58:22<39:16, 21.62s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1152/1260 [6:58:44<38:50, 21.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1153/1260 [6:59:05<38:31, 21.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1154/1260 [6:59:27<38:12, 21.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1155/1260 [6:59:49<37:48, 21.60s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.2727710410394903, 'learning_rate': 6.421306834560126e-07, 'epoch': 18.33}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1155/1260 [6:59:49<37:48, 21.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1156/1260 [7:00:11<37:34, 21.68s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1157/1260 [7:00:32<37:04, 21.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1158/1260 [7:00:53<36:35, 21.53s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1159/1260 [7:01:15<36:09, 21.48s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1160/1260 [7:01:36<35:55, 21.55s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.358370127583572, 'learning_rate': 5.83367621422376e-07, 'epoch': 18.41}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1160/1260 [7:01:36<35:55, 21.55s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1161/1260 [7:01:58<35:27, 21.49s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1162/1260 [7:02:19<35:02, 21.45s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1163/1260 [7:02:41<34:41, 21.46s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1164/1260 [7:03:02<34:25, 21.51s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1165/1260 [7:03:24<33:59, 21.47s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.3268054585047722, 'learning_rate': 5.273706762564761e-07, 'epoch': 18.49}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1165/1260 [7:03:24<33:59, 21.47s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1166/1260 [7:03:45<33:40, 21.49s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1167/1260 [7:04:07<33:29, 21.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1168/1260 [7:04:29<33:04, 21.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1169/1260 [7:04:50<32:42, 21.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1170/1260 [7:05:11<32:16, 21.52s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.2235912327065529, 'learning_rate': 4.741505920829131e-07, 'epoch': 18.57}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1170/1260 [7:05:11<32:16, 21.52s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1171/1260 [7:05:33<31:53, 21.50s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1172/1260 [7:05:55<31:35, 21.54s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1173/1260 [7:06:16<31:19, 21.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1174/1260 [7:06:38<30:58, 21.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1175/1260 [7:07:00<30:47, 21.73s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.20161444984971555, 'learning_rate': 4.2371758023042604e-07, 'epoch': 18.65}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1175/1260 [7:07:00<30:47, 21.73s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1176/1260 [7:07:22<30:23, 21.70s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1177/1260 [7:07:43<29:55, 21.64s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1178/1260 [7:08:04<29:28, 21.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1179/1260 [7:08:26<29:13, 21.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1180/1260 [7:08:48<28:45, 21.57s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.1424820461042209, 'learning_rate': 3.760813172726457e-07, 'epoch': 18.73}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1180/1260 [7:08:48<28:45, 21.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1181/1260 [7:09:09<28:27, 21.61s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1182/1260 [7:09:31<28:01, 21.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1183/1260 [7:09:52<27:36, 21.52s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1184/1260 [7:10:14<27:17, 21.54s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1185/1260 [7:10:35<26:56, 21.56s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.2055586318006972, 'learning_rate': 3.312509431714661e-07, 'epoch': 18.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1185/1260 [7:10:35<26:56, 21.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1186/1260 [7:10:57<26:37, 21.59s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1187/1260 [7:11:19<26:13, 21.55s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1188/1260 [7:11:40<25:47, 21.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1189/1260 [7:12:01<25:22, 21.45s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1190/1260 [7:12:23<24:59, 21.43s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.11484039714377968, 'learning_rate': 2.892350595233406e-07, 'epoch': 18.89}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1190/1260 [7:12:23<24:59, 21.43s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1191/1260 [7:12:44<24:43, 21.50s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1192/1260 [7:13:06<24:19, 21.46s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1193/1260 [7:13:27<23:56, 21.44s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1194/1260 [7:13:49<23:39, 21.51s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1195/1260 [7:14:11<23:24, 21.61s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.2315851669607458, 'learning_rate': 2.50041727908909e-07, 'epoch': 18.97}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1195/1260 [7:14:11<23:24, 21.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1196/1260 [7:14:32<22:59, 21.55s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1197/1260 [7:14:52<22:02, 20.98s/it][INFO|trainer.py:3993] 2025-07-05 04:48:00,439 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197
[INFO|configuration_utils.py:424] 2025-07-05 04:48:00,444 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:48:00,445 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:48:03,609 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:48:03,611 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:48:03,611 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:48:03,612 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/special_tokens_map.json
[2025-07-05 04:48:03,807] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1194 is about to be saved!
[2025-07-05 04:48:03,817] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:48:03,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:48:03,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:48:03,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:48:08,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:48:08,449] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:48:11,755] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1194 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:48:11,767 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:48:11,768 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:48:11,769 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:48:11,769 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:48:11,925 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:48:12,331 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1197/chat_template.jinja
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1198/1260 [7:15:29<26:36, 25.75s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1199/1260 [7:15:50<24:56, 24.53s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1200/1260 [7:16:12<23:34, 23.58s/it]                                                     {'loss': 0.001, 'grad_norm': 0.08903753996926939, 'learning_rate': 2.1367846834621952e-07, 'epoch': 19.05}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1200/1260 [7:16:12<23:34, 23.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1201/1260 [7:16:33<22:37, 23.01s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1202/1260 [7:16:55<21:48, 22.56s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1203/1260 [7:17:16<21:10, 22.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1204/1260 [7:17:38<20:36, 22.09s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1205/1260 [7:18:00<20:04, 21.91s/it]                                                     {'loss': 0.001, 'grad_norm': 0.22386282876276162, 'learning_rate': 1.8015225784786483e-07, 'epoch': 19.13}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1205/1260 [7:18:00<20:04, 21.91s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1206/1260 [7:18:21<19:41, 21.89s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1207/1260 [7:18:43<19:18, 21.86s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1208/1260 [7:19:05<18:50, 21.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1209/1260 [7:19:26<18:23, 21.63s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1210/1260 [7:19:47<17:59, 21.58s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.17557607016152021, 'learning_rate': 1.4946952908230448e-07, 'epoch': 19.21}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1210/1260 [7:19:47<17:59, 21.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1211/1260 [7:20:09<17:37, 21.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1212/1260 [7:20:31<17:24, 21.76s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1213/1260 [7:20:53<16:58, 21.67s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1214/1260 [7:21:15<16:40, 21.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1215/1260 [7:21:36<16:16, 21.70s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.11126659797353508, 'learning_rate': 1.2163616913962395e-07, 'epoch': 19.29}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1215/1260 [7:21:36<16:16, 21.70s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1216/1260 [7:21:58<15:53, 21.66s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1217/1260 [7:22:19<15:29, 21.62s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1218/1260 [7:22:41<15:10, 21.67s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1219/1260 [7:23:03<14:50, 21.72s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1220/1260 [7:23:25<14:31, 21.78s/it]                                                     {'loss': 0.0008, 'grad_norm': 0.11671087333877586, 'learning_rate': 9.6657518401988e-08, 'epoch': 19.37}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1220/1260 [7:23:25<14:31, 21.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1221/1260 [7:23:46<14:05, 21.68s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1222/1260 [7:24:08<13:40, 21.60s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1223/1260 [7:24:29<13:20, 21.63s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1224/1260 [7:24:51<12:57, 21.59s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1225/1260 [7:25:13<12:37, 21.66s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.1689803026529961, 'learning_rate': 7.453836951897885e-08, 'epoch': 19.45}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1225/1260 [7:25:13<12:37, 21.66s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1226/1260 [7:25:34<12:13, 21.58s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1227/1260 [7:25:56<11:50, 21.54s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1228/1260 [7:26:17<11:28, 21.53s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1229/1260 [7:26:39<11:10, 21.62s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1230/1260 [7:27:00<10:48, 21.61s/it]                                                     {'loss': 0.001, 'grad_norm': 0.13885357396996545, 'learning_rate': 5.528296648803166e-08, 'epoch': 19.52}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1230/1260 [7:27:00<10:48, 21.61s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1231/1260 [7:27:22<10:25, 21.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1232/1260 [7:27:43<10:02, 21.54s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1233/1260 [7:28:05<09:45, 21.67s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1234/1260 [7:28:27<09:22, 21.65s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1235/1260 [7:28:48<08:59, 21.58s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.2312546695886745, 'learning_rate': 3.889500384013755e-08, 'epoch': 19.6}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1235/1260 [7:28:48<08:59, 21.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1236/1260 [7:29:10<08:37, 21.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1237/1260 [7:29:32<08:16, 21.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1238/1260 [7:29:53<07:54, 21.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1239/1260 [7:30:15<07:32, 21.55s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1240/1260 [7:30:36<07:12, 21.64s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.091827424339099, 'learning_rate': 2.5377625930977367e-08, 'epoch': 19.68}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1240/1260 [7:30:36<07:12, 21.64s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1241/1260 [7:30:58<06:51, 21.67s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1242/1260 [7:31:20<06:31, 21.76s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1243/1260 [7:31:42<06:09, 21.76s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1244/1260 [7:32:04<05:47, 21.74s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1245/1260 [7:32:25<05:24, 21.64s/it]                                                     {'loss': 0.001, 'grad_norm': 0.0945924791193724, 'learning_rate': 1.4733426337610877e-08, 'epoch': 19.76}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1245/1260 [7:32:25<05:24, 21.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1246/1260 [7:32:47<05:02, 21.59s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1260 [7:33:08<04:40, 21.56s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1248/1260 [7:33:29<04:18, 21.52s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1249/1260 [7:33:51<03:57, 21.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1250/1260 [7:34:13<03:35, 21.59s/it]                                                     {'loss': 0.001, 'grad_norm': 0.1561674933265924, 'learning_rate': 6.964447360853221e-09, 'epoch': 19.84}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1250/1260 [7:34:13<03:35, 21.59s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1251/1260 [7:34:35<03:14, 21.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1252/1260 [7:34:56<02:53, 21.63s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1253/1260 [7:35:18<02:32, 21.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1254/1260 [7:35:40<02:10, 21.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1255/1260 [7:36:01<01:48, 21.63s/it]                                                     {'loss': 0.001, 'grad_norm': 0.12986748088525252, 'learning_rate': 2.0721796334149945e-09, 'epoch': 19.92}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1255/1260 [7:36:01<01:48, 21.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1256/1260 [7:36:23<01:26, 21.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1257/1260 [7:36:44<01:04, 21.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1258/1260 [7:37:05<00:42, 21.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1259/1260 [7:37:27<00:21, 21.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [7:37:46<00:00, 20.89s/it]                                                     {'loss': 0.001, 'grad_norm': 0.2173444477525068, 'learning_rate': 5.756183389271641e-11, 'epoch': 20.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [7:37:46<00:00, 20.89s/it][INFO|trainer.py:3993] 2025-07-05 05:10:55,299 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260
[INFO|configuration_utils.py:424] 2025-07-05 05:10:55,305 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/config.json
[INFO|configuration_utils.py:904] 2025-07-05 05:10:55,305 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 05:10:58,234 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:10:58,236 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:10:58,237 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:10:58,237 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/special_tokens_map.json
[2025-07-05 05:10:58,430] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1257 is about to be saved!
[2025-07-05 05:10:58,445] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 05:10:58,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 05:10:58,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 05:10:58,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 05:11:03,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 05:11:03,272] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 05:11:06,435] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1257 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 05:11:06,446 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:11:06,447 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:11:06,448 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:11:06,448 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 05:11:06,606 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 05:11:07,024 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/checkpoint-1260/chat_template.jinja
[INFO|trainer.py:2676] 2025-07-05 05:11:07,025 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 27481.8659, 'train_samples_per_second': 11.707, 'train_steps_per_second': 0.046, 'train_loss': 0.13741423702052247, 'epoch': 20.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [7:38:00<00:00, 20.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [7:38:00<00:00, 21.81s/it]
[INFO|image_processing_base.py:260] 2025-07-05 05:11:07,032 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_def/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:11:07,032 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:11:07,033 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:11:07,034 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 05:11:07,188 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_def/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 05:11:07,586 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/chat_template.jinja
[INFO|trainer.py:3993] 2025-07-05 05:11:08,650 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_def
[INFO|configuration_utils.py:424] 2025-07-05 05:11:08,655 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/config.json
[INFO|configuration_utils.py:904] 2025-07-05 05:11:08,656 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_def/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 05:11:11,343 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_def/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:11:11,344 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_def/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:11:11,344 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_def/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:11:11,345 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_def/special_tokens_map.json
***** train metrics *****
  epoch                    =       20.0
  total_flos               =  2003230GF
  train_loss               =     0.1374
  train_runtime            = 7:38:01.86
  train_samples_per_second =     11.707
  train_steps_per_second   =      0.046
Figure saved at: saves/qwen2_vl-3b/vindr_sft_def/training_loss.png
[WARNING|2025-07-05 05:11:11] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-07-05 05:11:11] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:450] 2025-07-05 05:11:11,677 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33msaves/qwen2_vl-3b/vindr_sft_def[0m at: [34mhttps://wandb.ai/compai/llamafactory/runs/hbyhvtgn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250704_213305-hbyhvtgn/logs[0m
### model
model_name_or_path: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/
image_max_pixels: 12845056
image_min_pixels: 3136
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
# freeze_vision_tower: true
# freeze_multi_modal_projector: true
# freeze_language_model: false
freeze_vision_tower: false
freeze_multi_modal_projector: false
freeze_language_model: false
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
dataset: vinder_train_def
template: qwen2_vl
cutoff_len: 1024
max_samples: 100000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen2_vl-3b/vindr_sft_def
logging_steps: 5
save_steps: 63
plot_loss: true
overwrite_output_dir: saves/qwen2_vl-3b/vindr_sft_def
save_only_model: false
report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
learning_rate: 3.0e-5
num_train_epochs: 20.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500
