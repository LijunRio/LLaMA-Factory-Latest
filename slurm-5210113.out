Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Fri Jul  4 21:29:08 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |
| N/A   35C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |
| N/A   34C    P0              56W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |
| N/A   35C    P0              55W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |
| N/A   34C    P0              55W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
🚀 Running experiment: Experiment 1:Qwen2-VL-2B-Instruct SFT Base
-----------------------------------
Model: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/
Output Dir: saves/qwen2_vl-3b/vindr_sft_base
Train Dataset: vinder_train_base
Cutoff Length: 1024
Save Steps: 63
Learning Rate: 3.0e-5
Num Train Epochs: 20.0
Logging Steps: 5
Per Device Train Batch Size: 8
Gradient Accumulation Steps: 8
-----------------------------------
[2025-07-04 21:29:19,878] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 07-04 21:29:23 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-07-04 21:29:26] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:42629
W0704 21:29:28.707000 2361161 site-packages/torch/distributed/run.py:792] 
W0704 21:29:28.707000 2361161 site-packages/torch/distributed/run.py:792] *****************************************
W0704 21:29:28.707000 2361161 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0704 21:29:28.707000 2361161 site-packages/torch/distributed/run.py:792] *****************************************
[2025-07-04 21:29:38,141] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:29:38,433] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:29:38,497] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:29:38,502] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-04 21:29:42,070] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-07-04 21:29:42] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[2025-07-04 21:29:42,522] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:29:42,552] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-07-04 21:29:42,553] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-07-04 21:29:42,614] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-07-04 21:29:42] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:42,829 >> loading file chat_template.jinja
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|2025-07-04 21:29:43] llamafactory.hparams.parser:406 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2299] 2025-07-04 21:29:43,152 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-07-04 21:29:43,153 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-07-04 21:29:43,157 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-07-04 21:29:43,164 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-07-04 21:29:43,165 >> loading file chat_template.jinja
[INFO|2025-07-04 21:29:43] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2299] 2025-07-04 21:29:43,479 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-07-04 21:29:43,481 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-07-04 21:29:43,481 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-07-04 21:29:43,481 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-07-04 21:29:43,483 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

[INFO|video_processing_utils.py:627] 2025-07-04 21:29:43,484 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-07-04 21:29:43,484 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[rank2]:[W704 21:29:43.939050781 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|processing_utils.py:990] 2025-07-04 21:29:43,972 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|2025-07-04 21:29:43] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/train_vindr_qwen2_len_16087.json...
[rank1]:[W704 21:29:44.629989291 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W704 21:29:44.633397979 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 0/16087 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   1%|          | 179/16087 [00:00<00:10, 1461.44 examples/s]Converting format of dataset (num_proc=16):  59%|█████▉    | 9558/16087 [00:00<00:00, 50964.01 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 16087/16087 [00:00<00:00, 37490.20 examples/s]
[rank0]:[W704 21:29:45.185991612 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/16087 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 1000/16087 [00:18<04:42, 53.44 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 2006/16087 [00:18<01:49, 129.16 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▊        | 3011/16087 [00:18<00:55, 234.28 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▍       | 4017/16087 [00:19<00:31, 379.81 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 6023/16087 [00:19<00:12, 788.50 examples/s]Running tokenizer on dataset (num_proc=16):  50%|████▉     | 8035/16087 [00:19<00:05, 1345.61 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 10051/16087 [00:19<00:03, 1995.20 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▍  | 12062/16087 [00:19<00:01, 2857.20 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 14072/16087 [00:20<00:00, 2611.35 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 15082/16087 [00:20<00:00, 2997.36 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 16082/16087 [00:20<00:00, 3111.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 16087/16087 [00:21<00:00, 757.88 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 12045, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198, 27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 15, 345, 414, 220, 21, 21, 21, 345, 414, 220, 16, 19, 15, 345, 414, 220, 22, 20, 20, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 12045, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pleural thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant
<answer>
[
  {
    "bbox_2d": [
      50,
      666,
      140,
      755
    ],
    "label": "Pleural thickening"
  }
]
</answer><|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 15, 345, 414, 220, 21, 21, 21, 345, 414, 220, 16, 19, 15, 345, 414, 220, 22, 20, 20, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 12045, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      50,
      666,
      140,
      755
    ],
    "label": "Pleural thickening"
  }
]
</answer><|im_end|>

[INFO|configuration_utils.py:696] 2025-07-04 21:30:09,566 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/config.json
[INFO|configuration_utils.py:770] 2025-07-04 21:30:09,568 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

[INFO|2025-07-04 21:30:09] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1148] 2025-07-04 21:30:09,635 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:3881] 2025-07-04 21:30:09,636 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-07-04 21:30:09,636] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[INFO|configuration_utils.py:1135] 2025-07-04 21:30:09,648 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|modeling_utils.py:2241] 2025-07-04 21:30:09,648 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.float32.
[2025-07-04 21:30:09,778] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:30:09,779] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:30:09,795] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[INFO|modeling_utils.py:2241] 2025-07-04 21:30:09,829 >> Instantiating Qwen2VLTextModel model under default dtype torch.float32.
[2025-07-04 21:30:09,977] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 730, num_elems = 2.44B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
[INFO|modeling_utils.py:5131] 2025-07-04 21:30:13,247 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-07-04 21:30:13,247 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-07-04 21:30:13,251 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/generation_config.json
[INFO|configuration_utils.py:1135] 2025-07-04 21:30:13,251 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

[INFO|2025-07-04 21:30:13] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-07-04 21:30:13] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-07-04 21:30:13] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-07-04 21:30:13] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2025-07-04 21:30:13] llamafactory.model.loader:143 >> trainable params: 2,208,985,600 || all params: 2,208,985,600 || trainable%: 100.0000
[INFO|trainer.py:756] 2025-07-04 21:30:13,281 >> Using auto half precision backend
[2025-07-04 21:30:13,670] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.6, git-hash=unknown, git-branch=unknown
[2025-07-04 21:30:13,671] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-07-04 21:30:13,683] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-07-04 21:30:13,685] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-07-04 21:30:13,685] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-07-04 21:30:13,719] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-07-04 21:30:13,719] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-07-04 21:30:13,719] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-07-04 21:30:13,720] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-07-04 21:30:13,982] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-07-04 21:30:13,983] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 2.33 GB         CA 1.03 GB         Max_CA 2 GB 
[2025-07-04 21:30:13,983] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.19 GB, percent = 5.7%
[2025-07-04 21:30:13,986] [INFO] [stage3.py:170:__init__] Reduce bucket size 2359296
[2025-07-04 21:30:13,986] [INFO] [stage3.py:171:__init__] Prefetch bucket size 2123366
[2025-07-04 21:30:14,227] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-07-04 21:30:14,228] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:30:14,228] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.18 GB, percent = 5.7%
Parameter Offload: Total persistent parameters: 686592 in 401 params
[2025-07-04 21:30:14,544] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-07-04 21:30:14,545] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:30:14,545] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.19 GB, percent = 5.7%
[2025-07-04 21:30:14,787] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-07-04 21:30:14,787] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.03 GB         Max_CA 1 GB 
[2025-07-04 21:30:14,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.19 GB, percent = 5.7%
[2025-07-04 21:30:16,532] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[2025-07-04 21:30:16,534] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.06 GB         Max_CA 1 GB 
[2025-07-04 21:30:16,534] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.19 GB, percent = 5.7%
[2025-07-04 21:30:16,781] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-07-04 21:30:16,783] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.03 GB         CA 1.06 GB         Max_CA 1 GB 
[2025-07-04 21:30:16,783] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.19 GB, percent = 5.7%
[2025-07-04 21:30:17,083] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-07-04 21:30:17,084] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 4.11 GB         CA 4.15 GB         Max_CA 4 GB 
[2025-07-04 21:30:17,084] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.18 GB, percent = 5.7%
[2025-07-04 21:30:17,327] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-07-04 21:30:17,328] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 3.09 GB         CA 4.15 GB         Max_CA 4 GB 
[2025-07-04 21:30:17,328] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.21 GB, percent = 5.7%
[2025-07-04 21:30:17,597] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-07-04 21:30:17,598] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 5.14 GB         CA 6.2 GB         Max_CA 6 GB 
[2025-07-04 21:30:17,598] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 57.21 GB, percent = 5.7%
[2025-07-04 21:30:17,599] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-07-04 21:30:18,148] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-07-04 21:30:18,149] [INFO] [utils.py:782:see_memory_usage] MA 4.12 GB         Max_MA 4.99 GB         CA 6.2 GB         Max_CA 6 GB 
[2025-07-04 21:30:18,149] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.8%
[2025-07-04 21:30:18,149] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-07-04 21:30:18,150] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-07-04 21:30:18,150] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-07-04 21:30:18,150] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-07-04 21:30:18,152] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-07-04 21:30:18,152] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdee9111d50>
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-07-04 21:30:18,153] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-07-04 21:30:18,154] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   train_batch_size ............. 256
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2359296 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=2123366 param_persistence_threshold=15360 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-07-04 21:30:18,155] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-07-04 21:30:18,155] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.359296e+06, 
        "stage3_prefetch_bucket_size": 2.123366e+06, 
        "stage3_param_persistence_threshold": 1.536000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2409] 2025-07-04 21:30:18,157 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-07-04 21:30:18,157 >>   Num examples = 16,087
[INFO|trainer.py:2411] 2025-07-04 21:30:18,157 >>   Num Epochs = 20
[INFO|trainer.py:2412] 2025-07-04 21:30:18,157 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2415] 2025-07-04 21:30:18,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:2416] 2025-07-04 21:30:18,157 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2417] 2025-07-04 21:30:18,157 >>   Total optimization steps = 1,260
[INFO|trainer.py:2418] 2025-07-04 21:30:18,160 >>   Number of trainable parameters = 2,208,985,600
[INFO|integration_utils.py:832] 2025-07-04 21:30:18,162 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jun_rio (compai) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/june/Code/LLaMA-Factory-Latest/wandb/run-20250704_213018-rt74h6lh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run saves/qwen2_vl-3b/vindr_sft_base
wandb: ⭐️ View project at https://wandb.ai/compai/llamafactory
wandb: 🚀 View run at https://wandb.ai/compai/llamafactory/runs/rt74h6lh
  0%|          | 0/1260 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-07-04 21:30:21,774 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  0%|          | 1/1260 [00:29<10:25:38, 29.82s/it]  0%|          | 2/1260 [00:52<8:51:30, 25.35s/it]   0%|          | 3/1260 [01:13<8:14:49, 23.62s/it]  0%|          | 4/1260 [01:35<7:59:11, 22.89s/it]  0%|          | 5/1260 [01:57<7:53:10, 22.62s/it]                                                  {'loss': 0.9582, 'grad_norm': 32.48702145836953, 'learning_rate': 9.523809523809523e-07, 'epoch': 0.08}
  0%|          | 5/1260 [01:57<7:53:10, 22.62s/it]  0%|          | 6/1260 [02:19<7:44:49, 22.24s/it]  1%|          | 7/1260 [02:40<7:40:35, 22.06s/it]  1%|          | 8/1260 [03:02<7:36:58, 21.90s/it]  1%|          | 9/1260 [03:24<7:40:46, 22.10s/it]  1%|          | 10/1260 [03:46<7:40:02, 22.08s/it]                                                   {'loss': 0.7589, 'grad_norm': 13.474720257581446, 'learning_rate': 2.1428571428571427e-06, 'epoch': 0.16}
  1%|          | 10/1260 [03:46<7:40:02, 22.08s/it]  1%|          | 11/1260 [04:09<7:40:17, 22.11s/it]  1%|          | 12/1260 [04:31<7:39:26, 22.09s/it]  1%|          | 13/1260 [04:52<7:36:11, 21.95s/it]  1%|          | 14/1260 [05:14<7:32:54, 21.81s/it]  1%|          | 15/1260 [05:35<7:30:04, 21.69s/it]                                                   {'loss': 0.4782, 'grad_norm': 1.751096771866588, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.24}
  1%|          | 15/1260 [05:35<7:30:04, 21.69s/it]  1%|▏         | 16/1260 [05:56<7:27:58, 21.61s/it]  1%|▏         | 17/1260 [06:18<7:29:02, 21.68s/it]  1%|▏         | 18/1260 [06:40<7:27:09, 21.60s/it]  2%|▏         | 19/1260 [07:02<7:28:42, 21.69s/it]  2%|▏         | 20/1260 [07:23<7:27:56, 21.67s/it]                                                   {'loss': 0.4401, 'grad_norm': 1.5063148101072312, 'learning_rate': 4.5238095238095235e-06, 'epoch': 0.32}
  2%|▏         | 20/1260 [07:23<7:27:56, 21.67s/it]  2%|▏         | 21/1260 [07:45<7:25:55, 21.59s/it]  2%|▏         | 22/1260 [08:06<7:25:57, 21.61s/it]  2%|▏         | 23/1260 [08:28<7:24:05, 21.54s/it]  2%|▏         | 24/1260 [08:49<7:23:00, 21.50s/it]  2%|▏         | 25/1260 [09:11<7:22:36, 21.50s/it]                                                   {'loss': 0.4152, 'grad_norm': 1.7763349330879459, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.4}
  2%|▏         | 25/1260 [09:11<7:22:36, 21.50s/it]  2%|▏         | 26/1260 [09:32<7:23:12, 21.55s/it]  2%|▏         | 27/1260 [09:54<7:21:51, 21.50s/it]  2%|▏         | 28/1260 [10:15<7:21:38, 21.51s/it]  2%|▏         | 29/1260 [10:37<7:22:27, 21.57s/it]  2%|▏         | 30/1260 [10:58<7:22:03, 21.56s/it]                                                   {'loss': 0.388, 'grad_norm': 1.644634154528539, 'learning_rate': 6.904761904761905e-06, 'epoch': 0.48}
  2%|▏         | 30/1260 [10:58<7:22:03, 21.56s/it]  2%|▏         | 31/1260 [11:20<7:21:53, 21.57s/it]  3%|▎         | 32/1260 [11:42<7:21:54, 21.59s/it]  3%|▎         | 33/1260 [12:04<7:23:29, 21.69s/it]  3%|▎         | 34/1260 [12:25<7:21:41, 21.62s/it]  3%|▎         | 35/1260 [12:47<7:23:06, 21.70s/it]                                                   {'loss': 0.3711, 'grad_norm': 1.6472129421771255, 'learning_rate': 8.095238095238095e-06, 'epoch': 0.56}
  3%|▎         | 35/1260 [12:47<7:23:06, 21.70s/it]  3%|▎         | 36/1260 [13:09<7:23:12, 21.73s/it]  3%|▎         | 37/1260 [13:30<7:22:21, 21.70s/it]  3%|▎         | 38/1260 [13:52<7:20:21, 21.62s/it]  3%|▎         | 39/1260 [14:14<7:20:20, 21.64s/it]  3%|▎         | 40/1260 [14:35<7:20:01, 21.64s/it]                                                   {'loss': 0.3542, 'grad_norm': 1.6106483459864223, 'learning_rate': 9.285714285714286e-06, 'epoch': 0.64}
  3%|▎         | 40/1260 [14:35<7:20:01, 21.64s/it]  3%|▎         | 41/1260 [14:57<7:18:09, 21.57s/it]  3%|▎         | 42/1260 [15:18<7:16:27, 21.50s/it]  3%|▎         | 43/1260 [15:40<7:16:51, 21.54s/it]  3%|▎         | 44/1260 [16:01<7:15:29, 21.49s/it]  4%|▎         | 45/1260 [16:23<7:15:47, 21.52s/it]                                                   {'loss': 0.3425, 'grad_norm': 1.5450803495632277, 'learning_rate': 1.0476190476190475e-05, 'epoch': 0.72}
  4%|▎         | 45/1260 [16:23<7:15:47, 21.52s/it]  4%|▎         | 46/1260 [16:44<7:17:49, 21.64s/it]  4%|▎         | 47/1260 [17:06<7:15:56, 21.56s/it]  4%|▍         | 48/1260 [17:27<7:15:26, 21.56s/it]  4%|▍         | 49/1260 [17:49<7:14:02, 21.51s/it]  4%|▍         | 50/1260 [18:10<7:13:32, 21.50s/it]                                                   {'loss': 0.3288, 'grad_norm': 1.9456479917941354, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.8}
  4%|▍         | 50/1260 [18:10<7:13:32, 21.50s/it]  4%|▍         | 51/1260 [18:32<7:13:33, 21.52s/it]  4%|▍         | 52/1260 [18:53<7:13:36, 21.54s/it]  4%|▍         | 53/1260 [19:15<7:12:48, 21.51s/it]  4%|▍         | 54/1260 [19:36<7:11:25, 21.46s/it]  4%|▍         | 55/1260 [19:58<7:12:39, 21.54s/it]                                                   {'loss': 0.3337, 'grad_norm': 1.843688449506229, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.87}
  4%|▍         | 55/1260 [19:58<7:12:39, 21.54s/it]  4%|▍         | 56/1260 [20:19<7:11:03, 21.48s/it]  5%|▍         | 57/1260 [20:41<7:11:32, 21.52s/it]  5%|▍         | 58/1260 [21:02<7:10:46, 21.50s/it]  5%|▍         | 59/1260 [21:24<7:11:25, 21.55s/it]  5%|▍         | 60/1260 [21:46<7:11:01, 21.55s/it]                                                   {'loss': 0.3274, 'grad_norm': 1.3943359398287867, 'learning_rate': 1.4047619047619048e-05, 'epoch': 0.95}
  5%|▍         | 60/1260 [21:46<7:11:01, 21.55s/it]  5%|▍         | 61/1260 [22:07<7:12:09, 21.63s/it]  5%|▍         | 62/1260 [22:29<7:11:20, 21.60s/it]  5%|▌         | 63/1260 [22:48<6:57:12, 20.91s/it][INFO|trainer.py:3993] 2025-07-04 21:53:10,841 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63
[INFO|configuration_utils.py:424] 2025-07-04 21:53:10,849 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/config.json
[INFO|configuration_utils.py:904] 2025-07-04 21:53:10,850 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 21:53:14,502 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 21:53:14,503 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 21:53:14,504 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 21:53:14,505 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/special_tokens_map.json
[2025-07-04 21:53:14,822] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step62 is about to be saved!
[2025-07-04 21:53:14,833] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 21:53:14,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 21:53:14,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 21:53:14,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 21:53:20,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 21:53:20,126] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/global_step62/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 21:53:23,037] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 21:53:23,065 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 21:53:23,066 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 21:53:23,066 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 21:53:23,067 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 21:53:23,216 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 21:53:23,612 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/chat_template.jinja
  5%|▌         | 64/1260 [23:26<8:40:55, 26.13s/it]  5%|▌         | 65/1260 [23:49<8:16:00, 24.90s/it]                                                   {'loss': 0.309, 'grad_norm': 1.334879216804754, 'learning_rate': 1.5238095238095238e-05, 'epoch': 1.03}
  5%|▌         | 65/1260 [23:49<8:16:00, 24.90s/it]  5%|▌         | 66/1260 [24:10<7:54:51, 23.86s/it]  5%|▌         | 67/1260 [24:31<7:40:07, 23.14s/it]  5%|▌         | 68/1260 [24:53<7:29:35, 22.63s/it]  5%|▌         | 69/1260 [25:15<7:24:00, 22.37s/it]  6%|▌         | 70/1260 [25:37<7:21:41, 22.27s/it]                                                   {'loss': 0.3124, 'grad_norm': 1.035644934717214, 'learning_rate': 1.6428571428571432e-05, 'epoch': 1.11}
  6%|▌         | 70/1260 [25:37<7:21:41, 22.27s/it]  6%|▌         | 71/1260 [25:58<7:17:12, 22.06s/it]  6%|▌         | 72/1260 [26:20<7:15:04, 21.97s/it]  6%|▌         | 73/1260 [26:42<7:12:18, 21.85s/it]  6%|▌         | 74/1260 [27:03<7:09:30, 21.73s/it]  6%|▌         | 75/1260 [27:24<7:07:22, 21.64s/it]                                                   {'loss': 0.3101, 'grad_norm': 1.0620931624634529, 'learning_rate': 1.761904761904762e-05, 'epoch': 1.19}
  6%|▌         | 75/1260 [27:24<7:07:22, 21.64s/it]  6%|▌         | 76/1260 [27:46<7:08:23, 21.71s/it]  6%|▌         | 77/1260 [28:08<7:07:58, 21.71s/it]  6%|▌         | 78/1260 [28:30<7:06:47, 21.66s/it]  6%|▋         | 79/1260 [28:51<7:07:10, 21.70s/it]  6%|▋         | 80/1260 [29:13<7:06:55, 21.71s/it]                                                   {'loss': 0.304, 'grad_norm': 1.1446341476200734, 'learning_rate': 1.880952380952381e-05, 'epoch': 1.27}
  6%|▋         | 80/1260 [29:13<7:06:55, 21.71s/it]  6%|▋         | 81/1260 [29:35<7:08:25, 21.80s/it]  7%|▋         | 82/1260 [29:57<7:06:07, 21.70s/it]  7%|▋         | 83/1260 [30:18<7:05:46, 21.70s/it]  7%|▋         | 84/1260 [30:40<7:05:50, 21.73s/it]  7%|▋         | 85/1260 [31:02<7:07:11, 21.81s/it]                                                   {'loss': 0.2987, 'grad_norm': 1.0142962058973013, 'learning_rate': 1.9999999999999998e-05, 'epoch': 1.35}
  7%|▋         | 85/1260 [31:02<7:07:11, 21.81s/it]  7%|▋         | 86/1260 [31:24<7:07:53, 21.87s/it]  7%|▋         | 87/1260 [31:46<7:05:10, 21.75s/it]  7%|▋         | 88/1260 [32:07<7:03:21, 21.67s/it]  7%|▋         | 89/1260 [32:28<7:01:32, 21.60s/it]  7%|▋         | 90/1260 [32:50<7:00:27, 21.56s/it]                                                   {'loss': 0.2964, 'grad_norm': 0.8222504089044799, 'learning_rate': 2.1190476190476193e-05, 'epoch': 1.43}
  7%|▋         | 90/1260 [32:50<7:00:27, 21.56s/it]  7%|▋         | 91/1260 [33:12<7:00:37, 21.59s/it]  7%|▋         | 92/1260 [33:33<7:00:24, 21.60s/it]  7%|▋         | 93/1260 [33:55<6:59:02, 21.54s/it]  7%|▋         | 94/1260 [34:16<6:57:50, 21.50s/it]  8%|▊         | 95/1260 [34:38<6:57:20, 21.49s/it]                                                   {'loss': 0.2994, 'grad_norm': 1.008078099308156, 'learning_rate': 2.238095238095238e-05, 'epoch': 1.51}
  8%|▊         | 95/1260 [34:38<6:57:20, 21.49s/it]  8%|▊         | 96/1260 [34:59<6:57:01, 21.50s/it]  8%|▊         | 97/1260 [35:21<6:57:34, 21.54s/it]  8%|▊         | 98/1260 [35:42<6:56:55, 21.53s/it]  8%|▊         | 99/1260 [36:04<6:58:45, 21.64s/it]  8%|▊         | 100/1260 [36:25<6:57:04, 21.57s/it]                                                    {'loss': 0.2998, 'grad_norm': 0.8895102495570505, 'learning_rate': 2.357142857142857e-05, 'epoch': 1.59}
  8%|▊         | 100/1260 [36:25<6:57:04, 21.57s/it]  8%|▊         | 101/1260 [36:47<6:56:27, 21.56s/it]  8%|▊         | 102/1260 [37:08<6:55:21, 21.52s/it]  8%|▊         | 103/1260 [37:30<6:55:07, 21.53s/it]  8%|▊         | 104/1260 [37:52<6:55:04, 21.54s/it]  8%|▊         | 105/1260 [38:13<6:54:20, 21.52s/it]                                                    {'loss': 0.2938, 'grad_norm': 0.9964180114286997, 'learning_rate': 2.4761904761904762e-05, 'epoch': 1.67}
  8%|▊         | 105/1260 [38:13<6:54:20, 21.52s/it]  8%|▊         | 106/1260 [38:35<6:55:18, 21.59s/it]  8%|▊         | 107/1260 [38:56<6:54:43, 21.58s/it]  9%|▊         | 108/1260 [39:18<6:53:45, 21.55s/it]  9%|▊         | 109/1260 [39:40<6:54:35, 21.61s/it]  9%|▊         | 110/1260 [40:01<6:55:19, 21.67s/it]                                                    {'loss': 0.2977, 'grad_norm': 0.8704762225056595, 'learning_rate': 2.5952380952380953e-05, 'epoch': 1.75}
  9%|▊         | 110/1260 [40:01<6:55:19, 21.67s/it]  9%|▉         | 111/1260 [40:24<6:57:58, 21.83s/it]  9%|▉         | 112/1260 [40:46<7:02:51, 22.10s/it]  9%|▉         | 113/1260 [41:08<6:59:54, 21.97s/it]  9%|▉         | 114/1260 [41:30<6:58:04, 21.89s/it]  9%|▉         | 115/1260 [41:51<6:55:38, 21.78s/it]                                                    {'loss': 0.2939, 'grad_norm': 0.8787178115284644, 'learning_rate': 2.7142857142857144e-05, 'epoch': 1.83}
  9%|▉         | 115/1260 [41:51<6:55:38, 21.78s/it]  9%|▉         | 116/1260 [42:13<6:54:52, 21.76s/it]  9%|▉         | 117/1260 [42:34<6:52:40, 21.66s/it]  9%|▉         | 118/1260 [42:56<6:53:59, 21.75s/it]  9%|▉         | 119/1260 [43:18<6:53:53, 21.76s/it] 10%|▉         | 120/1260 [43:40<6:52:32, 21.71s/it]                                                    {'loss': 0.2942, 'grad_norm': 0.6407835600653305, 'learning_rate': 2.8333333333333332e-05, 'epoch': 1.91}
 10%|▉         | 120/1260 [43:40<6:52:32, 21.71s/it] 10%|▉         | 121/1260 [44:02<6:52:53, 21.75s/it] 10%|▉         | 122/1260 [44:23<6:51:12, 21.68s/it] 10%|▉         | 123/1260 [44:45<6:50:34, 21.67s/it] 10%|▉         | 124/1260 [45:06<6:49:33, 21.63s/it] 10%|▉         | 125/1260 [45:28<6:47:55, 21.56s/it]                                                    {'loss': 0.2894, 'grad_norm': 0.9406222038632305, 'learning_rate': 2.9523809523809523e-05, 'epoch': 1.99}
 10%|▉         | 125/1260 [45:28<6:47:55, 21.56s/it] 10%|█         | 126/1260 [45:47<6:36:24, 20.97s/it][INFO|trainer.py:3993] 2025-07-04 22:16:09,176 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126
[INFO|configuration_utils.py:424] 2025-07-04 22:16:09,182 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/config.json
[INFO|configuration_utils.py:904] 2025-07-04 22:16:09,183 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 22:16:12,372 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:16:12,374 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:16:12,375 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:16:12,375 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/special_tokens_map.json
[2025-07-04 22:16:12,563] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step125 is about to be saved!
[2025-07-04 22:16:12,574] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 22:16:12,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 22:16:12,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 22:16:12,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 22:16:17,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 22:16:17,787] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 22:16:20,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step125 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 22:16:20,837 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:16:20,837 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:16:20,838 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:16:20,838 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 22:16:20,999 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 22:16:21,410 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/chat_template.jinja
 10%|█         | 127/1260 [46:25<8:08:30, 25.87s/it] 10%|█         | 128/1260 [46:46<7:44:57, 24.64s/it] 10%|█         | 129/1260 [47:08<7:28:57, 23.82s/it] 10%|█         | 130/1260 [47:30<7:15:08, 23.11s/it]                                                    {'loss': 0.2725, 'grad_norm': 1.2125540101573133, 'learning_rate': 2.9999481946145652e-05, 'epoch': 2.06}
 10%|█         | 130/1260 [47:30<7:15:08, 23.11s/it] 10%|█         | 131/1260 [47:51<7:07:26, 22.72s/it] 10%|█         | 132/1260 [48:13<7:02:30, 22.47s/it] 11%|█         | 133/1260 [48:35<6:57:44, 22.24s/it] 11%|█         | 134/1260 [48:57<6:54:04, 22.06s/it] 11%|█         | 135/1260 [49:18<6:50:38, 21.90s/it]                                                    {'loss': 0.2806, 'grad_norm': 0.8003325524276537, 'learning_rate': 2.9996316191067322e-05, 'epoch': 2.14}
 11%|█         | 135/1260 [49:18<6:50:38, 21.90s/it] 11%|█         | 136/1260 [49:40<6:48:30, 21.81s/it] 11%|█         | 137/1260 [50:01<6:47:11, 21.76s/it] 11%|█         | 138/1260 [50:23<6:45:30, 21.69s/it] 11%|█         | 139/1260 [50:44<6:43:47, 21.61s/it] 11%|█         | 140/1260 [51:06<6:42:45, 21.58s/it]                                                    {'loss': 0.2793, 'grad_norm': 1.5365148644937847, 'learning_rate': 2.999027309528412e-05, 'epoch': 2.22}
 11%|█         | 140/1260 [51:06<6:42:45, 21.58s/it] 11%|█         | 141/1260 [51:28<6:43:01, 21.61s/it] 11%|█▏        | 142/1260 [51:49<6:43:09, 21.64s/it] 11%|█▏        | 143/1260 [52:11<6:42:01, 21.60s/it] 11%|█▏        | 144/1260 [52:33<6:45:30, 21.80s/it] 12%|█▏        | 145/1260 [52:55<6:46:42, 21.89s/it]                                                    {'loss': 0.2839, 'grad_norm': 0.8417733627852486, 'learning_rate': 2.9981353818283835e-05, 'epoch': 2.3}
 12%|█▏        | 145/1260 [52:55<6:46:42, 21.89s/it] 12%|█▏        | 146/1260 [53:17<6:44:26, 21.78s/it] 12%|█▏        | 147/1260 [53:38<6:42:13, 21.68s/it] 12%|█▏        | 148/1260 [54:00<6:40:32, 21.61s/it] 12%|█▏        | 149/1260 [54:21<6:39:57, 21.60s/it] 12%|█▏        | 150/1260 [54:43<6:39:23, 21.59s/it]                                                    {'loss': 0.2778, 'grad_norm': 0.937771338254109, 'learning_rate': 2.996956007140667e-05, 'epoch': 2.38}
 12%|█▏        | 150/1260 [54:43<6:39:23, 21.59s/it] 12%|█▏        | 151/1260 [55:04<6:38:17, 21.55s/it] 12%|█▏        | 152/1260 [55:26<6:37:15, 21.51s/it] 12%|█▏        | 153/1260 [55:47<6:37:37, 21.55s/it] 12%|█▏        | 154/1260 [56:09<6:39:32, 21.68s/it] 12%|█▏        | 155/1260 [56:31<6:38:10, 21.62s/it]                                                    {'loss': 0.2797, 'grad_norm': 0.7794736930633519, 'learning_rate': 2.995489411751688e-05, 'epoch': 2.46}
 12%|█▏        | 155/1260 [56:31<6:38:10, 21.62s/it] 12%|█▏        | 156/1260 [56:52<6:38:20, 21.65s/it] 12%|█▏        | 157/1260 [57:14<6:37:02, 21.60s/it] 13%|█▎        | 158/1260 [57:35<6:36:00, 21.56s/it] 13%|█▎        | 159/1260 [57:57<6:35:18, 21.54s/it] 13%|█▎        | 160/1260 [58:19<6:35:46, 21.59s/it]                                                    {'loss': 0.2801, 'grad_norm': 0.7760797045305565, 'learning_rate': 2.9937358770568617e-05, 'epoch': 2.54}
 13%|█▎        | 160/1260 [58:19<6:35:46, 21.59s/it] 13%|█▎        | 161/1260 [58:40<6:36:20, 21.64s/it] 13%|█▎        | 162/1260 [59:02<6:34:58, 21.58s/it] 13%|█▎        | 163/1260 [59:23<6:34:00, 21.55s/it] 13%|█▎        | 164/1260 [59:45<6:33:48, 21.56s/it] 13%|█▎        | 165/1260 [1:00:07<6:34:17, 21.61s/it]                                                      {'loss': 0.2803, 'grad_norm': 1.1308096799154856, 'learning_rate': 2.9916957395065996e-05, 'epoch': 2.62}
 13%|█▎        | 165/1260 [1:00:07<6:34:17, 21.61s/it] 13%|█▎        | 166/1260 [1:00:28<6:33:03, 21.56s/it] 13%|█▎        | 167/1260 [1:00:50<6:33:08, 21.58s/it] 13%|█▎        | 168/1260 [1:01:11<6:32:19, 21.56s/it] 13%|█▎        | 169/1260 [1:01:33<6:32:45, 21.60s/it] 13%|█▎        | 170/1260 [1:01:55<6:34:30, 21.72s/it]                                                      {'loss': 0.2776, 'grad_norm': 1.2533818447094907, 'learning_rate': 2.9893693905417548e-05, 'epoch': 2.7}
 13%|█▎        | 170/1260 [1:01:55<6:34:30, 21.72s/it] 14%|█▎        | 171/1260 [1:02:17<6:33:59, 21.71s/it] 14%|█▎        | 172/1260 [1:02:38<6:32:38, 21.65s/it] 14%|█▎        | 173/1260 [1:03:00<6:35:03, 21.81s/it] 14%|█▍        | 174/1260 [1:03:22<6:33:22, 21.73s/it] 14%|█▍        | 175/1260 [1:03:44<6:33:09, 21.74s/it]                                                      {'loss': 0.2793, 'grad_norm': 0.7718969795194219, 'learning_rate': 2.9867572765185192e-05, 'epoch': 2.78}
 14%|█▍        | 175/1260 [1:03:44<6:33:09, 21.74s/it] 14%|█▍        | 176/1260 [1:04:05<6:31:38, 21.68s/it] 14%|█▍        | 177/1260 [1:04:26<6:29:56, 21.60s/it] 14%|█▍        | 178/1260 [1:04:49<6:32:17, 21.75s/it] 14%|█▍        | 179/1260 [1:05:10<6:32:11, 21.77s/it] 14%|█▍        | 180/1260 [1:05:32<6:30:54, 21.72s/it]                                                      {'loss': 0.282, 'grad_norm': 1.5510514806874258, 'learning_rate': 2.9838598986227776e-05, 'epoch': 2.86}
 14%|█▍        | 180/1260 [1:05:32<6:30:54, 21.72s/it] 14%|█▍        | 181/1260 [1:05:54<6:30:17, 21.70s/it] 14%|█▍        | 182/1260 [1:06:15<6:29:00, 21.65s/it] 15%|█▍        | 183/1260 [1:06:37<6:29:14, 21.69s/it] 15%|█▍        | 184/1260 [1:06:59<6:29:59, 21.75s/it] 15%|█▍        | 185/1260 [1:07:21<6:29:40, 21.75s/it]                                                      {'loss': 0.2734, 'grad_norm': 0.5976988985573842, 'learning_rate': 2.9806778127739467e-05, 'epoch': 2.94}
 15%|█▍        | 185/1260 [1:07:21<6:29:40, 21.75s/it] 15%|█▍        | 186/1260 [1:07:43<6:30:37, 21.82s/it] 15%|█▍        | 187/1260 [1:08:04<6:28:46, 21.74s/it] 15%|█▍        | 188/1260 [1:08:26<6:27:09, 21.67s/it] 15%|█▌        | 189/1260 [1:08:45<6:16:09, 21.07s/it][INFO|trainer.py:3993] 2025-07-04 22:39:07,195 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189
[INFO|configuration_utils.py:424] 2025-07-04 22:39:07,203 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/config.json
[INFO|configuration_utils.py:904] 2025-07-04 22:39:07,204 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 22:39:10,323 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:39:10,325 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:39:10,327 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:39:10,328 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/special_tokens_map.json
[2025-07-04 22:39:10,523] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step188 is about to be saved!
[2025-07-04 22:39:10,535] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 22:39:10,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 22:39:10,569] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 22:39:10,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 22:39:15,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 22:39:15,845] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/global_step188/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 22:39:18,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step188 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 22:39:18,709 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 22:39:18,710 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 22:39:18,710 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 22:39:18,711 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 22:39:18,871 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 22:39:19,280 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/chat_template.jinja
 15%|█▌        | 190/1260 [1:09:22<7:41:01, 25.85s/it]                                                      {'loss': 0.2605, 'grad_norm': 1.4354307393347543, 'learning_rate': 2.977211629518312e-05, 'epoch': 3.02}
 15%|█▌        | 190/1260 [1:09:22<7:41:01, 25.85s/it] 15%|█▌        | 191/1260 [1:09:44<7:19:20, 24.66s/it] 15%|█▌        | 192/1260 [1:10:06<7:03:49, 23.81s/it] 15%|█▌        | 193/1260 [1:10:27<6:50:45, 23.10s/it] 15%|█▌        | 194/1260 [1:10:49<6:41:46, 22.61s/it] 15%|█▌        | 195/1260 [1:11:10<6:35:27, 22.28s/it]                                                      {'loss': 0.2609, 'grad_norm': 1.2787489860177768, 'learning_rate': 2.9734620139118812e-05, 'epoch': 3.1}
 15%|█▌        | 195/1260 [1:11:10<6:35:27, 22.28s/it] 16%|█▌        | 196/1260 [1:11:32<6:32:53, 22.16s/it] 16%|█▌        | 197/1260 [1:11:54<6:28:45, 21.94s/it] 16%|█▌        | 198/1260 [1:12:15<6:26:25, 21.83s/it] 16%|█▌        | 199/1260 [1:12:37<6:24:27, 21.74s/it] 16%|█▌        | 200/1260 [1:12:59<6:24:19, 21.75s/it]                                                      {'loss': 0.2572, 'grad_norm': 1.2316033248170994, 'learning_rate': 2.9694296853927792e-05, 'epoch': 3.17}
 16%|█▌        | 200/1260 [1:12:59<6:24:19, 21.75s/it] 16%|█▌        | 201/1260 [1:13:20<6:23:24, 21.72s/it] 16%|█▌        | 202/1260 [1:13:42<6:24:13, 21.79s/it] 16%|█▌        | 203/1260 [1:14:04<6:22:59, 21.74s/it] 16%|█▌        | 204/1260 [1:14:26<6:24:49, 21.87s/it] 16%|█▋        | 205/1260 [1:14:48<6:22:30, 21.75s/it]                                                      {'loss': 0.2653, 'grad_norm': 0.9084765964903823, 'learning_rate': 2.965115417643212e-05, 'epoch': 3.25}
 16%|█▋        | 205/1260 [1:14:48<6:22:30, 21.75s/it] 16%|█▋        | 206/1260 [1:15:09<6:20:16, 21.65s/it] 16%|█▋        | 207/1260 [1:15:31<6:21:17, 21.73s/it] 17%|█▋        | 208/1260 [1:15:52<6:19:53, 21.67s/it] 17%|█▋        | 209/1260 [1:16:14<6:19:27, 21.66s/it] 17%|█▋        | 210/1260 [1:16:36<6:18:19, 21.62s/it]                                                      {'loss': 0.2599, 'grad_norm': 0.9200242618615303, 'learning_rate': 2.960520038441018e-05, 'epoch': 3.33}
 17%|█▋        | 210/1260 [1:16:36<6:18:19, 21.62s/it] 17%|█▋        | 211/1260 [1:16:57<6:18:04, 21.62s/it] 17%|█▋        | 212/1260 [1:17:19<6:18:07, 21.65s/it] 17%|█▋        | 213/1260 [1:17:41<6:17:59, 21.66s/it] 17%|█▋        | 214/1260 [1:18:02<6:17:54, 21.68s/it] 17%|█▋        | 215/1260 [1:18:24<6:16:49, 21.64s/it]                                                      {'loss': 0.2626, 'grad_norm': 0.9820335026740419, 'learning_rate': 2.9556444295008444e-05, 'epoch': 3.41}
 17%|█▋        | 215/1260 [1:18:24<6:16:49, 21.64s/it] 17%|█▋        | 216/1260 [1:18:46<6:18:18, 21.74s/it] 17%|█▋        | 217/1260 [1:19:07<6:16:19, 21.65s/it] 17%|█▋        | 218/1260 [1:19:29<6:14:59, 21.59s/it] 17%|█▋        | 219/1260 [1:19:50<6:14:08, 21.56s/it] 17%|█▋        | 220/1260 [1:20:12<6:15:36, 21.67s/it]                                                      {'loss': 0.2622, 'grad_norm': 1.299042001496409, 'learning_rate': 2.950489526304971e-05, 'epoch': 3.49}
 17%|█▋        | 220/1260 [1:20:12<6:15:36, 21.67s/it] 18%|█▊        | 221/1260 [1:20:34<6:15:42, 21.70s/it] 18%|█▊        | 222/1260 [1:20:56<6:15:45, 21.72s/it] 18%|█▊        | 223/1260 [1:21:17<6:13:48, 21.63s/it] 18%|█▊        | 224/1260 [1:21:39<6:13:30, 21.63s/it] 18%|█▊        | 225/1260 [1:22:00<6:12:10, 21.57s/it]                                                      {'loss': 0.264, 'grad_norm': 1.3149307815353002, 'learning_rate': 2.9450563179238207e-05, 'epoch': 3.57}
 18%|█▊        | 225/1260 [1:22:00<6:12:10, 21.57s/it] 18%|█▊        | 226/1260 [1:22:22<6:11:15, 21.54s/it] 18%|█▊        | 227/1260 [1:22:43<6:10:17, 21.51s/it] 18%|█▊        | 228/1260 [1:23:04<6:09:18, 21.47s/it] 18%|█▊        | 229/1260 [1:23:26<6:08:59, 21.47s/it] 18%|█▊        | 230/1260 [1:23:48<6:10:00, 21.55s/it]                                                      {'loss': 0.2693, 'grad_norm': 0.937751863361122, 'learning_rate': 2.939345846826186e-05, 'epoch': 3.65}
 18%|█▊        | 230/1260 [1:23:48<6:10:00, 21.55s/it] 18%|█▊        | 231/1260 [1:24:10<6:11:43, 21.68s/it] 18%|█▊        | 232/1260 [1:24:32<6:13:33, 21.80s/it] 18%|█▊        | 233/1260 [1:24:54<6:13:48, 21.84s/it] 19%|█▊        | 234/1260 [1:25:15<6:11:30, 21.73s/it] 19%|█▊        | 235/1260 [1:25:37<6:10:03, 21.66s/it]                                                      {'loss': 0.2644, 'grad_norm': 0.9611997067607455, 'learning_rate': 2.9333592086792113e-05, 'epoch': 3.73}
 19%|█▊        | 235/1260 [1:25:37<6:10:03, 21.66s/it] 19%|█▊        | 236/1260 [1:25:58<6:08:26, 21.59s/it] 19%|█▉        | 237/1260 [1:26:20<6:08:07, 21.59s/it] 19%|█▉        | 238/1260 [1:26:41<6:08:53, 21.66s/it] 19%|█▉        | 239/1260 [1:27:03<6:08:42, 21.67s/it] 19%|█▉        | 240/1260 [1:27:25<6:07:26, 21.61s/it]                                                      {'loss': 0.2634, 'grad_norm': 0.7577406062366931, 'learning_rate': 2.927097552138166e-05, 'epoch': 3.81}
 19%|█▉        | 240/1260 [1:27:25<6:07:26, 21.61s/it] 19%|█▉        | 241/1260 [1:27:46<6:06:16, 21.57s/it] 19%|█▉        | 242/1260 [1:28:08<6:06:22, 21.59s/it] 19%|█▉        | 243/1260 [1:28:29<6:04:43, 21.52s/it] 19%|█▉        | 244/1260 [1:28:51<6:05:53, 21.61s/it] 19%|█▉        | 245/1260 [1:29:12<6:05:05, 21.58s/it]                                                      {'loss': 0.2664, 'grad_norm': 0.7635278585960072, 'learning_rate': 2.920562078626055e-05, 'epoch': 3.89}
 19%|█▉        | 245/1260 [1:29:12<6:05:05, 21.58s/it] 20%|█▉        | 246/1260 [1:29:34<6:05:01, 21.60s/it] 20%|█▉        | 247/1260 [1:29:56<6:04:58, 21.62s/it] 20%|█▉        | 248/1260 [1:30:17<6:04:33, 21.61s/it] 20%|█▉        | 249/1260 [1:30:39<6:03:17, 21.56s/it] 20%|█▉        | 250/1260 [1:31:00<6:02:21, 21.53s/it]                                                      {'loss': 0.2665, 'grad_norm': 0.7802847401970638, 'learning_rate': 2.9137540421030987e-05, 'epoch': 3.97}
 20%|█▉        | 250/1260 [1:31:00<6:02:21, 21.53s/it] 20%|█▉        | 251/1260 [1:31:22<6:01:41, 21.51s/it] 20%|██        | 252/1260 [1:31:41<5:51:10, 20.90s/it][INFO|trainer.py:3993] 2025-07-04 23:02:03,231 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252
[INFO|configuration_utils.py:424] 2025-07-04 23:02:03,236 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:02:03,237 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:02:06,376 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:02:06,378 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:02:06,379 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:02:06,379 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/special_tokens_map.json
[2025-07-04 23:02:06,573] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step251 is about to be saved!
[2025-07-04 23:02:06,583] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:02:06,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:02:06,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:02:06,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:02:12,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:02:12,005] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/global_step251/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:02:14,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step251 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:02:14,753 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:02:14,753 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:02:14,754 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:02:14,755 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:02:14,911 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:02:15,343 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/chat_template.jinja
 20%|██        | 253/1260 [1:32:18<7:12:40, 25.78s/it] 20%|██        | 254/1260 [1:32:40<6:50:58, 24.51s/it] 20%|██        | 255/1260 [1:33:01<6:35:43, 23.62s/it]                                                      {'loss': 0.2466, 'grad_norm': 0.8687623869364026, 'learning_rate': 2.9066747488261378e-05, 'epoch': 4.05}
 20%|██        | 255/1260 [1:33:01<6:35:43, 23.62s/it] 20%|██        | 256/1260 [1:33:23<6:25:55, 23.06s/it] 20%|██        | 257/1260 [1:33:45<6:18:12, 22.62s/it] 20%|██        | 258/1260 [1:34:06<6:12:03, 22.28s/it] 21%|██        | 259/1260 [1:34:28<6:07:49, 22.05s/it] 21%|██        | 260/1260 [1:34:50<6:07:43, 22.06s/it]                                                      {'loss': 0.251, 'grad_norm': 1.2772772512494146, 'learning_rate': 2.899325557098001e-05, 'epoch': 4.13}
 21%|██        | 260/1260 [1:34:50<6:07:43, 22.06s/it] 21%|██        | 261/1260 [1:35:12<6:05:23, 21.95s/it] 21%|██        | 262/1260 [1:35:33<6:02:18, 21.78s/it] 21%|██        | 263/1260 [1:35:55<6:03:11, 21.86s/it] 21%|██        | 264/1260 [1:36:17<6:01:28, 21.78s/it] 21%|██        | 265/1260 [1:36:38<5:59:32, 21.68s/it]                                                      {'loss': 0.2519, 'grad_norm': 1.2670403381723527, 'learning_rate': 2.8917078770068882e-05, 'epoch': 4.21}
 21%|██        | 265/1260 [1:36:38<5:59:32, 21.68s/it] 21%|██        | 266/1260 [1:36:59<5:58:02, 21.61s/it] 21%|██        | 267/1260 [1:37:21<5:57:58, 21.63s/it] 21%|██▏       | 268/1260 [1:37:43<5:56:26, 21.56s/it] 21%|██▏       | 269/1260 [1:38:04<5:56:23, 21.58s/it] 21%|██▏       | 270/1260 [1:38:26<5:55:27, 21.54s/it]                                                      {'loss': 0.2465, 'grad_norm': 0.9709302034471968, 'learning_rate': 2.8838231701558182e-05, 'epoch': 4.29}
 21%|██▏       | 270/1260 [1:38:26<5:55:27, 21.54s/it] 22%|██▏       | 271/1260 [1:38:47<5:55:43, 21.58s/it] 22%|██▏       | 272/1260 [1:39:09<5:56:16, 21.64s/it] 22%|██▏       | 273/1260 [1:39:31<5:56:06, 21.65s/it] 22%|██▏       | 274/1260 [1:39:52<5:56:18, 21.68s/it] 22%|██▏       | 275/1260 [1:40:14<5:55:58, 21.68s/it]                                                      {'loss': 0.2495, 'grad_norm': 1.2658689768609659, 'learning_rate': 2.8756729493821883e-05, 'epoch': 4.37}
 22%|██▏       | 275/1260 [1:40:14<5:55:58, 21.68s/it] 22%|██▏       | 276/1260 [1:40:36<5:54:26, 21.61s/it] 22%|██▏       | 277/1260 [1:40:57<5:53:56, 21.60s/it] 22%|██▏       | 278/1260 [1:41:19<5:53:14, 21.58s/it] 22%|██▏       | 279/1260 [1:41:40<5:52:07, 21.54s/it] 22%|██▏       | 280/1260 [1:42:02<5:52:22, 21.57s/it]                                                      {'loss': 0.2446, 'grad_norm': 1.199008925222064, 'learning_rate': 2.8672587784675098e-05, 'epoch': 4.45}
 22%|██▏       | 280/1260 [1:42:02<5:52:22, 21.57s/it] 22%|██▏       | 281/1260 [1:42:23<5:51:31, 21.54s/it] 22%|██▏       | 282/1260 [1:42:45<5:50:51, 21.53s/it] 22%|██▏       | 283/1260 [1:43:06<5:50:01, 21.50s/it] 23%|██▎       | 284/1260 [1:43:28<5:50:00, 21.52s/it] 23%|██▎       | 285/1260 [1:43:49<5:49:44, 21.52s/it]                                                      {'loss': 0.2461, 'grad_norm': 0.8343952944606546, 'learning_rate': 2.8585822718373623e-05, 'epoch': 4.52}
 23%|██▎       | 285/1260 [1:43:49<5:49:44, 21.52s/it] 23%|██▎       | 286/1260 [1:44:11<5:51:23, 21.65s/it] 23%|██▎       | 287/1260 [1:44:33<5:50:33, 21.62s/it] 23%|██▎       | 288/1260 [1:44:55<5:51:34, 21.70s/it] 23%|██▎       | 289/1260 [1:45:16<5:49:54, 21.62s/it] 23%|██▎       | 290/1260 [1:45:38<5:50:49, 21.70s/it]                                                      {'loss': 0.2519, 'grad_norm': 1.315522949653855, 'learning_rate': 2.8496450942516373e-05, 'epoch': 4.6}
 23%|██▎       | 290/1260 [1:45:38<5:50:49, 21.70s/it] 23%|██▎       | 291/1260 [1:45:59<5:48:59, 21.61s/it] 23%|██▎       | 292/1260 [1:46:21<5:48:52, 21.62s/it] 23%|██▎       | 293/1260 [1:46:43<5:50:05, 21.72s/it] 23%|██▎       | 294/1260 [1:47:05<5:49:34, 21.71s/it] 23%|██▎       | 295/1260 [1:47:26<5:48:08, 21.65s/it]                                                      {'loss': 0.2477, 'grad_norm': 1.0674242658492386, 'learning_rate': 2.8404489604851186e-05, 'epoch': 4.68}
 23%|██▎       | 295/1260 [1:47:26<5:48:08, 21.65s/it] 23%|██▎       | 296/1260 [1:47:48<5:48:56, 21.72s/it] 24%|██▎       | 297/1260 [1:48:10<5:47:10, 21.63s/it] 24%|██▎       | 298/1260 [1:48:31<5:45:45, 21.56s/it] 24%|██▎       | 299/1260 [1:48:53<5:46:08, 21.61s/it] 24%|██▍       | 300/1260 [1:49:14<5:46:22, 21.65s/it]                                                      {'loss': 0.2468, 'grad_norm': 1.1433428509333103, 'learning_rate': 2.8309956349984684e-05, 'epoch': 4.76}
 24%|██▍       | 300/1260 [1:49:14<5:46:22, 21.65s/it] 24%|██▍       | 301/1260 [1:49:36<5:44:50, 21.58s/it] 24%|██▍       | 302/1260 [1:49:57<5:43:44, 21.53s/it] 24%|██▍       | 303/1260 [1:50:19<5:44:30, 21.60s/it] 24%|██▍       | 304/1260 [1:50:41<5:43:58, 21.59s/it] 24%|██▍       | 305/1260 [1:51:02<5:43:45, 21.60s/it]                                                      {'loss': 0.2525, 'grad_norm': 0.7148621542211445, 'learning_rate': 2.821286931599684e-05, 'epoch': 4.84}
 24%|██▍       | 305/1260 [1:51:02<5:43:45, 21.60s/it] 24%|██▍       | 306/1260 [1:51:24<5:42:46, 21.56s/it] 24%|██▍       | 307/1260 [1:51:46<5:45:41, 21.76s/it] 24%|██▍       | 308/1260 [1:52:07<5:43:20, 21.64s/it] 25%|██▍       | 309/1260 [1:52:29<5:43:49, 21.69s/it] 25%|██▍       | 310/1260 [1:52:51<5:43:14, 21.68s/it]                                                      {'loss': 0.2482, 'grad_norm': 2.222159040106017, 'learning_rate': 2.8113247130960783e-05, 'epoch': 4.92}
 25%|██▍       | 310/1260 [1:52:51<5:43:14, 21.68s/it] 25%|██▍       | 311/1260 [1:53:12<5:41:43, 21.61s/it] 25%|██▍       | 312/1260 [1:53:34<5:40:38, 21.56s/it] 25%|██▍       | 313/1260 [1:53:55<5:41:57, 21.67s/it] 25%|██▍       | 314/1260 [1:54:17<5:41:38, 21.67s/it] 25%|██▌       | 315/1260 [1:54:36<5:30:15, 20.97s/it]                                                      {'loss': 0.2438, 'grad_norm': 1.1569860122416549, 'learning_rate': 2.801110890936867e-05, 'epoch': 5.0}
 25%|██▌       | 315/1260 [1:54:36<5:30:15, 20.97s/it][INFO|trainer.py:3993] 2025-07-04 23:24:58,622 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315
[INFO|configuration_utils.py:424] 2025-07-04 23:24:58,628 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:24:58,630 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:25:02,267 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:25:02,270 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:25:02,271 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:25:02,272 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/special_tokens_map.json
[2025-07-04 23:25:02,460] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step314 is about to be saved!
[2025-07-04 23:25:02,472] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:25:02,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:25:02,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:25:02,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:25:10,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:25:10,116] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/global_step314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:25:10,634] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step314 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:25:10,646 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:25:10,647 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:25:10,648 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:25:10,648 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:25:10,825 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:25:11,238 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/chat_template.jinja
 25%|██▌       | 316/1260 [1:55:14<6:49:41, 26.04s/it] 25%|██▌       | 317/1260 [1:55:36<6:29:45, 24.80s/it] 25%|██▌       | 318/1260 [1:55:58<6:14:46, 23.87s/it] 25%|██▌       | 319/1260 [1:56:20<6:04:54, 23.27s/it] 25%|██▌       | 320/1260 [1:56:41<5:56:05, 22.73s/it]                                                      {'loss': 0.2353, 'grad_norm': 0.9953272261140819, 'learning_rate': 2.790647424846417e-05, 'epoch': 5.08}
 25%|██▌       | 320/1260 [1:56:41<5:56:05, 22.73s/it] 25%|██▌       | 321/1260 [1:57:03<5:52:20, 22.51s/it] 26%|██▌       | 322/1260 [1:57:25<5:49:14, 22.34s/it] 26%|██▌       | 323/1260 [1:57:47<5:44:59, 22.09s/it] 26%|██▌       | 324/1260 [1:58:08<5:42:16, 21.94s/it] 26%|██▌       | 325/1260 [1:58:30<5:41:20, 21.90s/it]                                                      {'loss': 0.2347, 'grad_norm': 1.6649746032292727, 'learning_rate': 2.7799363224482334e-05, 'epoch': 5.16}
 26%|██▌       | 325/1260 [1:58:30<5:41:20, 21.90s/it] 26%|██▌       | 326/1260 [1:58:52<5:39:17, 21.80s/it] 26%|██▌       | 327/1260 [1:59:13<5:38:16, 21.75s/it] 26%|██▌       | 328/1260 [1:59:35<5:37:08, 21.70s/it] 26%|██▌       | 329/1260 [1:59:56<5:35:29, 21.62s/it] 26%|██▌       | 330/1260 [2:00:18<5:35:08, 21.62s/it]                                                      {'loss': 0.2347, 'grad_norm': 1.1187409506693142, 'learning_rate': 2.7689796388797616e-05, 'epoch': 5.24}
 26%|██▌       | 330/1260 [2:00:18<5:35:08, 21.62s/it] 26%|██▋       | 331/1260 [2:00:39<5:34:11, 21.58s/it] 26%|██▋       | 332/1260 [2:01:01<5:35:16, 21.68s/it] 26%|██▋       | 333/1260 [2:01:23<5:34:17, 21.64s/it] 27%|██▋       | 334/1260 [2:01:45<5:34:29, 21.67s/it] 27%|██▋       | 335/1260 [2:02:06<5:33:41, 21.65s/it]                                                      {'loss': 0.2352, 'grad_norm': 0.9020578663733999, 'learning_rate': 2.7577794763980634e-05, 'epoch': 5.32}
 27%|██▋       | 335/1260 [2:02:06<5:33:41, 21.65s/it] 27%|██▋       | 336/1260 [2:02:28<5:32:54, 21.62s/it] 27%|██▋       | 337/1260 [2:02:50<5:33:20, 21.67s/it] 27%|██▋       | 338/1260 [2:03:11<5:33:21, 21.69s/it] 27%|██▋       | 339/1260 [2:03:33<5:32:59, 21.69s/it] 27%|██▋       | 340/1260 [2:03:55<5:31:43, 21.63s/it]                                                      {'loss': 0.2352, 'grad_norm': 0.759692936315537, 'learning_rate': 2.7463379839764596e-05, 'epoch': 5.4}
 27%|██▋       | 340/1260 [2:03:55<5:31:43, 21.63s/it] 27%|██▋       | 341/1260 [2:04:16<5:31:48, 21.66s/it] 27%|██▋       | 342/1260 [2:04:38<5:31:31, 21.67s/it] 27%|██▋       | 343/1260 [2:04:59<5:30:03, 21.60s/it] 27%|██▋       | 344/1260 [2:05:21<5:29:08, 21.56s/it] 27%|██▋       | 345/1260 [2:05:43<5:30:44, 21.69s/it]                                                      {'loss': 0.2363, 'grad_norm': 0.9368271637603729, 'learning_rate': 2.734657356892208e-05, 'epoch': 5.48}
 27%|██▋       | 345/1260 [2:05:43<5:30:44, 21.69s/it] 27%|██▋       | 346/1260 [2:06:05<5:30:27, 21.69s/it] 28%|██▊       | 347/1260 [2:06:26<5:29:17, 21.64s/it] 28%|██▊       | 348/1260 [2:06:48<5:28:00, 21.58s/it] 28%|██▊       | 349/1260 [2:07:09<5:27:03, 21.54s/it] 28%|██▊       | 350/1260 [2:07:30<5:26:24, 21.52s/it]                                                      {'loss': 0.24, 'grad_norm': 1.1669897110120544, 'learning_rate': 2.7227398363052918e-05, 'epoch': 5.56}
 28%|██▊       | 350/1260 [2:07:30<5:26:24, 21.52s/it] 28%|██▊       | 351/1260 [2:07:52<5:25:42, 21.50s/it] 28%|██▊       | 352/1260 [2:08:14<5:28:27, 21.70s/it] 28%|██▊       | 353/1260 [2:08:36<5:28:46, 21.75s/it] 28%|██▊       | 354/1260 [2:08:58<5:29:28, 21.82s/it] 28%|██▊       | 355/1260 [2:09:20<5:29:10, 21.82s/it]                                                      {'loss': 0.2358, 'grad_norm': 1.262622820523757, 'learning_rate': 2.710587708828414e-05, 'epoch': 5.64}
 28%|██▊       | 355/1260 [2:09:20<5:29:10, 21.82s/it] 28%|██▊       | 356/1260 [2:09:41<5:27:49, 21.76s/it] 28%|██▊       | 357/1260 [2:10:03<5:27:56, 21.79s/it] 28%|██▊       | 358/1260 [2:10:25<5:26:18, 21.71s/it] 28%|██▊       | 359/1260 [2:10:46<5:24:52, 21.63s/it] 29%|██▊       | 360/1260 [2:11:08<5:24:33, 21.64s/it]                                                      {'loss': 0.2429, 'grad_norm': 1.8122524295236455, 'learning_rate': 2.698203306088262e-05, 'epoch': 5.72}
 29%|██▊       | 360/1260 [2:11:08<5:24:33, 21.64s/it] 29%|██▊       | 361/1260 [2:11:29<5:23:38, 21.60s/it] 29%|██▊       | 362/1260 [2:11:51<5:22:47, 21.57s/it] 29%|██▉       | 363/1260 [2:12:12<5:21:45, 21.52s/it] 29%|██▉       | 364/1260 [2:12:34<5:20:55, 21.49s/it] 29%|██▉       | 365/1260 [2:12:55<5:21:18, 21.54s/it]                                                      {'loss': 0.2412, 'grad_norm': 1.0589668294745727, 'learning_rate': 2.685589004278139e-05, 'epoch': 5.8}
 29%|██▉       | 365/1260 [2:12:55<5:21:18, 21.54s/it] 29%|██▉       | 366/1260 [2:13:17<5:22:04, 21.62s/it] 29%|██▉       | 367/1260 [2:13:39<5:22:02, 21.64s/it] 29%|██▉       | 368/1260 [2:14:00<5:21:45, 21.64s/it] 29%|██▉       | 369/1260 [2:14:22<5:21:41, 21.66s/it] 29%|██▉       | 370/1260 [2:14:44<5:20:07, 21.58s/it]                                                      {'loss': 0.24, 'grad_norm': 0.8685839392289889, 'learning_rate': 2.672747223702045e-05, 'epoch': 5.87}
 29%|██▉       | 370/1260 [2:14:44<5:20:07, 21.58s/it] 29%|██▉       | 371/1260 [2:15:05<5:19:55, 21.59s/it] 30%|██▉       | 372/1260 [2:15:27<5:19:00, 21.55s/it] 30%|██▉       | 373/1260 [2:15:48<5:18:28, 21.54s/it] 30%|██▉       | 374/1260 [2:16:10<5:18:01, 21.54s/it] 30%|██▉       | 375/1260 [2:16:31<5:18:10, 21.57s/it]                                                      {'loss': 0.2385, 'grad_norm': 1.2063854703297427, 'learning_rate': 2.6596804283102928e-05, 'epoch': 5.95}
 30%|██▉       | 375/1260 [2:16:31<5:18:10, 21.57s/it] 30%|██▉       | 376/1260 [2:16:53<5:17:22, 21.54s/it] 30%|██▉       | 377/1260 [2:17:15<5:18:25, 21.64s/it] 30%|███       | 378/1260 [2:17:34<5:08:51, 21.01s/it][INFO|trainer.py:3993] 2025-07-04 23:47:56,529 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378
[INFO|configuration_utils.py:424] 2025-07-04 23:47:56,534 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/config.json
[INFO|configuration_utils.py:904] 2025-07-04 23:47:56,535 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-04 23:47:59,732 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:47:59,734 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:47:59,735 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:47:59,735 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/special_tokens_map.json
[2025-07-04 23:47:59,927] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step377 is about to be saved!
[2025-07-04 23:47:59,937] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-04 23:47:59,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-04 23:47:59,971] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-04 23:47:59,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-04 23:48:05,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-04 23:48:05,524] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/global_step377/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-04 23:48:08,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step377 is ready now!
[INFO|image_processing_base.py:260] 2025-07-04 23:48:08,077 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-04 23:48:08,078 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-04 23:48:08,079 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-04 23:48:08,079 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-04 23:48:08,247 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-04 23:48:08,660 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/chat_template.jinja
 30%|███       | 379/1260 [2:18:12<6:22:38, 26.06s/it] 30%|███       | 380/1260 [2:18:33<6:01:49, 24.67s/it]                                                      {'loss': 0.2229, 'grad_norm': 1.0867394003779582, 'learning_rate': 2.646391125226751e-05, 'epoch': 6.03}
 30%|███       | 380/1260 [2:18:33<6:01:49, 24.67s/it] 30%|███       | 381/1260 [2:18:55<5:47:28, 23.72s/it] 30%|███       | 382/1260 [2:19:16<5:37:25, 23.06s/it] 30%|███       | 383/1260 [2:19:38<5:30:41, 22.62s/it] 30%|███       | 384/1260 [2:20:00<5:25:19, 22.28s/it] 31%|███       | 385/1260 [2:20:21<5:21:35, 22.05s/it]                                                      {'loss': 0.2242, 'grad_norm': 2.344672449530056, 'learning_rate': 2.6328818642678026e-05, 'epoch': 6.11}
 31%|███       | 385/1260 [2:20:21<5:21:35, 22.05s/it] 31%|███       | 386/1260 [2:20:43<5:18:53, 21.89s/it] 31%|███       | 387/1260 [2:21:04<5:17:17, 21.81s/it] 31%|███       | 388/1260 [2:21:26<5:15:20, 21.70s/it] 31%|███       | 389/1260 [2:21:47<5:13:57, 21.63s/it] 31%|███       | 390/1260 [2:22:09<5:12:52, 21.58s/it]                                                      {'loss': 0.2183, 'grad_norm': 1.2650776940480963, 'learning_rate': 2.6191552374531105e-05, 'epoch': 6.19}
 31%|███       | 390/1260 [2:22:09<5:12:52, 21.58s/it] 31%|███       | 391/1260 [2:22:30<5:12:47, 21.60s/it] 31%|███       | 392/1260 [2:22:52<5:13:08, 21.65s/it] 31%|███       | 393/1260 [2:23:14<5:12:47, 21.65s/it] 31%|███▏      | 394/1260 [2:23:35<5:12:33, 21.66s/it] 31%|███▏      | 395/1260 [2:23:57<5:11:26, 21.60s/it]                                                      {'loss': 0.2163, 'grad_norm': 1.3348328422858073, 'learning_rate': 2.6052138785082897e-05, 'epoch': 6.27}
 31%|███▏      | 395/1260 [2:23:57<5:11:26, 21.60s/it] 31%|███▏      | 396/1260 [2:24:18<5:10:59, 21.60s/it] 32%|███▏      | 397/1260 [2:24:40<5:10:05, 21.56s/it] 32%|███▏      | 398/1260 [2:25:01<5:09:35, 21.55s/it] 32%|███▏      | 399/1260 [2:25:23<5:09:27, 21.57s/it] 32%|███▏      | 400/1260 [2:25:45<5:10:49, 21.69s/it]                                                      {'loss': 0.2178, 'grad_norm': 1.0723068982085373, 'learning_rate': 2.5910604623595732e-05, 'epoch': 6.35}
 32%|███▏      | 400/1260 [2:25:45<5:10:49, 21.69s/it] 32%|███▏      | 401/1260 [2:26:07<5:10:15, 21.67s/it] 32%|███▏      | 402/1260 [2:26:28<5:09:06, 21.62s/it] 32%|███▏      | 403/1260 [2:26:50<5:08:53, 21.63s/it] 32%|███▏      | 404/1260 [2:27:11<5:07:41, 21.57s/it] 32%|███▏      | 405/1260 [2:27:33<5:07:11, 21.56s/it]                                                      {'loss': 0.2161, 'grad_norm': 0.8220941125327542, 'learning_rate': 2.5766977046205735e-05, 'epoch': 6.43}
 32%|███▏      | 405/1260 [2:27:33<5:07:11, 21.56s/it] 32%|███▏      | 406/1260 [2:27:54<5:07:42, 21.62s/it] 32%|███▏      | 407/1260 [2:28:17<5:10:17, 21.83s/it] 32%|███▏      | 408/1260 [2:28:39<5:10:21, 21.86s/it] 32%|███▏      | 409/1260 [2:29:01<5:09:55, 21.85s/it] 33%|███▎      | 410/1260 [2:29:22<5:08:16, 21.76s/it]                                                      {'loss': 0.2221, 'grad_norm': 1.9522165216615843, 'learning_rate': 2.5621283610712407e-05, 'epoch': 6.51}
 33%|███▎      | 410/1260 [2:29:22<5:08:16, 21.76s/it] 33%|███▎      | 411/1260 [2:29:44<5:08:10, 21.78s/it] 33%|███▎      | 412/1260 [2:30:05<5:06:33, 21.69s/it] 33%|███▎      | 413/1260 [2:30:27<5:05:18, 21.63s/it] 33%|███▎      | 414/1260 [2:30:49<5:05:20, 21.66s/it] 33%|███▎      | 415/1260 [2:31:10<5:04:38, 21.63s/it]                                                      {'loss': 0.2209, 'grad_norm': 1.0662833754801682, 'learning_rate': 2.5473552271291092e-05, 'epoch': 6.59}
 33%|███▎      | 415/1260 [2:31:10<5:04:38, 21.63s/it] 33%|███▎      | 416/1260 [2:31:32<5:03:55, 21.61s/it] 33%|███▎      | 417/1260 [2:31:53<5:03:07, 21.57s/it] 33%|███▎      | 418/1260 [2:32:15<5:02:12, 21.54s/it] 33%|███▎      | 419/1260 [2:32:36<5:02:39, 21.59s/it] 33%|███▎      | 420/1260 [2:32:58<5:02:45, 21.63s/it]                                                      {'loss': 0.2207, 'grad_norm': 1.0802637785718283, 'learning_rate': 2.5323811373129434e-05, 'epoch': 6.67}
 33%|███▎      | 420/1260 [2:32:58<5:02:45, 21.63s/it] 33%|███▎      | 421/1260 [2:33:20<5:02:34, 21.64s/it] 33%|███▎      | 422/1260 [2:33:41<5:02:26, 21.65s/it] 34%|███▎      | 423/1260 [2:34:03<5:02:27, 21.68s/it] 34%|███▎      | 424/1260 [2:34:25<5:03:07, 21.76s/it] 34%|███▎      | 425/1260 [2:34:47<5:01:29, 21.66s/it]                                                      {'loss': 0.217, 'grad_norm': 1.189923287003791, 'learning_rate': 2.5172089646988765e-05, 'epoch': 6.75}
 34%|███▎      | 425/1260 [2:34:47<5:01:29, 21.66s/it] 34%|███▍      | 426/1260 [2:35:08<5:01:36, 21.70s/it] 34%|███▍      | 427/1260 [2:35:30<5:01:49, 21.74s/it] 34%|███▍      | 428/1260 [2:35:52<5:00:20, 21.66s/it] 34%|███▍      | 429/1260 [2:36:14<5:00:49, 21.72s/it] 34%|███▍      | 430/1260 [2:36:35<4:59:15, 21.63s/it]                                                      {'loss': 0.2183, 'grad_norm': 0.970374574133261, 'learning_rate': 2.501841620369156e-05, 'epoch': 6.83}
 34%|███▍      | 430/1260 [2:36:35<4:59:15, 21.63s/it] 34%|███▍      | 431/1260 [2:36:57<4:59:15, 21.66s/it] 34%|███▍      | 432/1260 [2:37:18<4:58:43, 21.65s/it] 34%|███▍      | 433/1260 [2:37:40<4:57:38, 21.59s/it] 34%|███▍      | 434/1260 [2:38:02<4:58:18, 21.67s/it] 35%|███▍      | 435/1260 [2:38:23<4:57:15, 21.62s/it]                                                      {'loss': 0.2241, 'grad_norm': 0.9196162949073359, 'learning_rate': 2.4862820528535955e-05, 'epoch': 6.91}
 35%|███▍      | 435/1260 [2:38:23<4:57:15, 21.62s/it] 35%|███▍      | 436/1260 [2:38:45<4:58:23, 21.73s/it] 35%|███▍      | 437/1260 [2:39:07<4:58:07, 21.74s/it] 35%|███▍      | 438/1260 [2:39:28<4:57:13, 21.69s/it] 35%|███▍      | 439/1260 [2:39:50<4:56:31, 21.67s/it] 35%|███▍      | 440/1260 [2:40:12<4:56:38, 21.71s/it]                                                      {'loss': 0.3931, 'grad_norm': 1.5038957699894588, 'learning_rate': 2.470533247563837e-05, 'epoch': 6.99}
 35%|███▍      | 440/1260 [2:40:12<4:56:38, 21.71s/it] 35%|███▌      | 441/1260 [2:40:32<4:49:11, 21.19s/it][INFO|trainer.py:3993] 2025-07-05 00:10:54,142 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441
[INFO|configuration_utils.py:424] 2025-07-05 00:10:54,149 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:10:54,150 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:10:56,852 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:10:56,854 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:10:56,855 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:10:56,856 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/special_tokens_map.json
[2025-07-05 00:10:57,055] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is about to be saved!
[2025-07-05 00:10:57,066] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:10:57,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:10:57,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:10:57,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:11:01,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:11:01,700] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:11:05,184] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step440 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:11:05,195 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:11:05,195 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:11:05,196 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:11:05,196 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:11:05,363 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:11:05,766 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/chat_template.jinja
 35%|███▌      | 442/1260 [2:41:09<5:56:12, 26.13s/it] 35%|███▌      | 443/1260 [2:41:31<5:37:59, 24.82s/it] 35%|███▌      | 444/1260 [2:41:53<5:24:50, 23.89s/it] 35%|███▌      | 445/1260 [2:42:15<5:15:36, 23.23s/it]                                                      {'loss': 0.2202, 'grad_norm': 1.4546240214667536, 'learning_rate': 2.4545982262205455e-05, 'epoch': 7.06}
 35%|███▌      | 445/1260 [2:42:15<5:15:36, 23.23s/it] 35%|███▌      | 446/1260 [2:42:36<5:08:12, 22.72s/it] 35%|███▌      | 447/1260 [2:42:58<5:04:00, 22.44s/it] 36%|███▌      | 448/1260 [2:43:20<5:01:52, 22.31s/it] 36%|███▌      | 449/1260 [2:43:41<4:58:24, 22.08s/it] 36%|███▌      | 450/1260 [2:44:03<4:55:30, 21.89s/it]                                                      {'loss': 0.2011, 'grad_norm': 1.31802019054049, 'learning_rate': 2.4384800462736265e-05, 'epoch': 7.14}
 36%|███▌      | 450/1260 [2:44:03<4:55:30, 21.89s/it] 36%|███▌      | 451/1260 [2:44:25<4:54:13, 21.82s/it] 36%|███▌      | 452/1260 [2:44:46<4:53:51, 21.82s/it] 36%|███▌      | 453/1260 [2:45:08<4:51:50, 21.70s/it] 36%|███▌      | 454/1260 [2:45:29<4:50:13, 21.61s/it] 36%|███▌      | 455/1260 [2:45:51<4:51:27, 21.72s/it]                                                      {'loss': 0.1986, 'grad_norm': 1.1378239567192576, 'learning_rate': 2.422181800315599e-05, 'epoch': 7.22}
 36%|███▌      | 455/1260 [2:45:51<4:51:27, 21.72s/it] 36%|███▌      | 456/1260 [2:46:13<4:50:04, 21.65s/it] 36%|███▋      | 457/1260 [2:46:34<4:49:17, 21.62s/it] 36%|███▋      | 458/1260 [2:46:56<4:48:16, 21.57s/it] 36%|███▋      | 459/1260 [2:47:17<4:48:18, 21.60s/it] 37%|███▋      | 460/1260 [2:47:39<4:47:08, 21.54s/it]                                                      {'loss': 0.2468, 'grad_norm': 11.062817979552259, 'learning_rate': 2.405706615488216e-05, 'epoch': 7.3}
 37%|███▋      | 460/1260 [2:47:39<4:47:08, 21.54s/it] 37%|███▋      | 461/1260 [2:48:00<4:46:25, 21.51s/it] 37%|███▋      | 462/1260 [2:48:22<4:46:45, 21.56s/it] 37%|███▋      | 463/1260 [2:48:43<4:45:58, 21.53s/it] 37%|███▋      | 464/1260 [2:49:05<4:45:37, 21.53s/it] 37%|███▋      | 465/1260 [2:49:26<4:45:35, 21.55s/it]                                                      {'loss': 0.2289, 'grad_norm': 1.4462608135925115, 'learning_rate': 2.3890576528824637e-05, 'epoch': 7.38}
 37%|███▋      | 465/1260 [2:49:26<4:45:35, 21.55s/it] 37%|███▋      | 466/1260 [2:49:48<4:44:53, 21.53s/it] 37%|███▋      | 467/1260 [2:50:10<4:45:44, 21.62s/it] 37%|███▋      | 468/1260 [2:50:31<4:44:46, 21.57s/it] 37%|███▋      | 469/1260 [2:50:53<4:45:02, 21.62s/it] 37%|███▋      | 470/1260 [2:51:15<4:45:26, 21.68s/it]                                                      {'loss': 0.2039, 'grad_norm': 2.514234480114598, 'learning_rate': 2.37223810693204e-05, 'epoch': 7.46}
 37%|███▋      | 470/1260 [2:51:15<4:45:26, 21.68s/it] 37%|███▋      | 471/1260 [2:51:36<4:44:24, 21.63s/it] 37%|███▋      | 472/1260 [2:51:58<4:44:05, 21.63s/it] 38%|███▊      | 473/1260 [2:52:20<4:45:23, 21.76s/it] 38%|███▊      | 474/1260 [2:52:42<4:46:55, 21.90s/it] 38%|███▊      | 475/1260 [2:53:04<4:45:10, 21.80s/it]                                                      {'loss': 0.1981, 'grad_norm': 1.2052787398201201, 'learning_rate': 2.3552512048004428e-05, 'epoch': 7.54}
 38%|███▊      | 475/1260 [2:53:04<4:45:10, 21.80s/it] 38%|███▊      | 476/1260 [2:53:26<4:46:20, 21.91s/it] 38%|███▊      | 477/1260 [2:53:47<4:44:02, 21.77s/it] 38%|███▊      | 478/1260 [2:54:09<4:42:30, 21.68s/it] 38%|███▊      | 479/1260 [2:54:31<4:43:00, 21.74s/it] 38%|███▊      | 480/1260 [2:54:52<4:41:28, 21.65s/it]                                                      {'loss': 0.1964, 'grad_norm': 1.1241909656671438, 'learning_rate': 2.3381002057617706e-05, 'epoch': 7.62}
 38%|███▊      | 480/1260 [2:54:52<4:41:28, 21.65s/it] 38%|███▊      | 481/1260 [2:55:14<4:41:28, 21.68s/it] 38%|███▊      | 482/1260 [2:55:35<4:40:10, 21.61s/it] 38%|███▊      | 483/1260 [2:55:57<4:41:07, 21.71s/it] 38%|███▊      | 484/1260 [2:56:19<4:39:49, 21.64s/it] 38%|███▊      | 485/1260 [2:56:41<4:40:24, 21.71s/it]                                                      {'loss': 0.1951, 'grad_norm': 1.1472987132637105, 'learning_rate': 2.3207884005753707e-05, 'epoch': 7.7}
 38%|███▊      | 485/1260 [2:56:41<4:40:24, 21.71s/it] 39%|███▊      | 486/1260 [2:57:02<4:40:25, 21.74s/it] 39%|███▊      | 487/1260 [2:57:24<4:39:45, 21.71s/it] 39%|███▊      | 488/1260 [2:57:46<4:39:11, 21.70s/it] 39%|███▉      | 489/1260 [2:58:07<4:38:34, 21.68s/it] 39%|███▉      | 490/1260 [2:58:29<4:37:35, 21.63s/it]                                                      {'loss': 0.1981, 'grad_norm': 1.7031534794136496, 'learning_rate': 2.303319110854438e-05, 'epoch': 7.78}
 39%|███▉      | 490/1260 [2:58:29<4:37:35, 21.63s/it] 39%|███▉      | 491/1260 [2:58:50<4:36:43, 21.59s/it] 39%|███▉      | 492/1260 [2:59:12<4:36:14, 21.58s/it] 39%|███▉      | 493/1260 [2:59:34<4:35:49, 21.58s/it] 39%|███▉      | 494/1260 [2:59:55<4:34:57, 21.54s/it] 39%|███▉      | 495/1260 [3:00:16<4:34:02, 21.49s/it]                                                      {'loss': 0.198, 'grad_norm': 1.0446701065952415, 'learning_rate': 2.2856956884286986e-05, 'epoch': 7.86}
 39%|███▉      | 495/1260 [3:00:16<4:34:02, 21.49s/it] 39%|███▉      | 496/1260 [3:00:38<4:33:31, 21.48s/it] 39%|███▉      | 497/1260 [3:01:00<4:33:45, 21.53s/it] 40%|███▉      | 498/1260 [3:01:21<4:33:12, 21.51s/it] 40%|███▉      | 499/1260 [3:01:43<4:34:39, 21.66s/it] 40%|███▉      | 500/1260 [3:02:05<4:33:59, 21.63s/it]                                                      {'loss': 0.1998, 'grad_norm': 1.0424409280037914, 'learning_rate': 2.2679215147012955e-05, 'epoch': 7.94}
 40%|███▉      | 500/1260 [3:02:05<4:33:59, 21.63s/it] 40%|███▉      | 501/1260 [3:02:26<4:33:25, 21.61s/it] 40%|███▉      | 502/1260 [3:02:48<4:32:32, 21.57s/it] 40%|███▉      | 503/1260 [3:03:09<4:32:14, 21.58s/it] 40%|████      | 504/1260 [3:03:29<4:23:24, 20.91s/it][INFO|trainer.py:3993] 2025-07-05 00:33:50,946 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504
[INFO|configuration_utils.py:424] 2025-07-05 00:33:50,952 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:33:50,953 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:33:53,825 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:33:53,828 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:33:53,829 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:33:53,829 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/special_tokens_map.json
[2025-07-05 00:33:54,029] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step503 is about to be saved!
[2025-07-05 00:33:54,040] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:33:54,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:33:54,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:33:54,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:33:58,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:33:58,666] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/global_step503/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:34:02,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step503 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:34:02,197 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:34:02,198 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:34:02,199 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:34:02,199 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:34:02,358 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:34:02,769 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/chat_template.jinja
 40%|████      | 505/1260 [3:04:06<5:26:36, 25.96s/it]                                                      {'loss': 0.186, 'grad_norm': 1.3929838005990742, 'learning_rate': 2.25e-05, 'epoch': 8.02}
 40%|████      | 505/1260 [3:04:06<5:26:36, 25.96s/it] 40%|████      | 506/1260 [3:04:28<5:09:36, 24.64s/it] 40%|████      | 507/1260 [3:04:49<4:57:40, 23.72s/it] 40%|████      | 508/1260 [3:05:11<4:48:46, 23.04s/it] 40%|████      | 509/1260 [3:05:33<4:43:31, 22.65s/it] 40%|████      | 510/1260 [3:05:54<4:38:50, 22.31s/it]                                                      {'loss': 0.1679, 'grad_norm': 2.02093378188208, 'learning_rate': 2.2319345829228706e-05, 'epoch': 8.1}
 40%|████      | 510/1260 [3:05:54<4:38:50, 22.31s/it] 41%|████      | 511/1260 [3:06:16<4:36:25, 22.14s/it] 41%|████      | 512/1260 [3:06:37<4:33:50, 21.97s/it] 41%|████      | 513/1260 [3:06:59<4:31:51, 21.84s/it] 41%|████      | 514/1260 [3:07:21<4:31:15, 21.82s/it] 41%|████      | 515/1260 [3:07:42<4:29:25, 21.70s/it]                                                      {'loss': 0.1672, 'grad_norm': 1.0332205173585078, 'learning_rate': 2.213728729678491e-05, 'epoch': 8.17}
 41%|████      | 515/1260 [3:07:42<4:29:25, 21.70s/it] 41%|████      | 516/1260 [3:08:04<4:28:10, 21.63s/it] 41%|████      | 517/1260 [3:08:26<4:28:49, 21.71s/it] 41%|████      | 518/1260 [3:08:47<4:28:23, 21.70s/it] 41%|████      | 519/1260 [3:09:09<4:27:11, 21.64s/it] 41%|████▏     | 520/1260 [3:09:30<4:26:14, 21.59s/it]                                                      {'loss': 0.1706, 'grad_norm': 1.1134285361677814, 'learning_rate': 2.1953859334209085e-05, 'epoch': 8.25}
 41%|████▏     | 520/1260 [3:09:30<4:26:14, 21.59s/it] 41%|████▏     | 521/1260 [3:09:52<4:25:38, 21.57s/it] 41%|████▏     | 522/1260 [3:10:13<4:24:59, 21.54s/it] 42%|████▏     | 523/1260 [3:10:35<4:24:19, 21.52s/it] 42%|████▏     | 524/1260 [3:10:56<4:24:27, 21.56s/it] 42%|████▏     | 525/1260 [3:11:18<4:23:35, 21.52s/it]                                                      {'loss': 0.1692, 'grad_norm': 2.1345382064456158, 'learning_rate': 2.1769097135794052e-05, 'epoch': 8.33}
 42%|████▏     | 525/1260 [3:11:18<4:23:35, 21.52s/it] 42%|████▏     | 526/1260 [3:11:39<4:23:47, 21.56s/it] 42%|████▏     | 527/1260 [3:12:01<4:24:05, 21.62s/it] 42%|████▏     | 528/1260 [3:12:23<4:24:58, 21.72s/it] 42%|████▏     | 529/1260 [3:12:44<4:23:21, 21.62s/it] 42%|████▏     | 530/1260 [3:13:06<4:22:05, 21.54s/it]                                                      {'loss': 0.171, 'grad_norm': 1.6699189986978338, 'learning_rate': 2.158303615183223e-05, 'epoch': 8.41}
 42%|████▏     | 530/1260 [3:13:06<4:22:05, 21.54s/it] 42%|████▏     | 531/1260 [3:13:27<4:21:34, 21.53s/it] 42%|████▏     | 532/1260 [3:13:50<4:24:06, 21.77s/it] 42%|████▏     | 533/1260 [3:14:11<4:23:00, 21.71s/it] 42%|████▏     | 534/1260 [3:14:33<4:21:45, 21.63s/it] 42%|████▏     | 535/1260 [3:14:54<4:21:34, 21.65s/it]                                                      {'loss': 0.1682, 'grad_norm': 2.1152823627175077, 'learning_rate': 2.139571208181381e-05, 'epoch': 8.49}
 42%|████▏     | 535/1260 [3:14:54<4:21:34, 21.65s/it] 43%|████▎     | 536/1260 [3:15:16<4:20:38, 21.60s/it] 43%|████▎     | 537/1260 [3:15:37<4:20:26, 21.61s/it] 43%|████▎     | 538/1260 [3:15:59<4:21:30, 21.73s/it] 43%|████▎     | 539/1260 [3:16:21<4:21:02, 21.72s/it] 43%|████▎     | 540/1260 [3:16:43<4:20:29, 21.71s/it]                                                      {'loss': 0.1652, 'grad_norm': 0.942644467270543, 'learning_rate': 2.1207160867577087e-05, 'epoch': 8.57}
 43%|████▎     | 540/1260 [3:16:43<4:20:29, 21.71s/it] 43%|████▎     | 541/1260 [3:17:05<4:20:31, 21.74s/it] 43%|████▎     | 542/1260 [3:17:27<4:21:07, 21.82s/it] 43%|████▎     | 543/1260 [3:17:48<4:19:30, 21.72s/it] 43%|████▎     | 544/1260 [3:18:10<4:18:17, 21.64s/it] 43%|████▎     | 545/1260 [3:18:31<4:17:32, 21.61s/it]                                                      {'loss': 0.168, 'grad_norm': 1.1882636729859994, 'learning_rate': 2.101741868641233e-05, 'epoch': 8.65}
 43%|████▎     | 545/1260 [3:18:31<4:17:32, 21.61s/it] 43%|████▎     | 546/1260 [3:18:53<4:16:26, 21.55s/it] 43%|████▎     | 547/1260 [3:19:14<4:16:00, 21.54s/it] 43%|████▎     | 548/1260 [3:19:36<4:15:27, 21.53s/it] 44%|████▎     | 549/1260 [3:19:57<4:14:42, 21.49s/it] 44%|████▎     | 550/1260 [3:20:18<4:14:16, 21.49s/it]                                                      {'loss': 0.1713, 'grad_norm': 1.2260310446067422, 'learning_rate': 2.082652194412042e-05, 'epoch': 8.73}
 44%|████▎     | 550/1260 [3:20:18<4:14:16, 21.49s/it] 44%|████▎     | 551/1260 [3:20:40<4:13:55, 21.49s/it] 44%|████▍     | 552/1260 [3:21:02<4:15:02, 21.61s/it] 44%|████▍     | 553/1260 [3:21:23<4:14:10, 21.57s/it] 44%|████▍     | 554/1260 [3:21:45<4:13:32, 21.55s/it] 44%|████▍     | 555/1260 [3:22:07<4:13:43, 21.59s/it]                                                      {'loss': 0.1706, 'grad_norm': 1.946166291878918, 'learning_rate': 2.0634507268027702e-05, 'epoch': 8.81}
 44%|████▍     | 555/1260 [3:22:07<4:13:43, 21.59s/it] 44%|████▍     | 556/1260 [3:22:28<4:13:45, 21.63s/it] 44%|████▍     | 557/1260 [3:22:50<4:13:39, 21.65s/it] 44%|████▍     | 558/1260 [3:23:12<4:13:36, 21.68s/it] 44%|████▍     | 559/1260 [3:23:33<4:12:22, 21.60s/it] 44%|████▍     | 560/1260 [3:23:55<4:11:56, 21.59s/it]                                                      {'loss': 0.1745, 'grad_norm': 2.7070965884612193, 'learning_rate': 2.0441411499958287e-05, 'epoch': 8.89}
 44%|████▍     | 560/1260 [3:23:55<4:11:56, 21.59s/it] 45%|████▍     | 561/1260 [3:24:16<4:11:09, 21.56s/it] 45%|████▍     | 562/1260 [3:24:38<4:10:23, 21.52s/it] 45%|████▍     | 563/1260 [3:24:59<4:09:37, 21.49s/it] 45%|████▍     | 564/1260 [3:25:20<4:09:14, 21.49s/it] 45%|████▍     | 565/1260 [3:25:42<4:09:51, 21.57s/it]                                                      {'loss': 0.1763, 'grad_norm': 1.3887197896841392, 'learning_rate': 2.0247271689165226e-05, 'epoch': 8.97}
 45%|████▍     | 565/1260 [3:25:42<4:09:51, 21.57s/it] 45%|████▍     | 566/1260 [3:26:04<4:11:03, 21.70s/it] 45%|████▌     | 567/1260 [3:26:24<4:02:40, 21.01s/it][INFO|trainer.py:3993] 2025-07-05 00:56:45,419 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567
[INFO|configuration_utils.py:424] 2025-07-05 00:56:45,426 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/config.json
[INFO|configuration_utils.py:904] 2025-07-05 00:56:45,427 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 00:56:48,570 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:56:48,573 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:56:48,574 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:56:48,575 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/special_tokens_map.json
[2025-07-05 00:56:48,766] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step565 is about to be saved!
[2025-07-05 00:56:48,777] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 00:56:48,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 00:56:48,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 00:56:48,812] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 00:56:53,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 00:56:53,630] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/global_step565/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 00:56:56,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step565 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 00:56:56,913 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 00:56:56,914 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 00:56:56,914 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 00:56:56,914 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 00:56:57,070 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 00:56:57,477 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/chat_template.jinja
 45%|████▌     | 568/1260 [3:27:01<4:57:57, 25.83s/it] 45%|████▌     | 569/1260 [3:27:23<4:44:13, 24.68s/it] 45%|████▌     | 570/1260 [3:27:44<4:32:59, 23.74s/it]                                                      {'loss': 0.1489, 'grad_norm': 1.2865519945888113, 'learning_rate': 2.0052125085221868e-05, 'epoch': 9.05}
 45%|████▌     | 570/1260 [3:27:44<4:32:59, 23.74s/it] 45%|████▌     | 571/1260 [3:28:06<4:25:14, 23.10s/it] 45%|████▌     | 572/1260 [3:28:27<4:19:36, 22.64s/it] 45%|████▌     | 573/1260 [3:28:49<4:15:15, 22.29s/it] 46%|████▌     | 574/1260 [3:29:10<4:12:05, 22.05s/it] 46%|████▌     | 575/1260 [3:29:32<4:09:41, 21.87s/it]                                                      {'loss': 0.1353, 'grad_norm': 2.107439747056331, 'learning_rate': 1.985600913087482e-05, 'epoch': 9.13}
 46%|████▌     | 575/1260 [3:29:32<4:09:41, 21.87s/it] 46%|████▌     | 576/1260 [3:29:53<4:08:25, 21.79s/it] 46%|████▌     | 577/1260 [3:30:15<4:08:35, 21.84s/it] 46%|████▌     | 578/1260 [3:30:37<4:07:32, 21.78s/it] 46%|████▌     | 579/1260 [3:30:59<4:06:03, 21.68s/it] 46%|████▌     | 580/1260 [3:31:20<4:04:59, 21.62s/it]                                                      {'loss': 0.1357, 'grad_norm': 2.1995164809075702, 'learning_rate': 1.9658961454859758e-05, 'epoch': 9.21}
 46%|████▌     | 580/1260 [3:31:20<4:04:59, 21.62s/it] 46%|████▌     | 581/1260 [3:31:42<4:04:59, 21.65s/it] 46%|████▌     | 582/1260 [3:32:03<4:03:55, 21.59s/it] 46%|████▋     | 583/1260 [3:32:25<4:04:05, 21.63s/it] 46%|████▋     | 584/1260 [3:32:47<4:04:32, 21.70s/it] 46%|████▋     | 585/1260 [3:33:08<4:03:47, 21.67s/it]                                                      {'loss': 0.1341, 'grad_norm': 1.9210240227382867, 'learning_rate': 1.946101986468167e-05, 'epoch': 9.29}
 46%|████▋     | 585/1260 [3:33:08<4:03:47, 21.67s/it] 47%|████▋     | 586/1260 [3:33:30<4:03:42, 21.70s/it] 47%|████▋     | 587/1260 [3:33:52<4:02:47, 21.65s/it] 47%|████▋     | 588/1260 [3:34:13<4:02:41, 21.67s/it] 47%|████▋     | 589/1260 [3:34:35<4:01:36, 21.61s/it] 47%|████▋     | 590/1260 [3:34:56<4:01:09, 21.60s/it]                                                      {'loss': 0.1292, 'grad_norm': 1.4152437721224567, 'learning_rate': 1.9262222339360684e-05, 'epoch': 9.37}
 47%|████▋     | 590/1260 [3:34:56<4:01:09, 21.60s/it] 47%|████▋     | 591/1260 [3:35:18<4:00:57, 21.61s/it] 47%|████▋     | 592/1260 [3:35:40<4:00:08, 21.57s/it] 47%|████▋     | 593/1260 [3:36:01<4:00:32, 21.64s/it] 47%|████▋     | 594/1260 [3:36:23<4:00:33, 21.67s/it] 47%|████▋     | 595/1260 [3:36:45<4:01:41, 21.81s/it]                                                      {'loss': 0.1259, 'grad_norm': 1.41819788582697, 'learning_rate': 1.906260702214508e-05, 'epoch': 9.45}
 47%|████▋     | 595/1260 [3:36:45<4:01:41, 21.81s/it] 47%|████▋     | 596/1260 [3:37:07<4:00:08, 21.70s/it] 47%|████▋     | 597/1260 [3:37:28<4:00:19, 21.75s/it] 47%|████▋     | 598/1260 [3:37:50<3:58:47, 21.64s/it] 48%|████▊     | 599/1260 [3:38:11<3:58:01, 21.61s/it] 48%|████▊     | 600/1260 [3:38:33<3:58:01, 21.64s/it]                                                      {'loss': 0.1284, 'grad_norm': 1.5882951641246916, 'learning_rate': 1.8862212213192718e-05, 'epoch': 9.52}
 48%|████▊     | 600/1260 [3:38:33<3:58:01, 21.64s/it] 48%|████▊     | 601/1260 [3:38:55<3:58:07, 21.68s/it] 48%|████▊     | 602/1260 [3:39:17<3:58:36, 21.76s/it] 48%|████▊     | 603/1260 [3:39:38<3:57:23, 21.68s/it] 48%|████▊     | 604/1260 [3:40:00<3:56:28, 21.63s/it] 48%|████▊     | 605/1260 [3:40:22<3:56:50, 21.70s/it]                                                      {'loss': 0.1293, 'grad_norm': 1.4215088833755312, 'learning_rate': 1.866107636222242e-05, 'epoch': 9.6}
 48%|████▊     | 605/1260 [3:40:22<3:56:50, 21.70s/it] 48%|████▊     | 606/1260 [3:40:43<3:56:28, 21.70s/it] 48%|████▊     | 607/1260 [3:41:05<3:56:03, 21.69s/it] 48%|████▊     | 608/1260 [3:41:27<3:55:05, 21.63s/it] 48%|████▊     | 609/1260 [3:41:48<3:54:27, 21.61s/it] 48%|████▊     | 610/1260 [3:42:10<3:54:22, 21.63s/it]                                                      {'loss': 0.1317, 'grad_norm': 2.3559131648356657, 'learning_rate': 1.8459238061136604e-05, 'epoch': 9.68}
 48%|████▊     | 610/1260 [3:42:10<3:54:22, 21.63s/it] 48%|████▊     | 611/1260 [3:42:32<3:55:27, 21.77s/it] 49%|████▊     | 612/1260 [3:42:53<3:54:22, 21.70s/it] 49%|████▊     | 613/1260 [3:43:15<3:54:02, 21.70s/it] 49%|████▊     | 614/1260 [3:43:37<3:53:34, 21.69s/it] 49%|████▉     | 615/1260 [3:43:58<3:52:25, 21.62s/it]                                                      {'loss': 0.1332, 'grad_norm': 1.19192553595736, 'learning_rate': 1.82567360366167e-05, 'epoch': 9.76}
 49%|████▉     | 615/1260 [3:43:58<3:52:25, 21.62s/it] 49%|████▉     | 616/1260 [3:44:20<3:52:25, 21.66s/it] 49%|████▉     | 617/1260 [3:44:42<3:52:26, 21.69s/it] 49%|████▉     | 618/1260 [3:45:04<3:52:59, 21.77s/it] 49%|████▉     | 619/1260 [3:45:25<3:51:31, 21.67s/it] 49%|████▉     | 620/1260 [3:45:47<3:50:24, 21.60s/it]                                                      {'loss': 0.1294, 'grad_norm': 1.757998001262446, 'learning_rate': 1.8053609142692608e-05, 'epoch': 9.84}
 49%|████▉     | 620/1260 [3:45:47<3:50:24, 21.60s/it] 49%|████▉     | 621/1260 [3:46:08<3:49:50, 21.58s/it] 49%|████▉     | 622/1260 [3:46:30<3:49:31, 21.59s/it] 49%|████▉     | 623/1260 [3:46:51<3:49:25, 21.61s/it] 50%|████▉     | 624/1260 [3:47:13<3:49:00, 21.60s/it] 50%|████▉     | 625/1260 [3:47:35<3:48:37, 21.60s/it]                                                      {'loss': 0.1297, 'grad_norm': 1.557549917675271, 'learning_rate': 1.7849896353287853e-05, 'epoch': 9.92}
 50%|████▉     | 625/1260 [3:47:35<3:48:37, 21.60s/it] 50%|████▉     | 626/1260 [3:47:56<3:47:43, 21.55s/it] 50%|████▉     | 627/1260 [3:48:18<3:47:14, 21.54s/it] 50%|████▉     | 628/1260 [3:48:39<3:47:38, 21.61s/it] 50%|████▉     | 629/1260 [3:49:01<3:46:48, 21.57s/it] 50%|█████     | 630/1260 [3:49:20<3:40:13, 20.97s/it]                                                      {'loss': 0.1273, 'grad_norm': 1.336592473897361, 'learning_rate': 1.7645636754741604e-05, 'epoch': 10.0}
 50%|█████     | 630/1260 [3:49:20<3:40:13, 20.97s/it][INFO|trainer.py:3993] 2025-07-05 01:19:42,143 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630
[INFO|configuration_utils.py:424] 2025-07-05 01:19:42,149 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/config.json
[INFO|configuration_utils.py:904] 2025-07-05 01:19:42,150 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 01:19:45,156 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:19:45,157 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:19:45,158 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:19:45,159 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/special_tokens_map.json
[2025-07-05 01:19:45,354] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step628 is about to be saved!
[2025-07-05 01:19:45,365] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 01:19:45,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 01:19:45,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 01:19:45,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 01:19:50,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 01:19:50,399] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/global_step628/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 01:19:53,895] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step628 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 01:19:53,904 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:19:53,905 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:19:53,905 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:19:53,907 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 01:19:54,062 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 01:19:54,467 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/chat_template.jinja
 50%|█████     | 631/1260 [3:49:58<4:31:12, 25.87s/it] 50%|█████     | 632/1260 [3:50:19<4:17:45, 24.63s/it] 50%|█████     | 633/1260 [3:50:41<4:08:10, 23.75s/it] 50%|█████     | 634/1260 [3:51:03<4:00:54, 23.09s/it] 50%|█████     | 635/1260 [3:51:24<3:55:39, 22.62s/it]                                                      {'loss': 0.0927, 'grad_norm': 1.0656364066514443, 'learning_rate': 1.744086953830922e-05, 'epoch': 10.08}
 50%|█████     | 635/1260 [3:51:24<3:55:39, 22.62s/it] 50%|█████     | 636/1260 [3:51:46<3:51:45, 22.29s/it] 51%|█████     | 637/1260 [3:52:07<3:49:46, 22.13s/it] 51%|█████     | 638/1260 [3:52:29<3:48:03, 22.00s/it] 51%|█████     | 639/1260 [3:52:51<3:46:01, 21.84s/it] 51%|█████     | 640/1260 [3:53:12<3:44:21, 21.71s/it]                                                      {'loss': 0.0936, 'grad_norm': 2.5200294165343977, 'learning_rate': 1.7235633992642615e-05, 'epoch': 10.16}
 51%|█████     | 640/1260 [3:53:12<3:44:21, 21.71s/it] 51%|█████     | 641/1260 [3:53:34<3:45:42, 21.88s/it] 51%|█████     | 642/1260 [3:53:56<3:45:14, 21.87s/it] 51%|█████     | 643/1260 [3:54:18<3:44:34, 21.84s/it] 51%|█████     | 644/1260 [3:54:39<3:43:06, 21.73s/it] 51%|█████     | 645/1260 [3:55:01<3:41:44, 21.63s/it]                                                      {'loss': 0.0974, 'grad_norm': 1.7480534162564885, 'learning_rate': 1.702996949625197e-05, 'epoch': 10.24}
 51%|█████     | 645/1260 [3:55:01<3:41:44, 21.63s/it] 51%|█████▏    | 646/1260 [3:55:22<3:40:54, 21.59s/it] 51%|█████▏    | 647/1260 [3:55:44<3:41:46, 21.71s/it] 51%|█████▏    | 648/1260 [3:56:06<3:41:06, 21.68s/it] 52%|█████▏    | 649/1260 [3:56:27<3:40:22, 21.64s/it] 52%|█████▏    | 650/1260 [3:56:49<3:40:35, 21.70s/it]                                                      {'loss': 0.0988, 'grad_norm': 1.781575106217159, 'learning_rate': 1.682391550995014e-05, 'epoch': 10.32}
 52%|█████▏    | 650/1260 [3:56:49<3:40:35, 21.70s/it] 52%|█████▏    | 651/1260 [3:57:11<3:39:59, 21.67s/it] 52%|█████▏    | 652/1260 [3:57:32<3:39:21, 21.65s/it] 52%|█████▏    | 653/1260 [3:57:54<3:39:53, 21.74s/it] 52%|█████▏    | 654/1260 [3:58:16<3:39:09, 21.70s/it] 52%|█████▏    | 655/1260 [3:58:38<3:38:58, 21.72s/it]                                                      {'loss': 0.0934, 'grad_norm': 1.1862292458298933, 'learning_rate': 1.6617511569281382e-05, 'epoch': 10.4}
 52%|█████▏    | 655/1260 [3:58:38<3:38:58, 21.72s/it] 52%|█████▏    | 656/1260 [3:59:00<3:39:07, 21.77s/it] 52%|█████▏    | 657/1260 [3:59:21<3:38:05, 21.70s/it] 52%|█████▏    | 658/1260 [3:59:43<3:37:51, 21.71s/it] 52%|█████▏    | 659/1260 [4:00:04<3:36:41, 21.63s/it] 52%|█████▏    | 660/1260 [4:00:26<3:37:20, 21.73s/it]                                                      {'loss': 0.0945, 'grad_norm': 1.507416388994345, 'learning_rate': 1.641079727693561e-05, 'epoch': 10.48}
 52%|█████▏    | 660/1260 [4:00:26<3:37:20, 21.73s/it] 52%|█████▏    | 661/1260 [4:00:48<3:36:47, 21.72s/it] 53%|█████▎    | 662/1260 [4:01:10<3:36:25, 21.72s/it] 53%|█████▎    | 663/1260 [4:01:31<3:35:37, 21.67s/it] 53%|█████▎    | 664/1260 [4:01:53<3:34:57, 21.64s/it] 53%|█████▎    | 665/1260 [4:02:15<3:34:39, 21.65s/it]                                                      {'loss': 0.0983, 'grad_norm': 1.1453901411531784, 'learning_rate': 1.6203812295149876e-05, 'epoch': 10.56}
 53%|█████▎    | 665/1260 [4:02:15<3:34:39, 21.65s/it] 53%|█████▎    | 666/1260 [4:02:36<3:34:12, 21.64s/it] 53%|█████▎    | 667/1260 [4:02:58<3:33:34, 21.61s/it] 53%|█████▎    | 668/1260 [4:03:19<3:32:54, 21.58s/it] 53%|█████▎    | 669/1260 [4:03:41<3:32:44, 21.60s/it] 53%|█████▎    | 670/1260 [4:04:02<3:32:15, 21.59s/it]                                                      {'loss': 0.1005, 'grad_norm': 2.040054224176996, 'learning_rate': 1.5996596338098365e-05, 'epoch': 10.64}
 53%|█████▎    | 670/1260 [4:04:02<3:32:15, 21.59s/it] 53%|█████▎    | 671/1260 [4:04:24<3:32:44, 21.67s/it] 53%|█████▎    | 672/1260 [4:04:46<3:33:14, 21.76s/it] 53%|█████▎    | 673/1260 [4:05:08<3:32:28, 21.72s/it] 53%|█████▎    | 674/1260 [4:05:29<3:31:22, 21.64s/it] 54%|█████▎    | 675/1260 [4:05:51<3:31:01, 21.64s/it]                                                      {'loss': 0.1035, 'grad_norm': 1.5341260353981976, 'learning_rate': 1.5789189164272456e-05, 'epoch': 10.72}
 54%|█████▎    | 675/1260 [4:05:51<3:31:01, 21.64s/it] 54%|█████▎    | 676/1260 [4:06:13<3:30:20, 21.61s/it] 54%|█████▎    | 677/1260 [4:06:34<3:29:28, 21.56s/it] 54%|█████▍    | 678/1260 [4:06:56<3:29:20, 21.58s/it] 54%|█████▍    | 679/1260 [4:07:17<3:29:24, 21.63s/it] 54%|█████▍    | 680/1260 [4:07:39<3:30:12, 21.75s/it]                                                      {'loss': 0.104, 'grad_norm': 1.3566748789426168, 'learning_rate': 1.5581630568852252e-05, 'epoch': 10.8}
 54%|█████▍    | 680/1260 [4:07:39<3:30:12, 21.75s/it] 54%|█████▍    | 681/1260 [4:08:01<3:29:49, 21.74s/it] 54%|█████▍    | 682/1260 [4:08:23<3:29:35, 21.76s/it] 54%|█████▍    | 683/1260 [4:08:44<3:28:29, 21.68s/it] 54%|█████▍    | 684/1260 [4:09:06<3:27:27, 21.61s/it] 54%|█████▍    | 685/1260 [4:09:27<3:26:56, 21.59s/it]                                                      {'loss': 0.1022, 'grad_norm': 1.2303651146822077, 'learning_rate': 1.5373960376071095e-05, 'epoch': 10.87}
 54%|█████▍    | 685/1260 [4:09:27<3:26:56, 21.59s/it] 54%|█████▍    | 686/1260 [4:09:49<3:26:40, 21.60s/it] 55%|█████▍    | 687/1260 [4:10:10<3:25:58, 21.57s/it] 55%|█████▍    | 688/1260 [4:10:32<3:26:02, 21.61s/it] 55%|█████▍    | 689/1260 [4:10:54<3:25:32, 21.60s/it] 55%|█████▍    | 690/1260 [4:11:15<3:25:09, 21.60s/it]                                                      {'loss': 0.1006, 'grad_norm': 1.29554787913504, 'learning_rate': 1.516621843157449e-05, 'epoch': 10.95}
 55%|█████▍    | 690/1260 [4:11:15<3:25:09, 21.60s/it] 55%|█████▍    | 691/1260 [4:11:37<3:24:16, 21.54s/it] 55%|█████▍    | 692/1260 [4:11:58<3:23:33, 21.50s/it] 55%|█████▌    | 693/1260 [4:12:18<3:17:22, 20.89s/it][INFO|trainer.py:3993] 2025-07-05 01:42:39,661 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693
[INFO|configuration_utils.py:424] 2025-07-05 01:42:39,667 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/config.json
[INFO|configuration_utils.py:904] 2025-07-05 01:42:39,669 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 01:42:42,464 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:42:42,466 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:42:42,467 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:42:42,469 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/special_tokens_map.json
[2025-07-05 01:42:42,658] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step691 is about to be saved!
[2025-07-05 01:42:42,669] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 01:42:42,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 01:42:42,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 01:42:42,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 01:42:47,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 01:42:47,615] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/global_step691/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 01:42:50,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 01:42:50,839 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 01:42:50,840 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 01:42:50,840 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 01:42:50,841 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 01:42:51,000 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 01:42:51,410 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-693/chat_template.jinja
 55%|█████▌    | 694/1260 [4:12:55<4:02:41, 25.73s/it] 55%|█████▌    | 695/1260 [4:13:16<3:50:08, 24.44s/it]                                                      {'loss': 0.0902, 'grad_norm': 1.6103532568358625, 'learning_rate': 1.495844459477494e-05, 'epoch': 11.03}
 55%|█████▌    | 695/1260 [4:13:16<3:50:08, 24.44s/it] 55%|█████▌    | 696/1260 [4:13:38<3:41:34, 23.57s/it] 55%|█████▌    | 697/1260 [4:13:59<3:35:46, 23.00s/it] 55%|█████▌    | 698/1260 [4:14:21<3:31:04, 22.53s/it] 55%|█████▌    | 699/1260 [4:14:42<3:27:53, 22.23s/it] 56%|█████▌    | 700/1260 [4:15:04<3:25:59, 22.07s/it]                                                      {'loss': 0.0716, 'grad_norm': 2.6687627165619197, 'learning_rate': 1.4750678731204108e-05, 'epoch': 11.11}
 56%|█████▌    | 700/1260 [4:15:04<3:25:59, 22.07s/it] 56%|█████▌    | 701/1260 [4:15:26<3:24:14, 21.92s/it] 56%|█████▌    | 702/1260 [4:15:47<3:23:46, 21.91s/it] 56%|█████▌    | 703/1260 [4:16:09<3:22:44, 21.84s/it] 56%|█████▌    | 704/1260 [4:16:31<3:21:24, 21.74s/it] 56%|█████▌    | 705/1260 [4:16:52<3:20:12, 21.64s/it]                                                      {'loss': 0.0751, 'grad_norm': 2.6620426314718806, 'learning_rate': 1.4542960704863842e-05, 'epoch': 11.19}
 56%|█████▌    | 705/1260 [4:16:52<3:20:12, 21.64s/it] 56%|█████▌    | 706/1260 [4:17:14<3:19:53, 21.65s/it] 56%|█████▌    | 707/1260 [4:17:35<3:19:38, 21.66s/it] 56%|█████▌    | 708/1260 [4:17:57<3:19:34, 21.69s/it] 56%|█████▋    | 709/1260 [4:18:19<3:19:37, 21.74s/it] 56%|█████▋    | 710/1260 [4:18:41<3:19:37, 21.78s/it]                                                      {'loss': 0.0726, 'grad_norm': 1.491842400690096, 'learning_rate': 1.4335330370577475e-05, 'epoch': 11.27}
 56%|█████▋    | 710/1260 [4:18:41<3:19:37, 21.78s/it] 56%|█████▋    | 711/1260 [4:19:03<3:19:04, 21.76s/it] 57%|█████▋    | 712/1260 [4:19:24<3:17:48, 21.66s/it] 57%|█████▋    | 713/1260 [4:19:46<3:17:08, 21.62s/it] 57%|█████▋    | 714/1260 [4:20:07<3:16:25, 21.59s/it] 57%|█████▋    | 715/1260 [4:20:29<3:16:46, 21.66s/it]                                                      {'loss': 0.0741, 'grad_norm': 2.104371352958873, 'learning_rate': 1.4127827566342864e-05, 'epoch': 11.35}
 57%|█████▋    | 715/1260 [4:20:29<3:16:46, 21.66s/it] 57%|█████▋    | 716/1260 [4:20:51<3:16:41, 21.69s/it] 57%|█████▋    | 717/1260 [4:21:12<3:16:32, 21.72s/it] 57%|█████▋    | 718/1260 [4:21:35<3:17:18, 21.84s/it] 57%|█████▋    | 719/1260 [4:21:56<3:15:47, 21.71s/it] 57%|█████▋    | 720/1260 [4:22:17<3:14:38, 21.63s/it]                                                      {'loss': 0.071, 'grad_norm': 1.5854508440894917, 'learning_rate': 1.3920492105688703e-05, 'epoch': 11.43}
 57%|█████▋    | 720/1260 [4:22:17<3:14:38, 21.63s/it] 57%|█████▋    | 721/1260 [4:22:39<3:14:25, 21.64s/it] 57%|█████▋    | 722/1260 [4:23:01<3:14:33, 21.70s/it] 57%|█████▋    | 723/1260 [4:23:23<3:14:30, 21.73s/it] 57%|█████▋    | 724/1260 [4:23:44<3:14:03, 21.72s/it] 58%|█████▊    | 725/1260 [4:24:06<3:14:34, 21.82s/it]                                                      {'loss': 0.068, 'grad_norm': 1.5465822956350492, 'learning_rate': 1.371336377003551e-05, 'epoch': 11.51}
 58%|█████▊    | 725/1260 [4:24:06<3:14:34, 21.82s/it] 58%|█████▊    | 726/1260 [4:24:28<3:13:07, 21.70s/it] 58%|█████▊    | 727/1260 [4:24:49<3:12:02, 21.62s/it] 58%|█████▊    | 728/1260 [4:25:11<3:11:53, 21.64s/it] 58%|█████▊    | 729/1260 [4:25:32<3:11:04, 21.59s/it] 58%|█████▊    | 730/1260 [4:25:54<3:11:41, 21.70s/it]                                                      {'loss': 0.0698, 'grad_norm': 1.8710597905345436, 'learning_rate': 1.3506482301062753e-05, 'epoch': 11.59}
 58%|█████▊    | 730/1260 [4:25:54<3:11:41, 21.70s/it] 58%|█████▊    | 731/1260 [4:26:16<3:11:28, 21.72s/it] 58%|█████▊    | 732/1260 [4:26:38<3:11:06, 21.72s/it] 58%|█████▊    | 733/1260 [4:27:00<3:10:29, 21.69s/it] 58%|█████▊    | 734/1260 [4:27:21<3:09:26, 21.61s/it] 58%|█████▊    | 735/1260 [4:27:43<3:09:13, 21.63s/it]                                                      {'loss': 0.0697, 'grad_norm': 2.0391528021090877, 'learning_rate': 1.3299887393083629e-05, 'epoch': 11.67}
 58%|█████▊    | 735/1260 [4:27:43<3:09:13, 21.63s/it] 58%|█████▊    | 736/1260 [4:28:04<3:08:26, 21.58s/it] 58%|█████▊    | 737/1260 [4:28:26<3:07:43, 21.54s/it] 59%|█████▊    | 738/1260 [4:28:47<3:07:07, 21.51s/it] 59%|█████▊    | 739/1260 [4:29:09<3:06:52, 21.52s/it] 59%|█████▊    | 740/1260 [4:29:30<3:07:06, 21.59s/it]                                                      {'loss': 0.0688, 'grad_norm': 1.8513149944987664, 'learning_rate': 1.309361868542893e-05, 'epoch': 11.75}
 59%|█████▊    | 740/1260 [4:29:30<3:07:06, 21.59s/it] 59%|█████▉    | 741/1260 [4:29:52<3:07:06, 21.63s/it] 59%|█████▉    | 742/1260 [4:30:14<3:07:36, 21.73s/it] 59%|█████▉    | 743/1260 [4:30:36<3:07:05, 21.71s/it] 59%|█████▉    | 744/1260 [4:30:57<3:06:46, 21.72s/it] 59%|█████▉    | 745/1260 [4:31:19<3:06:10, 21.69s/it]                                                      {'loss': 0.0684, 'grad_norm': 1.5092787996743686, 'learning_rate': 1.288771575484145e-05, 'epoch': 11.83}
 59%|█████▉    | 745/1260 [4:31:19<3:06:10, 21.69s/it] 59%|█████▉    | 746/1260 [4:31:41<3:05:49, 21.69s/it] 59%|█████▉    | 747/1260 [4:32:02<3:04:49, 21.62s/it] 59%|█████▉    | 748/1260 [4:32:24<3:04:08, 21.58s/it] 59%|█████▉    | 749/1260 [4:32:45<3:04:05, 21.62s/it] 60%|█████▉    | 750/1260 [4:33:07<3:03:17, 21.56s/it]                                                      {'loss': 0.0683, 'grad_norm': 2.4600368242536836, 'learning_rate': 1.2682218107882393e-05, 'epoch': 11.91}
 60%|█████▉    | 750/1260 [4:33:07<3:03:17, 21.56s/it] 60%|█████▉    | 751/1260 [4:33:28<3:02:33, 21.52s/it] 60%|█████▉    | 752/1260 [4:33:50<3:02:03, 21.50s/it] 60%|█████▉    | 753/1260 [4:34:11<3:02:08, 21.56s/it] 60%|█████▉    | 754/1260 [4:34:33<3:01:29, 21.52s/it] 60%|█████▉    | 755/1260 [4:34:54<3:00:54, 21.49s/it]                                                      {'loss': 0.0664, 'grad_norm': 1.3132259342690549, 'learning_rate': 1.2477165173351256e-05, 'epoch': 11.99}
 60%|█████▉    | 755/1260 [4:34:54<3:00:54, 21.49s/it] 60%|██████    | 756/1260 [4:35:14<2:55:58, 20.95s/it][INFO|trainer.py:3993] 2025-07-05 02:05:35,776 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756
[INFO|configuration_utils.py:424] 2025-07-05 02:05:35,782 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:05:35,783 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:05:38,952 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:05:38,953 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:05:38,954 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:05:38,955 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/special_tokens_map.json
[2025-07-05 02:05:39,143] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step754 is about to be saved!
[2025-07-05 02:05:39,154] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:05:39,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:05:39,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:05:39,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:05:44,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:05:44,415] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/global_step754/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:05:47,289] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step754 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:05:47,298 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:05:47,299 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:05:47,299 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:05:47,299 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:05:47,460 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:05:47,865 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-756/chat_template.jinja
 60%|██████    | 757/1260 [4:35:52<3:37:37, 25.96s/it] 60%|██████    | 758/1260 [4:36:13<3:26:06, 24.63s/it] 60%|██████    | 759/1260 [4:36:35<3:17:57, 23.71s/it] 60%|██████    | 760/1260 [4:36:56<3:12:22, 23.08s/it]                                                      {'loss': 0.0471, 'grad_norm': 1.5278286917951267, 'learning_rate': 1.227259629472064e-05, 'epoch': 12.06}
 60%|██████    | 760/1260 [4:36:56<3:12:22, 23.08s/it] 60%|██████    | 761/1260 [4:37:18<3:08:13, 22.63s/it] 60%|██████    | 762/1260 [4:37:40<3:05:36, 22.36s/it] 61%|██████    | 763/1260 [4:38:01<3:03:19, 22.13s/it] 61%|██████    | 764/1260 [4:38:23<3:01:54, 22.01s/it] 61%|██████    | 765/1260 [4:38:44<3:00:07, 21.83s/it]                                                      {'loss': 0.0458, 'grad_norm': 1.7182360625180646, 'learning_rate': 1.206855072258742e-05, 'epoch': 12.14}
 61%|██████    | 765/1260 [4:38:44<3:00:07, 21.83s/it] 61%|██████    | 766/1260 [4:39:06<2:58:52, 21.73s/it] 61%|██████    | 767/1260 [4:39:28<2:58:44, 21.75s/it] 61%|██████    | 768/1260 [4:39:49<2:58:19, 21.75s/it] 61%|██████    | 769/1260 [4:40:11<2:57:09, 21.65s/it] 61%|██████    | 770/1260 [4:40:32<2:56:19, 21.59s/it]                                                      {'loss': 0.0469, 'grad_norm': 1.3417405447571622, 'learning_rate': 1.1865067607141743e-05, 'epoch': 12.22}
 61%|██████    | 770/1260 [4:40:32<2:56:19, 21.59s/it] 61%|██████    | 771/1260 [4:40:54<2:55:58, 21.59s/it] 61%|██████▏   | 772/1260 [4:41:15<2:55:29, 21.58s/it] 61%|██████▏   | 773/1260 [4:41:37<2:55:11, 21.58s/it] 61%|██████▏   | 774/1260 [4:41:59<2:55:03, 21.61s/it] 62%|██████▏   | 775/1260 [4:42:21<2:55:32, 21.72s/it]                                                      {'loss': 0.0469, 'grad_norm': 1.224675169775239, 'learning_rate': 1.1662185990655285e-05, 'epoch': 12.3}
 62%|██████▏   | 775/1260 [4:42:21<2:55:32, 21.72s/it] 62%|██████▏   | 776/1260 [4:42:42<2:54:29, 21.63s/it] 62%|██████▏   | 777/1260 [4:43:04<2:54:12, 21.64s/it] 62%|██████▏   | 778/1260 [4:43:25<2:53:24, 21.59s/it] 62%|██████▏   | 779/1260 [4:43:46<2:52:32, 21.52s/it] 62%|██████▏   | 780/1260 [4:44:08<2:53:11, 21.65s/it]                                                      {'loss': 0.0492, 'grad_norm': 2.5505675579248654, 'learning_rate': 1.1459944799990203e-05, 'epoch': 12.38}
 62%|██████▏   | 780/1260 [4:44:08<2:53:11, 21.65s/it] 62%|██████▏   | 781/1260 [4:44:30<2:52:15, 21.58s/it] 62%|██████▏   | 782/1260 [4:44:51<2:51:29, 21.53s/it] 62%|██████▏   | 783/1260 [4:45:13<2:51:48, 21.61s/it] 62%|██████▏   | 784/1260 [4:45:34<2:50:57, 21.55s/it] 62%|██████▏   | 785/1260 [4:45:56<2:50:42, 21.56s/it]                                                      {'loss': 0.0454, 'grad_norm': 2.1140522428118595, 'learning_rate': 1.1258382839130282e-05, 'epoch': 12.46}
 62%|██████▏   | 785/1260 [4:45:56<2:50:42, 21.56s/it] 62%|██████▏   | 786/1260 [4:46:17<2:49:58, 21.52s/it] 62%|██████▏   | 787/1260 [4:46:39<2:49:34, 21.51s/it] 63%|██████▎   | 788/1260 [4:47:01<2:49:36, 21.56s/it] 63%|██████▎   | 789/1260 [4:47:22<2:49:06, 21.54s/it] 63%|██████▎   | 790/1260 [4:47:44<2:49:03, 21.58s/it]                                                      {'loss': 0.0427, 'grad_norm': 0.9204715836679864, 'learning_rate': 1.1057538781735571e-05, 'epoch': 12.54}
 63%|██████▎   | 790/1260 [4:47:44<2:49:03, 21.58s/it] 63%|██████▎   | 791/1260 [4:48:05<2:48:17, 21.53s/it] 63%|██████▎   | 792/1260 [4:48:27<2:47:37, 21.49s/it] 63%|██████▎   | 793/1260 [4:48:48<2:47:17, 21.49s/it] 63%|██████▎   | 794/1260 [4:49:10<2:48:14, 21.66s/it] 63%|██████▎   | 795/1260 [4:49:32<2:48:28, 21.74s/it]                                                      {'loss': 0.0422, 'grad_norm': 1.3248907642711685, 'learning_rate': 1.0857451163722119e-05, 'epoch': 12.62}
 63%|██████▎   | 795/1260 [4:49:32<2:48:28, 21.74s/it] 63%|██████▎   | 796/1260 [4:49:54<2:47:39, 21.68s/it] 63%|██████▎   | 797/1260 [4:50:15<2:46:54, 21.63s/it] 63%|██████▎   | 798/1260 [4:50:37<2:45:57, 21.55s/it] 63%|██████▎   | 799/1260 [4:50:58<2:45:43, 21.57s/it] 63%|██████▎   | 800/1260 [4:51:20<2:45:36, 21.60s/it]                                                      {'loss': 0.0471, 'grad_norm': 1.9286520732427752, 'learning_rate': 1.0658158375868056e-05, 'epoch': 12.7}
 63%|██████▎   | 800/1260 [4:51:20<2:45:36, 21.60s/it] 64%|██████▎   | 801/1260 [4:51:42<2:45:35, 21.65s/it] 64%|██████▎   | 802/1260 [4:52:03<2:44:49, 21.59s/it] 64%|██████▎   | 803/1260 [4:52:25<2:44:38, 21.62s/it] 64%|██████▍   | 804/1260 [4:52:46<2:44:15, 21.61s/it] 64%|██████▍   | 805/1260 [4:53:08<2:44:11, 21.65s/it]                                                      {'loss': 0.045, 'grad_norm': 1.1198491426709223, 'learning_rate': 1.0459698656447612e-05, 'epoch': 12.78}
 64%|██████▍   | 805/1260 [4:53:08<2:44:11, 21.65s/it] 64%|██████▍   | 806/1260 [4:53:30<2:43:39, 21.63s/it] 64%|██████▍   | 807/1260 [4:53:51<2:42:57, 21.58s/it] 64%|██████▍   | 808/1260 [4:54:13<2:42:45, 21.60s/it] 64%|██████▍   | 809/1260 [4:54:34<2:41:56, 21.54s/it] 64%|██████▍   | 810/1260 [4:54:56<2:41:31, 21.54s/it]                                                      {'loss': 0.0456, 'grad_norm': 1.2282612459719662, 'learning_rate': 1.0262110083894285e-05, 'epoch': 12.86}
 64%|██████▍   | 810/1260 [4:54:56<2:41:31, 21.54s/it] 64%|██████▍   | 811/1260 [4:55:17<2:40:57, 21.51s/it] 64%|██████▍   | 812/1260 [4:55:39<2:40:20, 21.47s/it] 65%|██████▍   | 813/1260 [4:56:00<2:40:08, 21.50s/it] 65%|██████▍   | 814/1260 [4:56:22<2:40:07, 21.54s/it] 65%|██████▍   | 815/1260 [4:56:43<2:40:01, 21.58s/it]                                                      {'loss': 0.0462, 'grad_norm': 1.380028348830987, 'learning_rate': 1.0065430569494785e-05, 'epoch': 12.94}
 65%|██████▍   | 815/1260 [4:56:43<2:40:01, 21.58s/it] 65%|██████▍   | 816/1260 [4:57:05<2:39:51, 21.60s/it] 65%|██████▍   | 817/1260 [4:57:27<2:40:05, 21.68s/it] 65%|██████▍   | 818/1260 [4:57:48<2:39:10, 21.61s/it] 65%|██████▌   | 819/1260 [4:58:08<2:34:03, 20.96s/it][INFO|trainer.py:3993] 2025-07-05 02:28:29,834 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819
[INFO|configuration_utils.py:424] 2025-07-05 02:28:29,840 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:28:29,841 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:28:32,992 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:28:32,994 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:28:32,995 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:28:32,996 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/special_tokens_map.json
[2025-07-05 02:28:33,193] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step817 is about to be saved!
[2025-07-05 02:28:33,203] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:28:33,204] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:28:33,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:28:33,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:28:37,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:28:37,981] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/global_step817/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:28:41,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step817 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:28:41,360 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:28:41,361 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:28:41,361 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:28:41,362 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:28:41,530 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:28:41,956 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-819/chat_template.jinja
 65%|██████▌   | 820/1260 [4:58:45<3:10:10, 25.93s/it]                                                      {'loss': 0.0428, 'grad_norm': 1.1597148996955415, 'learning_rate': 9.86969785011497e-06, 'epoch': 13.02}
 65%|██████▌   | 820/1260 [4:58:45<3:10:10, 25.93s/it] 65%|██████▌   | 821/1260 [4:59:07<3:00:11, 24.63s/it] 65%|██████▌   | 822/1260 [4:59:29<2:54:07, 23.85s/it] 65%|██████▌   | 823/1260 [4:59:50<2:48:39, 23.16s/it] 65%|██████▌   | 824/1260 [5:00:12<2:44:36, 22.65s/it] 65%|██████▌   | 825/1260 [5:00:33<2:41:33, 22.28s/it]                                                      {'loss': 0.0342, 'grad_norm': 1.7960268888623052, 'learning_rate': 9.67494948095931e-06, 'epoch': 13.1}
 65%|██████▌   | 825/1260 [5:00:33<2:41:33, 22.28s/it] 66%|██████▌   | 826/1260 [5:00:55<2:39:32, 22.06s/it] 66%|██████▌   | 827/1260 [5:01:16<2:37:50, 21.87s/it] 66%|██████▌   | 828/1260 [5:01:38<2:36:51, 21.78s/it] 66%|██████▌   | 829/1260 [5:02:00<2:37:17, 21.90s/it] 66%|██████▌   | 830/1260 [5:02:22<2:36:09, 21.79s/it]                                                      {'loss': 0.0341, 'grad_norm': 1.0668817793657825, 'learning_rate': 9.481222828365151e-06, 'epoch': 13.17}
 66%|██████▌   | 830/1260 [5:02:22<2:36:09, 21.79s/it] 66%|██████▌   | 831/1260 [5:02:43<2:35:04, 21.69s/it] 66%|██████▌   | 832/1260 [5:03:05<2:34:28, 21.66s/it] 66%|██████▌   | 833/1260 [5:03:26<2:33:42, 21.60s/it] 66%|██████▌   | 834/1260 [5:03:48<2:33:41, 21.65s/it] 66%|██████▋   | 835/1260 [5:04:09<2:32:54, 21.59s/it]                                                      {'loss': 0.0324, 'grad_norm': 1.0585093661366023, 'learning_rate': 9.288555062633258e-06, 'epoch': 13.25}
 66%|██████▋   | 835/1260 [5:04:09<2:32:54, 21.59s/it] 66%|██████▋   | 836/1260 [5:04:31<2:33:07, 21.67s/it] 66%|██████▋   | 837/1260 [5:04:53<2:32:15, 21.60s/it] 67%|██████▋   | 838/1260 [5:05:14<2:32:23, 21.67s/it] 67%|██████▋   | 839/1260 [5:05:36<2:31:49, 21.64s/it] 67%|██████▋   | 840/1260 [5:05:58<2:31:34, 21.65s/it]                                                      {'loss': 0.0301, 'grad_norm': 1.4201043415362555, 'learning_rate': 9.096983150895936e-06, 'epoch': 13.33}
 67%|██████▋   | 840/1260 [5:05:58<2:31:34, 21.65s/it] 67%|██████▋   | 841/1260 [5:06:20<2:32:33, 21.85s/it] 67%|██████▋   | 842/1260 [5:06:42<2:31:51, 21.80s/it] 67%|██████▋   | 843/1260 [5:07:04<2:31:44, 21.83s/it] 67%|██████▋   | 844/1260 [5:07:25<2:30:59, 21.78s/it] 67%|██████▋   | 845/1260 [5:07:47<2:30:22, 21.74s/it]                                                      {'loss': 0.0308, 'grad_norm': 0.9407833744735417, 'learning_rate': 8.906543850024186e-06, 'epoch': 13.41}
 67%|██████▋   | 845/1260 [5:07:47<2:30:22, 21.74s/it] 67%|██████▋   | 846/1260 [5:08:09<2:29:45, 21.70s/it] 67%|██████▋   | 847/1260 [5:08:30<2:29:04, 21.66s/it] 67%|██████▋   | 848/1260 [5:08:52<2:28:20, 21.60s/it] 67%|██████▋   | 849/1260 [5:09:13<2:28:20, 21.66s/it] 67%|██████▋   | 850/1260 [5:09:35<2:27:34, 21.60s/it]                                                      {'loss': 0.0317, 'grad_norm': 1.4556290511819818, 'learning_rate': 8.71727369957513e-06, 'epoch': 13.49}
 67%|██████▋   | 850/1260 [5:09:35<2:27:34, 21.60s/it] 68%|██████▊   | 851/1260 [5:09:56<2:27:00, 21.57s/it] 68%|██████▊   | 852/1260 [5:10:18<2:26:45, 21.58s/it] 68%|██████▊   | 853/1260 [5:10:39<2:26:10, 21.55s/it] 68%|██████▊   | 854/1260 [5:11:01<2:26:10, 21.60s/it] 68%|██████▊   | 855/1260 [5:11:23<2:25:33, 21.56s/it]                                                      {'loss': 0.0326, 'grad_norm': 2.2463082831763344, 'learning_rate': 8.529209014781202e-06, 'epoch': 13.57}
 68%|██████▊   | 855/1260 [5:11:23<2:25:33, 21.56s/it] 68%|██████▊   | 856/1260 [5:11:44<2:25:25, 21.60s/it] 68%|██████▊   | 857/1260 [5:12:06<2:25:17, 21.63s/it] 68%|██████▊   | 858/1260 [5:12:27<2:24:41, 21.60s/it] 68%|██████▊   | 859/1260 [5:12:49<2:24:21, 21.60s/it] 68%|██████▊   | 860/1260 [5:13:11<2:24:08, 21.62s/it]                                                      {'loss': 0.0332, 'grad_norm': 1.280089856982753, 'learning_rate': 8.342385879582332e-06, 'epoch': 13.65}
 68%|██████▊   | 860/1260 [5:13:11<2:24:08, 21.62s/it] 68%|██████▊   | 861/1260 [5:13:32<2:23:46, 21.62s/it] 68%|██████▊   | 862/1260 [5:13:54<2:23:39, 21.66s/it] 68%|██████▊   | 863/1260 [5:14:16<2:24:18, 21.81s/it] 69%|██████▊   | 864/1260 [5:14:38<2:23:49, 21.79s/it] 69%|██████▊   | 865/1260 [5:14:59<2:22:49, 21.70s/it]                                                      {'loss': 0.0334, 'grad_norm': 1.1133984452865286, 'learning_rate': 8.156840139702554e-06, 'epoch': 13.73}
 69%|██████▊   | 865/1260 [5:14:59<2:22:49, 21.70s/it] 69%|██████▊   | 866/1260 [5:15:21<2:22:00, 21.62s/it] 69%|██████▉   | 867/1260 [5:15:42<2:21:27, 21.60s/it] 69%|██████▉   | 868/1260 [5:16:04<2:21:55, 21.72s/it] 69%|██████▉   | 869/1260 [5:16:26<2:21:31, 21.72s/it] 69%|██████▉   | 870/1260 [5:16:48<2:20:54, 21.68s/it]                                                      {'loss': 0.0318, 'grad_norm': 1.318324640656861, 'learning_rate': 7.972607395772256e-06, 'epoch': 13.81}
 69%|██████▉   | 870/1260 [5:16:48<2:20:54, 21.68s/it] 69%|██████▉   | 871/1260 [5:17:09<2:20:06, 21.61s/it] 69%|██████▉   | 872/1260 [5:17:31<2:19:50, 21.62s/it] 69%|██████▉   | 873/1260 [5:17:52<2:19:18, 21.60s/it] 69%|██████▉   | 874/1260 [5:18:14<2:18:41, 21.56s/it] 69%|██████▉   | 875/1260 [5:18:35<2:18:03, 21.52s/it]                                                      {'loss': 0.0329, 'grad_norm': 1.6162328999783382, 'learning_rate': 7.789722996497514e-06, 'epoch': 13.89}
 69%|██████▉   | 875/1260 [5:18:35<2:18:03, 21.52s/it] 70%|██████▉   | 876/1260 [5:18:57<2:18:05, 21.58s/it] 70%|██████▉   | 877/1260 [5:19:19<2:18:22, 21.68s/it] 70%|██████▉   | 878/1260 [5:19:41<2:17:48, 21.64s/it] 70%|██████▉   | 879/1260 [5:20:02<2:17:42, 21.69s/it] 70%|██████▉   | 880/1260 [5:20:24<2:16:53, 21.61s/it]                                                      {'loss': 0.0324, 'grad_norm': 1.5270639070759273, 'learning_rate': 7.608222031877732e-06, 'epoch': 13.97}
 70%|██████▉   | 880/1260 [5:20:24<2:16:53, 21.61s/it] 70%|██████▉   | 881/1260 [5:20:45<2:16:21, 21.59s/it] 70%|███████   | 882/1260 [5:21:05<2:11:57, 20.95s/it][INFO|trainer.py:3993] 2025-07-05 02:51:26,960 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882
[INFO|configuration_utils.py:424] 2025-07-05 02:51:26,966 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/config.json
[INFO|configuration_utils.py:904] 2025-07-05 02:51:26,966 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 02:51:30,170 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:51:30,172 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:51:30,173 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:51:30,173 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/special_tokens_map.json
[2025-07-05 02:51:30,362] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step880 is about to be saved!
[2025-07-05 02:51:30,373] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 02:51:30,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 02:51:30,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 02:51:30,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 02:51:35,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 02:51:35,017] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 02:51:38,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step880 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 02:51:38,527 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 02:51:38,528 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 02:51:38,529 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 02:51:38,529 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 02:51:38,699 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 02:51:39,125 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-882/chat_template.jinja
 70%|███████   | 883/1260 [5:21:42<2:43:02, 25.95s/it] 70%|███████   | 884/1260 [5:22:04<2:34:16, 24.62s/it] 70%|███████   | 885/1260 [5:22:26<2:28:35, 23.77s/it]                                                      {'loss': 0.0228, 'grad_norm': 0.8264371153253206, 'learning_rate': 7.4281393264729584e-06, 'epoch': 14.05}
 70%|███████   | 885/1260 [5:22:26<2:28:35, 23.77s/it] 70%|███████   | 886/1260 [5:22:48<2:24:49, 23.23s/it] 70%|███████   | 887/1260 [5:23:09<2:21:16, 22.73s/it] 70%|███████   | 888/1260 [5:23:31<2:19:20, 22.47s/it] 71%|███████   | 889/1260 [5:23:52<2:17:01, 22.16s/it] 71%|███████   | 890/1260 [5:24:14<2:15:40, 22.00s/it]                                                      {'loss': 0.0204, 'grad_norm': 1.0961827966977844, 'learning_rate': 7.249509432722056e-06, 'epoch': 14.13}
 71%|███████   | 890/1260 [5:24:14<2:15:40, 22.00s/it] 71%|███████   | 891/1260 [5:24:36<2:14:35, 21.88s/it] 71%|███████   | 892/1260 [5:24:57<2:13:24, 21.75s/it] 71%|███████   | 893/1260 [5:25:19<2:12:31, 21.67s/it] 71%|███████   | 894/1260 [5:25:40<2:12:24, 21.71s/it] 71%|███████   | 895/1260 [5:26:02<2:12:15, 21.74s/it]                                                      {'loss': 0.0212, 'grad_norm': 1.7765393458161916, 'learning_rate': 7.072366624313169e-06, 'epoch': 14.21}
 71%|███████   | 895/1260 [5:26:02<2:12:15, 21.74s/it] 71%|███████   | 896/1260 [5:26:24<2:11:25, 21.66s/it] 71%|███████   | 897/1260 [5:26:46<2:11:21, 21.71s/it] 71%|███████▏  | 898/1260 [5:27:07<2:10:34, 21.64s/it] 71%|███████▏  | 899/1260 [5:27:29<2:10:17, 21.65s/it] 71%|███████▏  | 900/1260 [5:27:50<2:09:35, 21.60s/it]                                                      {'loss': 0.0198, 'grad_norm': 0.8282444402281276, 'learning_rate': 6.896744889607612e-06, 'epoch': 14.29}
 71%|███████▏  | 900/1260 [5:27:50<2:09:35, 21.60s/it] 72%|███████▏  | 901/1260 [5:28:12<2:09:01, 21.56s/it] 72%|███████▏  | 902/1260 [5:28:34<2:09:12, 21.66s/it] 72%|███████▏  | 903/1260 [5:28:55<2:09:02, 21.69s/it] 72%|███████▏  | 904/1260 [5:29:17<2:08:13, 21.61s/it] 72%|███████▏  | 905/1260 [5:29:39<2:08:26, 21.71s/it]                                                      {'loss': 0.0193, 'grad_norm': 1.1666119755301707, 'learning_rate': 6.722677925118561e-06, 'epoch': 14.37}
 72%|███████▏  | 905/1260 [5:29:39<2:08:26, 21.71s/it] 72%|███████▏  | 906/1260 [5:30:00<2:07:39, 21.64s/it] 72%|███████▏  | 907/1260 [5:30:22<2:07:21, 21.65s/it] 72%|███████▏  | 908/1260 [5:30:44<2:07:09, 21.68s/it] 72%|███████▏  | 909/1260 [5:31:05<2:06:35, 21.64s/it] 72%|███████▏  | 910/1260 [5:31:27<2:06:14, 21.64s/it]                                                      {'loss': 0.0212, 'grad_norm': 3.763155014224558, 'learning_rate': 6.55019912904567e-06, 'epoch': 14.45}
 72%|███████▏  | 910/1260 [5:31:27<2:06:14, 21.64s/it] 72%|███████▏  | 911/1260 [5:31:48<2:05:48, 21.63s/it] 72%|███████▏  | 912/1260 [5:32:10<2:05:10, 21.58s/it] 72%|███████▏  | 913/1260 [5:32:31<2:04:50, 21.59s/it] 73%|███████▎  | 914/1260 [5:32:53<2:04:17, 21.55s/it] 73%|███████▎  | 915/1260 [5:33:14<2:03:41, 21.51s/it]                                                      {'loss': 0.0224, 'grad_norm': 2.424978479227498, 'learning_rate': 6.379341594866983e-06, 'epoch': 14.52}
 73%|███████▎  | 915/1260 [5:33:14<2:03:41, 21.51s/it] 73%|███████▎  | 916/1260 [5:33:36<2:03:17, 21.50s/it] 73%|███████▎  | 917/1260 [5:33:57<2:02:56, 21.51s/it] 73%|███████▎  | 918/1260 [5:34:19<2:02:44, 21.53s/it] 73%|███████▎  | 919/1260 [5:34:41<2:03:14, 21.69s/it] 73%|███████▎  | 920/1260 [5:35:03<2:03:15, 21.75s/it]                                                      {'loss': 0.0211, 'grad_norm': 1.3263968651065081, 'learning_rate': 6.2101381049893e-06, 'epoch': 14.6}
 73%|███████▎  | 920/1260 [5:35:03<2:03:15, 21.75s/it] 73%|███████▎  | 921/1260 [5:35:24<2:02:24, 21.66s/it] 73%|███████▎  | 922/1260 [5:35:46<2:01:51, 21.63s/it] 73%|███████▎  | 923/1260 [5:36:07<2:01:22, 21.61s/it] 73%|███████▎  | 924/1260 [5:36:29<2:00:39, 21.55s/it] 73%|███████▎  | 925/1260 [5:36:50<2:00:28, 21.58s/it]                                                      {'loss': 0.0193, 'grad_norm': 1.2823949203607552, 'learning_rate': 6.0426211244582105e-06, 'epoch': 14.68}
 73%|███████▎  | 925/1260 [5:36:51<2:00:28, 21.58s/it] 73%|███████▎  | 926/1260 [5:37:12<2:00:17, 21.61s/it] 74%|███████▎  | 927/1260 [5:37:34<2:00:15, 21.67s/it] 74%|███████▎  | 928/1260 [5:37:55<1:59:25, 21.58s/it] 74%|███████▎  | 929/1260 [5:38:17<1:58:56, 21.56s/it] 74%|███████▍  | 930/1260 [5:38:38<1:58:17, 21.51s/it]                                                      {'loss': 0.0193, 'grad_norm': 2.014626225859072, 'learning_rate': 5.876822794729035e-06, 'epoch': 14.76}
 74%|███████▍  | 930/1260 [5:38:38<1:58:17, 21.51s/it] 74%|███████▍  | 931/1260 [5:39:00<1:58:11, 21.55s/it] 74%|███████▍  | 932/1260 [5:39:22<1:58:13, 21.63s/it] 74%|███████▍  | 933/1260 [5:39:43<1:57:52, 21.63s/it] 74%|███████▍  | 934/1260 [5:40:05<1:57:10, 21.57s/it] 74%|███████▍  | 935/1260 [5:40:26<1:56:32, 21.52s/it]                                                      {'loss': 0.0195, 'grad_norm': 1.3601180128532093, 'learning_rate': 5.712774927499851e-06, 'epoch': 14.84}
 74%|███████▍  | 935/1260 [5:40:26<1:56:32, 21.52s/it] 74%|███████▍  | 936/1260 [5:40:48<1:56:03, 21.49s/it] 74%|███████▍  | 937/1260 [5:41:09<1:55:41, 21.49s/it] 74%|███████▍  | 938/1260 [5:41:30<1:55:06, 21.45s/it] 75%|███████▍  | 939/1260 [5:41:52<1:55:00, 21.50s/it] 75%|███████▍  | 940/1260 [5:42:13<1:54:29, 21.47s/it]                                                      {'loss': 0.0192, 'grad_norm': 1.0896215209913216, 'learning_rate': 5.55050899860778e-06, 'epoch': 14.92}
 75%|███████▍  | 940/1260 [5:42:13<1:54:29, 21.47s/it] 75%|███████▍  | 941/1260 [5:42:35<1:54:52, 21.61s/it] 75%|███████▍  | 942/1260 [5:42:57<1:55:14, 21.74s/it] 75%|███████▍  | 943/1260 [5:43:19<1:54:31, 21.68s/it] 75%|███████▍  | 944/1260 [5:43:40<1:53:52, 21.62s/it] 75%|███████▌  | 945/1260 [5:44:00<1:50:19, 21.02s/it]                                                      {'loss': 0.0178, 'grad_norm': 1.0683651337652607, 'learning_rate': 5.390056141989745e-06, 'epoch': 15.0}
 75%|███████▌  | 945/1260 [5:44:00<1:50:19, 21.02s/it][INFO|trainer.py:3993] 2025-07-05 03:14:22,042 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945
[INFO|configuration_utils.py:424] 2025-07-05 03:14:22,048 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/config.json
[INFO|configuration_utils.py:904] 2025-07-05 03:14:22,049 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 03:14:25,270 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:14:25,272 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:14:25,273 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:14:25,274 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/special_tokens_map.json
[2025-07-05 03:14:25,465] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step943 is about to be saved!
[2025-07-05 03:14:25,476] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 03:14:25,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 03:14:25,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 03:14:25,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 03:14:30,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 03:14:30,663] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/global_step943/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 03:14:33,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step943 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 03:14:33,651 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:14:33,652 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:14:33,652 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:14:33,654 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 03:14:33,818 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 03:14:34,224 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-945/chat_template.jinja
 75%|███████▌  | 946/1260 [5:44:37<2:15:43, 25.94s/it] 75%|███████▌  | 947/1260 [5:45:00<2:09:15, 24.78s/it] 75%|███████▌  | 948/1260 [5:45:21<2:03:38, 23.78s/it] 75%|███████▌  | 949/1260 [5:45:43<1:59:55, 23.14s/it] 75%|███████▌  | 950/1260 [5:46:04<1:56:57, 22.64s/it]                                                      {'loss': 0.0121, 'grad_norm': 0.9360251237571281, 'learning_rate': 5.231447143708774e-06, 'epoch': 15.08}
 75%|███████▌  | 950/1260 [5:46:04<1:56:57, 22.64s/it] 75%|███████▌  | 951/1260 [5:46:26<1:55:06, 22.35s/it] 76%|███████▌  | 952/1260 [5:46:47<1:53:26, 22.10s/it] 76%|███████▌  | 953/1260 [5:47:09<1:52:10, 21.92s/it] 76%|███████▌  | 954/1260 [5:47:30<1:51:15, 21.81s/it] 76%|███████▌  | 955/1260 [5:47:52<1:50:42, 21.78s/it]                                                      {'loss': 0.011, 'grad_norm': 1.2203779237141539, 'learning_rate': 5.0747124360471125e-06, 'epoch': 15.16}
 76%|███████▌  | 955/1260 [5:47:52<1:50:42, 21.78s/it] 76%|███████▌  | 956/1260 [5:48:14<1:49:49, 21.68s/it] 76%|███████▌  | 957/1260 [5:48:35<1:49:10, 21.62s/it] 76%|███████▌  | 958/1260 [5:48:57<1:48:42, 21.60s/it] 76%|███████▌  | 959/1260 [5:49:18<1:48:09, 21.56s/it] 76%|███████▌  | 960/1260 [5:49:40<1:48:10, 21.64s/it]                                                      {'loss': 0.0111, 'grad_norm': 2.698913016386223, 'learning_rate': 4.9198820916671634e-06, 'epoch': 15.24}
 76%|███████▌  | 960/1260 [5:49:40<1:48:10, 21.64s/it] 76%|███████▋  | 961/1260 [5:50:02<1:48:22, 21.75s/it] 76%|███████▋  | 962/1260 [5:50:24<1:48:00, 21.75s/it] 76%|███████▋  | 963/1260 [5:50:45<1:47:51, 21.79s/it] 77%|███████▋  | 964/1260 [5:51:08<1:47:56, 21.88s/it] 77%|███████▋  | 965/1260 [5:51:29<1:47:11, 21.80s/it]                                                      {'loss': 0.0117, 'grad_norm': 1.380765136020527, 'learning_rate': 4.766985817841482e-06, 'epoch': 15.32}
 77%|███████▋  | 965/1260 [5:51:29<1:47:11, 21.80s/it] 77%|███████▋  | 966/1260 [5:51:51<1:46:20, 21.70s/it] 77%|███████▋  | 967/1260 [5:52:12<1:45:44, 21.66s/it] 77%|███████▋  | 968/1260 [5:52:34<1:45:40, 21.71s/it] 77%|███████▋  | 969/1260 [5:52:56<1:45:17, 21.71s/it] 77%|███████▋  | 970/1260 [5:53:18<1:44:59, 21.72s/it]                                                      {'loss': 0.012, 'grad_norm': 1.6935195250004558, 'learning_rate': 4.616052950752807e-06, 'epoch': 15.4}
 77%|███████▋  | 970/1260 [5:53:18<1:44:59, 21.72s/it] 77%|███████▋  | 971/1260 [5:53:39<1:44:53, 21.78s/it] 77%|███████▋  | 972/1260 [5:54:01<1:44:29, 21.77s/it] 77%|███████▋  | 973/1260 [5:54:23<1:43:50, 21.71s/it] 77%|███████▋  | 974/1260 [5:54:44<1:43:04, 21.62s/it] 77%|███████▋  | 975/1260 [5:55:06<1:42:30, 21.58s/it]                                                      {'loss': 0.0118, 'grad_norm': 2.7789468538561177, 'learning_rate': 4.4671124498653624e-06, 'epoch': 15.48}
 77%|███████▋  | 975/1260 [5:55:06<1:42:30, 21.58s/it] 77%|███████▋  | 976/1260 [5:55:27<1:42:31, 21.66s/it] 78%|███████▊  | 977/1260 [5:55:49<1:42:10, 21.66s/it] 78%|███████▊  | 978/1260 [5:56:11<1:41:50, 21.67s/it] 78%|███████▊  | 979/1260 [5:56:32<1:41:15, 21.62s/it] 78%|███████▊  | 980/1260 [5:56:54<1:40:39, 21.57s/it]                                                      {'loss': 0.0129, 'grad_norm': 0.6555567149824163, 'learning_rate': 4.320192892368389e-06, 'epoch': 15.56}
 78%|███████▊  | 980/1260 [5:56:54<1:40:39, 21.57s/it] 78%|███████▊  | 981/1260 [5:57:16<1:40:42, 21.66s/it] 78%|███████▊  | 982/1260 [5:57:37<1:40:01, 21.59s/it] 78%|███████▊  | 983/1260 [5:57:59<1:39:34, 21.57s/it] 78%|███████▊  | 984/1260 [5:58:20<1:39:33, 21.64s/it] 78%|███████▊  | 985/1260 [5:58:42<1:39:17, 21.66s/it]                                                      {'loss': 0.0111, 'grad_norm': 0.8630378293421606, 'learning_rate': 4.175322467693068e-06, 'epoch': 15.64}
 78%|███████▊  | 985/1260 [5:58:42<1:39:17, 21.66s/it] 78%|███████▊  | 986/1260 [5:59:04<1:39:29, 21.79s/it] 78%|███████▊  | 987/1260 [5:59:26<1:38:53, 21.73s/it] 78%|███████▊  | 988/1260 [5:59:48<1:38:34, 21.74s/it] 78%|███████▊  | 989/1260 [6:00:09<1:37:48, 21.66s/it] 79%|███████▊  | 990/1260 [6:00:31<1:37:14, 21.61s/it]                                                      {'loss': 0.0106, 'grad_norm': 0.6158406084727938, 'learning_rate': 4.032528972103797e-06, 'epoch': 15.72}
 79%|███████▊  | 990/1260 [6:00:31<1:37:14, 21.61s/it] 79%|███████▊  | 991/1260 [6:00:52<1:36:37, 21.55s/it] 79%|███████▊  | 992/1260 [6:01:14<1:36:38, 21.64s/it] 79%|███████▉  | 993/1260 [6:01:35<1:36:10, 21.61s/it] 79%|███████▉  | 994/1260 [6:01:57<1:35:47, 21.61s/it] 79%|███████▉  | 995/1260 [6:02:19<1:35:25, 21.60s/it]                                                      {'loss': 0.011, 'grad_norm': 0.9668229968721485, 'learning_rate': 3.891839803364934e-06, 'epoch': 15.8}
 79%|███████▉  | 995/1260 [6:02:19<1:35:25, 21.60s/it] 79%|███████▉  | 996/1260 [6:02:40<1:35:03, 21.60s/it] 79%|███████▉  | 997/1260 [6:03:02<1:34:50, 21.64s/it] 79%|███████▉  | 998/1260 [6:03:23<1:34:28, 21.64s/it] 79%|███████▉  | 999/1260 [6:03:45<1:34:31, 21.73s/it] 79%|███████▉  | 1000/1260 [6:04:07<1:34:31, 21.81s/it]                                                       {'loss': 0.0103, 'grad_norm': 0.5723734371451407, 'learning_rate': 3.7532819554839853e-06, 'epoch': 15.87}
 79%|███████▉  | 1000/1260 [6:04:07<1:34:31, 21.81s/it] 79%|███████▉  | 1001/1260 [6:04:29<1:33:46, 21.72s/it] 80%|███████▉  | 1002/1260 [6:04:51<1:33:25, 21.73s/it] 80%|███████▉  | 1003/1260 [6:05:12<1:32:49, 21.67s/it] 80%|███████▉  | 1004/1260 [6:05:34<1:32:35, 21.70s/it] 80%|███████▉  | 1005/1260 [6:05:56<1:32:04, 21.67s/it]                                                       {'loss': 0.011, 'grad_norm': 1.0376926197434526, 'learning_rate': 3.6168820135322987e-06, 'epoch': 15.95}
 80%|███████▉  | 1005/1260 [6:05:56<1:32:04, 21.67s/it] 80%|███████▉  | 1006/1260 [6:06:17<1:31:45, 21.67s/it] 80%|███████▉  | 1007/1260 [6:06:39<1:31:04, 21.60s/it] 80%|████████  | 1008/1260 [6:06:58<1:28:00, 20.95s/it][INFO|trainer.py:3993] 2025-07-05 03:37:19,909 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008
[INFO|configuration_utils.py:424] 2025-07-05 03:37:19,914 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/config.json
[INFO|configuration_utils.py:904] 2025-07-05 03:37:19,915 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 03:37:23,085 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:37:23,087 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:37:23,088 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:37:23,088 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/special_tokens_map.json
[2025-07-05 03:37:23,276] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1006 is about to be saved!
[2025-07-05 03:37:23,286] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 03:37:23,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 03:37:23,320] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 03:37:23,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 03:37:28,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 03:37:28,641] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/global_step1006/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 03:37:31,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1006 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 03:37:31,519 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 03:37:31,520 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 03:37:31,521 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 03:37:31,521 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 03:37:31,681 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 03:37:32,095 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1008/chat_template.jinja
 80%|████████  | 1009/1260 [6:07:35<1:48:08, 25.85s/it] 80%|████████  | 1010/1260 [6:07:57<1:42:21, 24.56s/it]                                                       {'loss': 0.0084, 'grad_norm': 0.5127232802582058, 'learning_rate': 3.482666148544146e-06, 'epoch': 16.03}
 80%|████████  | 1010/1260 [6:07:57<1:42:21, 24.56s/it] 80%|████████  | 1011/1260 [6:08:18<1:38:05, 23.64s/it] 80%|████████  | 1012/1260 [6:08:40<1:34:58, 22.98s/it] 80%|████████  | 1013/1260 [6:09:02<1:33:11, 22.64s/it] 80%|████████  | 1014/1260 [6:09:23<1:31:23, 22.29s/it] 81%|████████  | 1015/1260 [6:09:45<1:30:22, 22.13s/it]                                                       {'loss': 0.0063, 'grad_norm': 0.6187439081269845, 'learning_rate': 3.3506601124953246e-06, 'epoch': 16.11}
 81%|████████  | 1015/1260 [6:09:45<1:30:22, 22.13s/it] 81%|████████  | 1016/1260 [6:10:06<1:29:10, 21.93s/it] 81%|████████  | 1017/1260 [6:10:28<1:28:45, 21.92s/it] 81%|████████  | 1018/1260 [6:10:50<1:27:49, 21.78s/it] 81%|████████  | 1019/1260 [6:11:12<1:27:46, 21.85s/it] 81%|████████  | 1020/1260 [6:11:33<1:26:55, 21.73s/it]                                                       {'loss': 0.0058, 'grad_norm': 0.6312995593325077, 'learning_rate': 3.220889233362113e-06, 'epoch': 16.19}
 81%|████████  | 1020/1260 [6:11:33<1:26:55, 21.73s/it] 81%|████████  | 1021/1260 [6:11:55<1:26:33, 21.73s/it] 81%|████████  | 1022/1260 [6:12:17<1:26:07, 21.71s/it] 81%|████████  | 1023/1260 [6:12:38<1:25:27, 21.64s/it] 81%|████████▏ | 1024/1260 [6:13:01<1:26:10, 21.91s/it] 81%|████████▏ | 1025/1260 [6:13:22<1:25:20, 21.79s/it]                                                       {'loss': 0.0069, 'grad_norm': 0.8295842879807579, 'learning_rate': 3.0933784102616147e-06, 'epoch': 16.27}
 81%|████████▏ | 1025/1260 [6:13:22<1:25:20, 21.79s/it] 81%|████████▏ | 1026/1260 [6:13:44<1:24:42, 21.72s/it] 82%|████████▏ | 1027/1260 [6:14:06<1:24:30, 21.76s/it] 82%|████████▏ | 1028/1260 [6:14:27<1:23:58, 21.72s/it] 82%|████████▏ | 1029/1260 [6:14:49<1:23:14, 21.62s/it] 82%|████████▏ | 1030/1260 [6:15:10<1:22:42, 21.58s/it]                                                       {'loss': 0.0055, 'grad_norm': 0.402216462233061, 'learning_rate': 2.9681521086743426e-06, 'epoch': 16.35}
 82%|████████▏ | 1030/1260 [6:15:10<1:22:42, 21.58s/it] 82%|████████▏ | 1031/1260 [6:15:31<1:22:09, 21.53s/it] 82%|████████▏ | 1032/1260 [6:15:53<1:21:43, 21.51s/it] 82%|████████▏ | 1033/1260 [6:16:14<1:21:18, 21.49s/it] 82%|████████▏ | 1034/1260 [6:16:36<1:20:52, 21.47s/it] 82%|████████▏ | 1035/1260 [6:16:57<1:20:24, 21.44s/it]                                                       {'loss': 0.0058, 'grad_norm': 1.7503078120954965, 'learning_rate': 2.845234355750051e-06, 'epoch': 16.43}
 82%|████████▏ | 1035/1260 [6:16:57<1:20:24, 21.44s/it] 82%|████████▏ | 1036/1260 [6:17:19<1:20:04, 21.45s/it] 82%|████████▏ | 1037/1260 [6:17:40<1:19:59, 21.52s/it] 82%|████████▏ | 1038/1260 [6:18:02<1:19:33, 21.50s/it] 82%|████████▏ | 1039/1260 [6:18:23<1:19:07, 21.48s/it] 83%|████████▎ | 1040/1260 [6:18:45<1:19:18, 21.63s/it]                                                       {'loss': 0.0061, 'grad_norm': 0.9631605818103928, 'learning_rate': 2.7246487356976447e-06, 'epoch': 16.51}
 83%|████████▎ | 1040/1260 [6:18:45<1:19:18, 21.63s/it] 83%|████████▎ | 1041/1260 [6:19:07<1:18:49, 21.60s/it] 83%|████████▎ | 1042/1260 [6:19:29<1:18:40, 21.65s/it] 83%|████████▎ | 1043/1260 [6:19:50<1:18:19, 21.66s/it] 83%|████████▎ | 1044/1260 [6:20:12<1:17:58, 21.66s/it] 83%|████████▎ | 1045/1260 [6:20:33<1:17:27, 21.61s/it]                                                       {'loss': 0.007, 'grad_norm': 1.0686474489752416, 'learning_rate': 2.60641838526008e-06, 'epoch': 16.59}
 83%|████████▎ | 1045/1260 [6:20:33<1:17:27, 21.61s/it] 83%|████████▎ | 1046/1260 [6:20:55<1:17:17, 21.67s/it] 83%|████████▎ | 1047/1260 [6:21:17<1:16:46, 21.63s/it] 83%|████████▎ | 1048/1260 [6:21:38<1:16:28, 21.64s/it] 83%|████████▎ | 1049/1260 [6:22:00<1:15:58, 21.61s/it] 83%|████████▎ | 1050/1260 [6:22:22<1:15:43, 21.63s/it]                                                       {'loss': 0.0058, 'grad_norm': 0.9867602272912058, 'learning_rate': 2.490565989275118e-06, 'epoch': 16.67}
 83%|████████▎ | 1050/1260 [6:22:22<1:15:43, 21.63s/it] 83%|████████▎ | 1051/1260 [6:22:43<1:15:07, 21.57s/it] 83%|████████▎ | 1052/1260 [6:23:05<1:14:55, 21.61s/it] 84%|████████▎ | 1053/1260 [6:23:26<1:14:27, 21.58s/it] 84%|████████▎ | 1054/1260 [6:23:48<1:14:02, 21.57s/it] 84%|████████▎ | 1055/1260 [6:24:09<1:13:31, 21.52s/it]                                                       {'loss': 0.0057, 'grad_norm': 0.8802630860197522, 'learning_rate': 2.3771137763228014e-06, 'epoch': 16.75}
 84%|████████▎ | 1055/1260 [6:24:09<1:13:31, 21.52s/it] 84%|████████▍ | 1056/1260 [6:24:31<1:13:30, 21.62s/it] 84%|████████▍ | 1057/1260 [6:24:53<1:13:27, 21.71s/it] 84%|████████▍ | 1058/1260 [6:25:14<1:12:46, 21.62s/it] 84%|████████▍ | 1059/1260 [6:25:36<1:12:26, 21.62s/it] 84%|████████▍ | 1060/1260 [6:25:58<1:12:02, 21.61s/it]                                                       {'loss': 0.0056, 'grad_norm': 0.9695988930158178, 'learning_rate': 2.2660835144604407e-06, 'epoch': 16.83}
 84%|████████▍ | 1060/1260 [6:25:58<1:12:02, 21.61s/it] 84%|████████▍ | 1061/1260 [6:26:19<1:11:26, 21.54s/it] 84%|████████▍ | 1062/1260 [6:26:41<1:11:09, 21.56s/it] 84%|████████▍ | 1063/1260 [6:27:02<1:10:46, 21.56s/it] 84%|████████▍ | 1064/1260 [6:27:23<1:10:13, 21.50s/it] 85%|████████▍ | 1065/1260 [6:27:45<1:09:58, 21.53s/it]                                                       {'loss': 0.0056, 'grad_norm': 1.595683387528161, 'learning_rate': 2.1574965070460047e-06, 'epoch': 16.91}
 85%|████████▍ | 1065/1260 [6:27:45<1:09:58, 21.53s/it] 85%|████████▍ | 1066/1260 [6:28:07<1:09:52, 21.61s/it] 85%|████████▍ | 1067/1260 [6:28:28<1:09:18, 21.54s/it] 85%|████████▍ | 1068/1260 [6:28:50<1:08:49, 21.51s/it] 85%|████████▍ | 1069/1260 [6:29:11<1:08:20, 21.47s/it] 85%|████████▍ | 1070/1260 [6:29:33<1:07:59, 21.47s/it]                                                       {'loss': 0.0059, 'grad_norm': 0.5757681228407996, 'learning_rate': 2.0513735886506258e-06, 'epoch': 16.99}
 85%|████████▍ | 1070/1260 [6:29:33<1:07:59, 21.47s/it] 85%|████████▌ | 1071/1260 [6:29:52<1:05:39, 20.84s/it][INFO|trainer.py:3993] 2025-07-05 04:00:14,096 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071
[INFO|configuration_utils.py:424] 2025-07-05 04:00:14,102 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:00:14,103 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:00:17,187 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:00:17,189 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:00:17,190 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:00:17,190 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/special_tokens_map.json
[2025-07-05 04:00:17,385] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1068 is about to be saved!
[2025-07-05 04:00:17,396] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:00:17,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:00:17,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:00:17,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:00:22,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:00:22,229] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/global_step1068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:00:25,589] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1068 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:00:25,600 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:00:25,601 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:00:25,601 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:00:25,601 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:00:25,770 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:00:26,181 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1071/chat_template.jinja
 85%|████████▌ | 1072/1260 [6:30:30<1:21:16, 25.94s/it] 85%|████████▌ | 1073/1260 [6:30:52<1:16:57, 24.69s/it] 85%|████████▌ | 1074/1260 [6:31:14<1:14:02, 23.88s/it] 85%|████████▌ | 1075/1260 [6:31:35<1:11:32, 23.20s/it]                                                       {'loss': 0.0031, 'grad_norm': 0.33124818413658813, 'learning_rate': 1.947735121061088e-06, 'epoch': 17.06}
 85%|████████▌ | 1075/1260 [6:31:35<1:11:32, 23.20s/it] 85%|████████▌ | 1076/1260 [6:31:57<1:09:34, 22.69s/it] 85%|████████▌ | 1077/1260 [6:32:18<1:08:06, 22.33s/it] 86%|████████▌ | 1078/1260 [6:32:40<1:06:54, 22.06s/it] 86%|████████▌ | 1079/1260 [6:33:01<1:06:23, 22.01s/it] 86%|████████▌ | 1080/1260 [6:33:23<1:05:32, 21.85s/it]                                                       {'loss': 0.0033, 'grad_norm': 0.40038884976896477, 'learning_rate': 1.8466009893730057e-06, 'epoch': 17.14}
 86%|████████▌ | 1080/1260 [6:33:23<1:05:32, 21.85s/it] 86%|████████▌ | 1081/1260 [6:33:45<1:05:09, 21.84s/it] 86%|████████▌ | 1082/1260 [6:34:07<1:04:48, 21.85s/it] 86%|████████▌ | 1083/1260 [6:34:28<1:04:15, 21.78s/it] 86%|████████▌ | 1084/1260 [6:34:50<1:03:44, 21.73s/it] 86%|████████▌ | 1085/1260 [6:35:11<1:03:14, 21.68s/it]                                                       {'loss': 0.0028, 'grad_norm': 0.19657692740859117, 'learning_rate': 1.7479905981754917e-06, 'epoch': 17.22}
 86%|████████▌ | 1085/1260 [6:35:11<1:03:14, 21.68s/it] 86%|████████▌ | 1086/1260 [6:35:33<1:02:58, 21.71s/it] 86%|████████▋ | 1087/1260 [6:35:55<1:02:23, 21.64s/it] 86%|████████▋ | 1088/1260 [6:36:16<1:01:56, 21.61s/it] 86%|████████▋ | 1089/1260 [6:36:38<1:01:46, 21.68s/it] 87%|████████▋ | 1090/1260 [6:37:00<1:01:22, 21.66s/it]                                                       {'loss': 0.0029, 'grad_norm': 0.6790228466685031, 'learning_rate': 1.6519228678279718e-06, 'epoch': 17.3}
 87%|████████▋ | 1090/1260 [6:37:00<1:01:22, 21.66s/it] 87%|████████▋ | 1091/1260 [6:37:21<1:01:00, 21.66s/it] 87%|████████▋ | 1092/1260 [6:37:43<1:00:34, 21.63s/it] 87%|████████▋ | 1093/1260 [6:38:04<1:00:04, 21.58s/it] 87%|████████▋ | 1094/1260 [6:38:26<59:49, 21.63s/it]   87%|████████▋ | 1095/1260 [6:38:48<59:47, 21.74s/it]                                                     {'loss': 0.0026, 'grad_norm': 0.24132616089779013, 'learning_rate': 1.5584162308299675e-06, 'epoch': 17.38}
 87%|████████▋ | 1095/1260 [6:38:48<59:47, 21.74s/it] 87%|████████▋ | 1096/1260 [6:39:10<59:43, 21.85s/it] 87%|████████▋ | 1097/1260 [6:39:32<59:05, 21.75s/it] 87%|████████▋ | 1098/1260 [6:39:53<58:33, 21.69s/it] 87%|████████▋ | 1099/1260 [6:40:15<58:37, 21.85s/it] 87%|████████▋ | 1100/1260 [6:40:37<58:19, 21.87s/it]                                                     {'loss': 0.003, 'grad_norm': 0.8035557435683165, 'learning_rate': 1.467488628284434e-06, 'epoch': 17.46}
 87%|████████▋ | 1100/1260 [6:40:37<58:19, 21.87s/it] 87%|████████▋ | 1101/1260 [6:40:59<57:37, 21.75s/it] 87%|████████▋ | 1102/1260 [6:41:21<57:14, 21.74s/it] 88%|████████▊ | 1103/1260 [6:41:42<56:39, 21.66s/it] 88%|████████▊ | 1104/1260 [6:42:04<56:14, 21.63s/it] 88%|████████▊ | 1105/1260 [6:42:25<55:47, 21.60s/it]                                                     {'loss': 0.0026, 'grad_norm': 0.45468600566398837, 'learning_rate': 1.3791575064554262e-06, 'epoch': 17.54}
 88%|████████▊ | 1105/1260 [6:42:25<55:47, 21.60s/it] 88%|████████▊ | 1106/1260 [6:42:47<55:19, 21.55s/it] 88%|████████▊ | 1107/1260 [6:43:08<55:04, 21.60s/it] 88%|████████▊ | 1108/1260 [6:43:30<54:47, 21.63s/it] 88%|████████▊ | 1109/1260 [6:43:51<54:15, 21.56s/it] 88%|████████▊ | 1110/1260 [6:44:13<53:48, 21.52s/it]                                                     {'loss': 0.0028, 'grad_norm': 0.45640208632820733, 'learning_rate': 1.2934398134206604e-06, 'epoch': 17.62}
 88%|████████▊ | 1110/1260 [6:44:13<53:48, 21.52s/it] 88%|████████▊ | 1111/1260 [6:44:34<53:24, 21.51s/it] 88%|████████▊ | 1112/1260 [6:44:56<53:16, 21.60s/it] 88%|████████▊ | 1113/1260 [6:45:18<53:04, 21.66s/it] 88%|████████▊ | 1114/1260 [6:45:40<52:56, 21.76s/it] 88%|████████▊ | 1115/1260 [6:46:01<52:25, 21.70s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.3534512903289927, 'learning_rate': 1.2103519958197084e-06, 'epoch': 17.7}
 88%|████████▊ | 1115/1260 [6:46:01<52:25, 21.70s/it] 89%|████████▊ | 1116/1260 [6:46:23<52:07, 21.72s/it] 89%|████████▊ | 1117/1260 [6:46:45<51:34, 21.64s/it] 89%|████████▊ | 1118/1260 [6:47:06<51:06, 21.59s/it] 89%|████████▉ | 1119/1260 [6:47:28<50:47, 21.61s/it] 89%|████████▉ | 1120/1260 [6:47:50<50:34, 21.68s/it]                                                     {'loss': 0.0029, 'grad_norm': 0.31208693975089064, 'learning_rate': 1.129909995698377e-06, 'epoch': 17.78}
 89%|████████▉ | 1120/1260 [6:47:50<50:34, 21.68s/it] 89%|████████▉ | 1121/1260 [6:48:11<50:04, 21.61s/it] 89%|████████▉ | 1122/1260 [6:48:33<49:46, 21.64s/it] 89%|████████▉ | 1123/1260 [6:48:54<49:26, 21.65s/it] 89%|████████▉ | 1124/1260 [6:49:16<48:54, 21.58s/it] 89%|████████▉ | 1125/1260 [6:49:38<48:37, 21.61s/it]                                                     {'loss': 0.0029, 'grad_norm': 0.3486428032624226, 'learning_rate': 1.052129247449915e-06, 'epoch': 17.86}
 89%|████████▉ | 1125/1260 [6:49:38<48:37, 21.61s/it] 89%|████████▉ | 1126/1260 [6:49:59<48:09, 21.57s/it] 89%|████████▉ | 1127/1260 [6:50:21<47:50, 21.59s/it] 90%|████████▉ | 1128/1260 [6:50:42<47:36, 21.64s/it] 90%|████████▉ | 1129/1260 [6:51:04<47:17, 21.66s/it] 90%|████████▉ | 1130/1260 [6:51:26<46:49, 21.61s/it]                                                     {'loss': 0.0027, 'grad_norm': 0.3114708331955041, 'learning_rate': 9.77024674853611e-07, 'epoch': 17.94}
 90%|████████▉ | 1130/1260 [6:51:26<46:49, 21.61s/it] 90%|████████▉ | 1131/1260 [6:51:48<46:48, 21.77s/it] 90%|████████▉ | 1132/1260 [6:52:09<46:17, 21.70s/it] 90%|████████▉ | 1133/1260 [6:52:31<45:53, 21.68s/it] 90%|█████████ | 1134/1260 [6:52:50<44:06, 21.00s/it][INFO|trainer.py:3993] 2025-07-05 04:23:12,153 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134
[INFO|configuration_utils.py:424] 2025-07-05 04:23:12,159 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:23:12,160 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:23:15,341 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:23:15,343 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:23:15,343 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:23:15,344 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/special_tokens_map.json
[2025-07-05 04:23:15,532] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1131 is about to be saved!
[2025-07-05 04:23:15,543] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:23:15,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:23:15,577] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:23:15,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:23:20,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:23:20,912] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/global_step1131/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:23:23,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1131 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:23:24,003 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:23:24,004 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:23:24,005 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:23:24,005 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:23:24,166 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:23:24,587 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1134/chat_template.jinja
 90%|█████████ | 1135/1260 [6:53:28<54:24, 26.12s/it]                                                     {'loss': 0.0025, 'grad_norm': 0.30643737667300186, 'learning_rate': 9.046106882113753e-07, 'epoch': 18.02}
 90%|█████████ | 1135/1260 [6:53:28<54:24, 26.12s/it] 90%|█████████ | 1136/1260 [6:53:50<51:14, 24.79s/it] 90%|█████████ | 1137/1260 [6:54:12<49:07, 23.96s/it] 90%|█████████ | 1138/1260 [6:54:34<47:16, 23.25s/it] 90%|█████████ | 1139/1260 [6:54:55<45:47, 22.71s/it] 90%|█████████ | 1140/1260 [6:55:17<44:46, 22.39s/it]                                                     {'loss': 0.0018, 'grad_norm': 0.26167552239058733, 'learning_rate': 8.349011815828322e-07, 'epoch': 18.1}
 90%|█████████ | 1140/1260 [6:55:17<44:46, 22.39s/it] 91%|█████████ | 1141/1260 [6:55:38<43:53, 22.13s/it] 91%|█████████ | 1142/1260 [6:56:00<43:22, 22.05s/it] 91%|█████████ | 1143/1260 [6:56:22<42:42, 21.90s/it] 91%|█████████ | 1144/1260 [6:56:43<42:07, 21.79s/it] 91%|█████████ | 1145/1260 [6:57:05<41:36, 21.71s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.17509969139663828, 'learning_rate': 7.679095301194849e-07, 'epoch': 18.17}
 91%|█████████ | 1145/1260 [6:57:05<41:36, 21.71s/it] 91%|█████████ | 1146/1260 [6:57:26<41:11, 21.68s/it] 91%|█████████ | 1147/1260 [6:57:48<40:57, 21.74s/it] 91%|█████████ | 1148/1260 [6:58:10<40:34, 21.73s/it] 91%|█████████ | 1149/1260 [6:58:32<40:17, 21.78s/it] 91%|█████████▏| 1150/1260 [6:58:54<39:49, 21.72s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.2943161176454276, 'learning_rate': 7.036485874984044e-07, 'epoch': 18.25}
 91%|█████████▏| 1150/1260 [6:58:54<39:49, 21.72s/it] 91%|█████████▏| 1151/1260 [6:59:15<39:19, 21.65s/it] 91%|█████████▏| 1152/1260 [6:59:36<38:50, 21.58s/it] 92%|█████████▏| 1153/1260 [6:59:58<38:33, 21.62s/it] 92%|█████████▏| 1154/1260 [7:00:20<38:15, 21.65s/it] 92%|█████████▏| 1155/1260 [7:00:41<37:49, 21.62s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.21949176947421373, 'learning_rate': 6.421306834560126e-07, 'epoch': 18.33}
 92%|█████████▏| 1155/1260 [7:00:41<37:49, 21.62s/it] 92%|█████████▏| 1156/1260 [7:01:03<37:35, 21.69s/it] 92%|█████████▏| 1157/1260 [7:01:25<37:06, 21.62s/it] 92%|█████████▏| 1158/1260 [7:01:46<36:39, 21.56s/it] 92%|█████████▏| 1159/1260 [7:02:08<36:12, 21.51s/it] 92%|█████████▏| 1160/1260 [7:02:29<35:55, 21.56s/it]                                                     {'loss': 0.002, 'grad_norm': 0.40287401470648204, 'learning_rate': 5.83367621422376e-07, 'epoch': 18.41}
 92%|█████████▏| 1160/1260 [7:02:29<35:55, 21.56s/it] 92%|█████████▏| 1161/1260 [7:02:51<35:30, 21.52s/it] 92%|█████████▏| 1162/1260 [7:03:12<35:06, 21.49s/it] 92%|█████████▏| 1163/1260 [7:03:34<34:44, 21.49s/it] 92%|█████████▏| 1164/1260 [7:03:55<34:28, 21.55s/it] 92%|█████████▏| 1165/1260 [7:04:17<34:04, 21.52s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.20298861984056735, 'learning_rate': 5.273706762564761e-07, 'epoch': 18.49}
 92%|█████████▏| 1165/1260 [7:04:17<34:04, 21.52s/it] 93%|█████████▎| 1166/1260 [7:04:38<33:43, 21.53s/it] 93%|█████████▎| 1167/1260 [7:05:00<33:32, 21.63s/it] 93%|█████████▎| 1168/1260 [7:05:22<33:04, 21.57s/it] 93%|█████████▎| 1169/1260 [7:05:43<32:42, 21.56s/it] 93%|█████████▎| 1170/1260 [7:06:05<32:18, 21.54s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.11315650152938168, 'learning_rate': 4.741505920829131e-07, 'epoch': 18.57}
 93%|█████████▎| 1170/1260 [7:06:05<32:18, 21.54s/it] 93%|█████████▎| 1171/1260 [7:06:26<31:53, 21.50s/it] 93%|█████████▎| 1172/1260 [7:06:48<31:35, 21.54s/it] 93%|█████████▎| 1173/1260 [7:07:09<31:18, 21.59s/it] 93%|█████████▎| 1174/1260 [7:07:31<30:57, 21.60s/it] 93%|█████████▎| 1175/1260 [7:07:53<30:48, 21.74s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.2207569305884125, 'learning_rate': 4.2371758023042604e-07, 'epoch': 18.65}
 93%|█████████▎| 1175/1260 [7:07:53<30:48, 21.74s/it] 93%|█████████▎| 1176/1260 [7:08:15<30:25, 21.73s/it] 93%|█████████▎| 1177/1260 [7:08:36<30:01, 21.70s/it] 93%|█████████▎| 1178/1260 [7:08:58<29:32, 21.62s/it] 94%|█████████▎| 1179/1260 [7:09:20<29:17, 21.70s/it] 94%|█████████▎| 1180/1260 [7:09:41<28:50, 21.63s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.23539712929152662, 'learning_rate': 3.760813172726457e-07, 'epoch': 18.73}
 94%|█████████▎| 1180/1260 [7:09:41<28:50, 21.63s/it] 94%|█████████▎| 1181/1260 [7:10:03<28:36, 21.73s/it] 94%|█████████▍| 1182/1260 [7:10:25<28:08, 21.65s/it] 94%|█████████▍| 1183/1260 [7:10:46<27:46, 21.65s/it] 94%|█████████▍| 1184/1260 [7:11:08<27:25, 21.65s/it] 94%|█████████▍| 1185/1260 [7:11:30<27:05, 21.68s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.13905886125763153, 'learning_rate': 3.312509431714661e-07, 'epoch': 18.81}
 94%|█████████▍| 1185/1260 [7:11:30<27:05, 21.68s/it] 94%|█████████▍| 1186/1260 [7:11:51<26:46, 21.70s/it] 94%|█████████▍| 1187/1260 [7:12:13<26:18, 21.63s/it] 94%|█████████▍| 1188/1260 [7:12:34<25:51, 21.55s/it] 94%|█████████▍| 1189/1260 [7:12:56<25:26, 21.51s/it] 94%|█████████▍| 1190/1260 [7:13:17<25:03, 21.48s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.1648242720566018, 'learning_rate': 2.892350595233406e-07, 'epoch': 18.89}
 94%|█████████▍| 1190/1260 [7:13:17<25:03, 21.48s/it] 95%|█████████▍| 1191/1260 [7:13:39<24:46, 21.54s/it] 95%|█████████▍| 1192/1260 [7:14:00<24:23, 21.52s/it] 95%|█████████▍| 1193/1260 [7:14:22<23:59, 21.48s/it] 95%|█████████▍| 1194/1260 [7:14:43<23:44, 21.58s/it] 95%|█████████▍| 1195/1260 [7:15:05<23:28, 21.68s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.43430507279276226, 'learning_rate': 2.50041727908909e-07, 'epoch': 18.97}
 95%|█████████▍| 1195/1260 [7:15:05<23:28, 21.68s/it] 95%|█████████▍| 1196/1260 [7:15:27<23:03, 21.62s/it] 95%|█████████▌| 1197/1260 [7:15:46<22:02, 20.99s/it][INFO|trainer.py:3993] 2025-07-05 04:46:08,184 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197
[INFO|configuration_utils.py:424] 2025-07-05 04:46:08,190 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/config.json
[INFO|configuration_utils.py:904] 2025-07-05 04:46:08,191 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 04:46:11,412 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:46:11,414 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:46:11,415 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:46:11,415 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/special_tokens_map.json
[2025-07-05 04:46:11,606] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1194 is about to be saved!
[2025-07-05 04:46:11,616] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 04:46:11,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 04:46:11,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 04:46:11,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 04:46:16,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 04:46:16,785] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/global_step1194/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 04:46:19,760] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1194 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 04:46:19,771 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 04:46:19,772 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 04:46:19,772 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 04:46:19,772 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 04:46:19,929 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 04:46:20,350 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1197/chat_template.jinja
 95%|█████████▌| 1198/1260 [7:16:24<26:55, 26.06s/it] 95%|█████████▌| 1199/1260 [7:16:46<25:11, 24.78s/it] 95%|█████████▌| 1200/1260 [7:17:07<23:46, 23.77s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.12904286747270374, 'learning_rate': 2.1367846834621952e-07, 'epoch': 19.05}
 95%|█████████▌| 1200/1260 [7:17:07<23:46, 23.77s/it] 95%|█████████▌| 1201/1260 [7:17:29<22:46, 23.15s/it] 95%|█████████▌| 1202/1260 [7:17:51<22:00, 22.76s/it] 95%|█████████▌| 1203/1260 [7:18:13<21:19, 22.44s/it] 96%|█████████▌| 1204/1260 [7:18:34<20:43, 22.21s/it] 96%|█████████▌| 1205/1260 [7:18:56<20:09, 21.99s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.10751831208490163, 'learning_rate': 1.8015225784786483e-07, 'epoch': 19.13}
 96%|█████████▌| 1205/1260 [7:18:56<20:09, 21.99s/it] 96%|█████████▌| 1206/1260 [7:19:17<19:39, 21.84s/it] 96%|█████████▌| 1207/1260 [7:19:39<19:16, 21.83s/it] 96%|█████████▌| 1208/1260 [7:20:01<18:48, 21.71s/it] 96%|█████████▌| 1209/1260 [7:20:22<18:22, 21.63s/it] 96%|█████████▌| 1210/1260 [7:20:43<17:59, 21.59s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.09862725102902124, 'learning_rate': 1.4946952908230448e-07, 'epoch': 19.21}
 96%|█████████▌| 1210/1260 [7:20:43<17:59, 21.59s/it] 96%|█████████▌| 1211/1260 [7:21:05<17:36, 21.57s/it] 96%|█████████▌| 1212/1260 [7:21:27<17:20, 21.67s/it] 96%|█████████▋| 1213/1260 [7:21:48<16:54, 21.58s/it] 96%|█████████▋| 1214/1260 [7:22:10<16:36, 21.66s/it] 96%|█████████▋| 1215/1260 [7:22:32<16:13, 21.63s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.13279570643079733, 'learning_rate': 1.2163616913962395e-07, 'epoch': 19.29}
 96%|█████████▋| 1215/1260 [7:22:32<16:13, 21.63s/it] 97%|█████████▋| 1216/1260 [7:22:53<15:50, 21.61s/it] 97%|█████████▋| 1217/1260 [7:23:15<15:27, 21.58s/it] 97%|█████████▋| 1218/1260 [7:23:37<15:09, 21.65s/it] 97%|█████████▋| 1219/1260 [7:23:58<14:49, 21.69s/it] 97%|█████████▋| 1220/1260 [7:24:20<14:30, 21.77s/it]                                                     {'loss': 0.0009, 'grad_norm': 0.1493849948047526, 'learning_rate': 9.6657518401988e-08, 'epoch': 19.37}
 97%|█████████▋| 1220/1260 [7:24:20<14:30, 21.77s/it] 97%|█████████▋| 1221/1260 [7:24:42<14:04, 21.66s/it] 97%|█████████▋| 1222/1260 [7:25:03<13:40, 21.59s/it] 97%|█████████▋| 1223/1260 [7:25:25<13:19, 21.62s/it] 97%|█████████▋| 1224/1260 [7:25:46<12:56, 21.58s/it] 97%|█████████▋| 1225/1260 [7:26:08<12:40, 21.72s/it]                                                     {'loss': 0.0015, 'grad_norm': 0.22443540030430398, 'learning_rate': 7.453836951897885e-08, 'epoch': 19.45}
 97%|█████████▋| 1225/1260 [7:26:08<12:40, 21.72s/it] 97%|█████████▋| 1226/1260 [7:26:30<12:15, 21.64s/it] 97%|█████████▋| 1227/1260 [7:26:51<11:52, 21.59s/it] 97%|█████████▋| 1228/1260 [7:27:13<11:30, 21.59s/it] 98%|█████████▊| 1229/1260 [7:27:35<11:11, 21.67s/it] 98%|█████████▊| 1230/1260 [7:27:56<10:49, 21.64s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.0886604012131861, 'learning_rate': 5.528296648803166e-08, 'epoch': 19.52}
 98%|█████████▊| 1230/1260 [7:27:56<10:49, 21.64s/it] 98%|█████████▊| 1231/1260 [7:28:18<10:26, 21.59s/it] 98%|█████████▊| 1232/1260 [7:28:39<10:03, 21.56s/it] 98%|█████████▊| 1233/1260 [7:29:01<09:45, 21.68s/it] 98%|█████████▊| 1234/1260 [7:29:23<09:23, 21.66s/it] 98%|█████████▊| 1235/1260 [7:29:44<09:00, 21.60s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.13734033255527855, 'learning_rate': 3.889500384013755e-08, 'epoch': 19.6}
 98%|█████████▊| 1235/1260 [7:29:44<09:00, 21.60s/it] 98%|█████████▊| 1236/1260 [7:30:06<08:37, 21.58s/it] 98%|█████████▊| 1237/1260 [7:30:27<08:16, 21.61s/it] 98%|█████████▊| 1238/1260 [7:30:49<07:55, 21.61s/it] 98%|█████████▊| 1239/1260 [7:31:11<07:33, 21.58s/it] 98%|█████████▊| 1240/1260 [7:31:32<07:13, 21.65s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.10767596105409218, 'learning_rate': 2.5377625930977367e-08, 'epoch': 19.68}
 98%|█████████▊| 1240/1260 [7:31:32<07:13, 21.65s/it] 98%|█████████▊| 1241/1260 [7:31:54<06:52, 21.69s/it] 99%|█████████▊| 1242/1260 [7:32:16<06:32, 21.79s/it] 99%|█████████▊| 1243/1260 [7:32:38<06:10, 21.77s/it] 99%|█████████▊| 1244/1260 [7:33:00<05:48, 21.76s/it] 99%|█████████▉| 1245/1260 [7:33:21<05:24, 21.66s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.1680823664644058, 'learning_rate': 1.4733426337610877e-08, 'epoch': 19.76}
 99%|█████████▉| 1245/1260 [7:33:21<05:24, 21.66s/it] 99%|█████████▉| 1246/1260 [7:33:43<05:02, 21.59s/it] 99%|█████████▉| 1247/1260 [7:34:04<04:40, 21.57s/it] 99%|█████████▉| 1248/1260 [7:34:25<04:18, 21.53s/it] 99%|█████████▉| 1249/1260 [7:34:47<03:57, 21.58s/it] 99%|█████████▉| 1250/1260 [7:35:09<03:35, 21.60s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.10587975982484749, 'learning_rate': 6.964447360853221e-09, 'epoch': 19.84}
 99%|█████████▉| 1250/1260 [7:35:09<03:35, 21.60s/it] 99%|█████████▉| 1251/1260 [7:35:31<03:14, 21.64s/it] 99%|█████████▉| 1252/1260 [7:35:53<02:53, 21.74s/it] 99%|█████████▉| 1253/1260 [7:36:14<02:32, 21.72s/it]100%|█████████▉| 1254/1260 [7:36:36<02:10, 21.72s/it]100%|█████████▉| 1255/1260 [7:36:57<01:48, 21.66s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.11722096054322337, 'learning_rate': 2.0721796334149945e-09, 'epoch': 19.92}
100%|█████████▉| 1255/1260 [7:36:57<01:48, 21.66s/it]100%|█████████▉| 1256/1260 [7:37:19<01:26, 21.60s/it]100%|█████████▉| 1257/1260 [7:37:40<01:04, 21.58s/it]100%|█████████▉| 1258/1260 [7:38:02<00:43, 21.53s/it]100%|█████████▉| 1259/1260 [7:38:23<00:21, 21.55s/it]100%|██████████| 1260/1260 [7:38:43<00:00, 20.94s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.19787839730289297, 'learning_rate': 5.756183389271641e-11, 'epoch': 20.0}
100%|██████████| 1260/1260 [7:38:43<00:00, 20.94s/it][INFO|trainer.py:3993] 2025-07-05 05:09:04,749 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260
[INFO|configuration_utils.py:424] 2025-07-05 05:09:04,755 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/config.json
[INFO|configuration_utils.py:904] 2025-07-05 05:09:04,756 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 05:09:07,926 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:09:07,928 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:09:07,928 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:09:07,929 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/special_tokens_map.json
[2025-07-05 05:09:08,118] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1257 is about to be saved!
[2025-07-05 05:09:08,129] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-07-05 05:09:08,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-07-05 05:09:08,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-07-05 05:09:08,165] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-07-05 05:09:13,551] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-07-05 05:09:13,553] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/global_step1257/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-07-05 05:09:16,264] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1257 is ready now!
[INFO|image_processing_base.py:260] 2025-07-05 05:09:16,275 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:09:16,276 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:09:16,276 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:09:16,276 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 05:09:16,432 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 05:09:16,837 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/checkpoint-1260/chat_template.jinja
[INFO|trainer.py:2676] 2025-07-05 05:09:16,838 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 27538.6777, 'train_samples_per_second': 11.683, 'train_steps_per_second': 0.046, 'train_loss': 0.14027680895875194, 'epoch': 20.0}
100%|██████████| 1260/1260 [7:38:57<00:00, 20.94s/it]100%|██████████| 1260/1260 [7:38:57<00:00, 21.86s/it]
[INFO|image_processing_base.py:260] 2025-07-05 05:09:16,844 >> Image processor saved in saves/qwen2_vl-3b/vindr_sft_base/preprocessor_config.json
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:09:16,845 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:09:16,845 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:09:16,846 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/special_tokens_map.json
[INFO|video_processing_utils.py:491] 2025-07-05 05:09:16,998 >> Video processor saved in saves/qwen2_vl-3b/vindr_sft_base/video_preprocessor_config.json
[INFO|processing_utils.py:674] 2025-07-05 05:09:17,400 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/chat_template.jinja
[INFO|trainer.py:3993] 2025-07-05 05:09:18,239 >> Saving model checkpoint to saves/qwen2_vl-3b/vindr_sft_base
[INFO|configuration_utils.py:424] 2025-07-05 05:09:18,244 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/config.json
[INFO|configuration_utils.py:904] 2025-07-05 05:09:18,245 >> Configuration saved in saves/qwen2_vl-3b/vindr_sft_base/generation_config.json
[INFO|modeling_utils.py:3725] 2025-07-05 05:09:20,042 >> Model weights saved in saves/qwen2_vl-3b/vindr_sft_base/model.safetensors
[INFO|tokenization_utils_base.py:2356] 2025-07-05 05:09:20,043 >> chat template saved in saves/qwen2_vl-3b/vindr_sft_base/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-05 05:09:20,044 >> tokenizer config file saved in saves/qwen2_vl-3b/vindr_sft_base/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-05 05:09:20,044 >> Special tokens file saved in saves/qwen2_vl-3b/vindr_sft_base/special_tokens_map.json
***** train metrics *****
  epoch                    =       20.0
  total_flos               =  1924347GF
  train_loss               =     0.1403
  train_runtime            = 7:38:58.67
  train_samples_per_second =     11.683
  train_steps_per_second   =      0.046
Figure saved at: saves/qwen2_vl-3b/vindr_sft_base/training_loss.png
[WARNING|2025-07-05 05:09:20] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-07-05 05:09:20] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:450] 2025-07-05 05:09:20,844 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33msaves/qwen2_vl-3b/vindr_sft_base[0m at: [34mhttps://wandb.ai/compai/llamafactory/runs/rt74h6lh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250704_213018-rt74h6lh/logs[0m
### model
model_name_or_path: /home/june/Code/new_llamafactory/saves/huggingface_origin/Qwen2-VL-2B-Instruct/
image_max_pixels: 12845056
image_min_pixels: 3136
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
# freeze_vision_tower: true
# freeze_multi_modal_projector: true
# freeze_language_model: false
freeze_vision_tower: false
freeze_multi_modal_projector: false
freeze_language_model: false
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
dataset: vinder_train_base
template: qwen2_vl
cutoff_len: 1024
max_samples: 100000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen2_vl-3b/vindr_sft_base
logging_steps: 5
save_steps: 63
plot_loss: true
overwrite_output_dir: saves/qwen2_vl-3b/vindr_sft_base
save_only_model: false
report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
learning_rate: 3.0e-5
num_train_epochs: 20.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500
