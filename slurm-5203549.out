Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Mon Jun 30 20:45:33 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |
| N/A   37C    P0              61W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Checkpoint directory: saves/qwen2_vl-3b/vindr_sft_base
Output directory: ./evaluate_outputs/train_results/vinder_adkg_test__20250630_204534/vindr_sft_base/qwen2_vl-3b
Processing model: qwen2_vl-3b
Running inference on checkpoint: checkpoint-126
INFO 06-30 20:45:54 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:45:57,283 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:45:57,333 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,381 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:45:57,768 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:45:57,770 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:45:57,775 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:45:57,782 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,782 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:45:57,783 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:45:58,158 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:45:58,160 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:45:58,348 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:45:58,797 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:45:58,854 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:45:58,854 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:45:58,856 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:46:11 [config.py:689] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 06-30 20:46:11 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,094 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:46:13,096 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:46:13,440 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:46:13,534 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:46:13,535 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:46:13 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:46:21 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:46:23 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:46:28 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1fb171a750>
INFO 06-30 20:46:29 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:46:29 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:46:32 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-126...
WARNING 06-30 20:46:32 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:46:32 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:46:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.49s/it]

INFO 06-30 20:46:40 [loader.py:458] Loading weights took 7.76 seconds
INFO 06-30 20:46:41 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 8.442105 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:46:42 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:47:05 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/746c31169e/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:47:05 [backends.py:426] Dynamo bytecode transform time: 14.25 s
INFO 06-30 20:47:10 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:47:35 [backends.py:144] Compiling a graph for general shape takes 29.89 s
INFO 06-30 20:47:49 [monitor.py:33] torch.compile takes 44.14 s in total
INFO 06-30 20:47:50 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:47:50 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:48:14 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:48:14 [core.py:163] init engine (profile, create kv cache, warmup model) took 93.79 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:48:16 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:48:16] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Converting format of dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 88/2108 [00:00<00:02, 724.80 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 2108/2108 [00:00<00:00, 7857.70 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 189.87 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 518.09 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 757.93 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 910.46 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 973.79 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1073.84 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1214.05 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:01<00:00, 1341.32 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1339.52 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 942.77 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fa84f18cea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:48:23.678748482 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-189
INFO 06-30 20:48:34 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:48:36,653 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:48:36,678 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,702 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,702 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,702 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,702 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,702 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,703 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:36,703 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:48:37,091 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:48:37,093 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:48:37,102 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:48:37,110 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:37,111 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:48:37,486 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:48:37,489 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:48:37,698 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:48:38,146 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:48:38,204 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:48:38,204 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:48:38,206 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:48:49 [config.py:689] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
INFO 06-30 20:48:49 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,734 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:48:50,736 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:48:51,076 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:48:51,167 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:48:51,167 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:48:51 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:48:58 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:49:01 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:49:02 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f144d1614d0>
INFO 06-30 20:49:03 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:49:03 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:49:06 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-189...
WARNING 06-30 20:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:49:06 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.34s/it]

INFO 06-30 20:49:13 [loader.py:458] Loading weights took 7.57 seconds
INFO 06-30 20:49:14 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.805672 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:49:15 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:49:29 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/c6c942b80a/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:49:29 [backends.py:426] Dynamo bytecode transform time: 7.39 s
INFO 06-30 20:49:33 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:49:58 [backends.py:144] Compiling a graph for general shape takes 28.56 s
INFO 06-30 20:50:12 [monitor.py:33] torch.compile takes 35.96 s in total
INFO 06-30 20:50:12 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:50:12 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:50:36 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:50:36 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.36 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:50:37 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:50:37] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 186.43 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:04, 372.40 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 553.20 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 829.38 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 927.56 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 999.93 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:00, 1064.01 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1166.68 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1297.45 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:01<00:00, 1363.77 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1416.07 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 940.73 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fe4b67ccea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:50:42.727244366 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-252
INFO 06-30 20:50:53 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:50:55,364 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:50:55,410 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,434 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,435 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:50:55,822 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:50:55,824 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:50:55,830 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:50:55,837 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:50:55,837 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:50:56,210 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:50:56,212 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:50:56,421 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:50:56,870 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:50:56,929 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:50:56,929 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:50:56,931 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:51:08 [config.py:689] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 06-30 20:51:08 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,467 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:51:09,469 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:51:09,805 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:51:09,900 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:51:09,901 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:51:09 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:51:17 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:51:20 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:51:21 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2cd8779450>
INFO 06-30 20:51:21 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:51:21 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:51:24 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-252...
WARNING 06-30 20:51:24 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:51:24 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:51:24 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.23s/it]

INFO 06-30 20:51:32 [loader.py:458] Loading weights took 7.45 seconds
INFO 06-30 20:51:32 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.637727 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:51:33 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:51:47 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/f97c2c3d76/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:51:47 [backends.py:426] Dynamo bytecode transform time: 7.36 s
INFO 06-30 20:51:51 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:52:16 [backends.py:144] Compiling a graph for general shape takes 28.51 s
INFO 06-30 20:52:30 [monitor.py:33] torch.compile takes 35.87 s in total
INFO 06-30 20:52:31 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:52:31 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:52:54 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:52:54 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.18 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:52:55 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:52:55] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 184.28 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 361.27 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 540.40 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 790.29 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 878.99 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 938.11 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 996.21 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1011.08 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1452/2108 [00:01<00:00, 1354.80 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1715/2108 [00:01<00:00, 1345.74 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 1290.19 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 894.12 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fc489dfcea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:53:00.100838044 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-315
INFO 06-30 20:53:11 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:53:13,638 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:53:13,688 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,711 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:13,712 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:53:14,097 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:53:14,099 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:53:14,105 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:53:14,111 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:14,112 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:53:14,491 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:53:14,493 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:53:14,702 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:53:15,141 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:53:15,195 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:53:15,195 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:53:15,197 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:53:26 [config.py:689] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 06-30 20:53:26 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,594 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,595 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,606 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,607 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,607 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,607 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:53:27,607 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:53:27,949 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:53:28,039 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:53:28,039 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:53:28 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:53:35 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:53:38 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:53:39 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb99512ed10>
INFO 06-30 20:53:39 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:53:39 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:53:42 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-315...
WARNING 06-30 20:53:42 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:53:42 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:53:42 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.32s/it]

INFO 06-30 20:53:50 [loader.py:458] Loading weights took 7.54 seconds
INFO 06-30 20:53:50 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.733376 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:53:51 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:54:06 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/64740082b7/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:54:06 [backends.py:426] Dynamo bytecode transform time: 7.41 s
INFO 06-30 20:54:09 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:54:34 [backends.py:144] Compiling a graph for general shape takes 28.42 s
INFO 06-30 20:54:48 [monitor.py:33] torch.compile takes 35.83 s in total
INFO 06-30 20:54:49 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:54:49 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:55:12 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:55:12 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.20 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:55:14 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:55:14] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 188.60 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 320.03 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 697.24 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 785.57 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 872.16 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 957.26 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1087.78 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1116.20 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1301.27 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1311.22 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1397.68 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 917.56 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fd20507cea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:55:19.237231057 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-378
INFO 06-30 20:55:30 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:55:32,021 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:55:32,074 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,098 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:55:32,485 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:55:32,487 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:55:32,492 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:55:32,499 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:32,500 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:55:32,871 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:55:32,874 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:55:33,077 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:55:33,532 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:55:33,582 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:55:33,582 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:55:33,584 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:55:45 [config.py:689] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 06-30 20:55:45 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,096 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:55:46,097 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:55:46,423 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:55:46,526 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:55:46,529 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:55:46 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:55:54 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:55:56 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:55:57 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fcb12a2d390>
INFO 06-30 20:55:58 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:55:58 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:56:01 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-378...
WARNING 06-30 20:56:01 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:56:01 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:56:01 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.34s/it]

INFO 06-30 20:56:09 [loader.py:458] Loading weights took 7.55 seconds
INFO 06-30 20:56:09 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.764260 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:56:10 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:56:24 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/e9c7653ba5/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:56:24 [backends.py:426] Dynamo bytecode transform time: 7.38 s
INFO 06-30 20:56:28 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:56:53 [backends.py:144] Compiling a graph for general shape takes 28.36 s
INFO 06-30 20:57:07 [monitor.py:33] torch.compile takes 35.74 s in total
INFO 06-30 20:57:07 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:57:07 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:57:31 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:57:31 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.24 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:57:32 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:57:32] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 180.87 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 364.93 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 544.83 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 694.74 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 837.75 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 938.74 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 1000.54 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1049.95 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1090.20 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1129.36 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1402.82 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1410.98 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1387.78 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 911.05 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fcab570cea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:57:37.818803793 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-441
INFO 06-30 20:57:48 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:57:50,670 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:57:50,730 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:50,753 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:57:51,141 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:57:51,143 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:57:51,149 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:57:51,155 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,164 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,174 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,175 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,176 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,176 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,185 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:51,186 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:57:51,557 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:57:51,561 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:57:51,769 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:57:52,223 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:57:52,275 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:57:52,276 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:57:52,278 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:58:03 [config.py:689] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.
INFO 06-30 20:58:03 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,761 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:04,762 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:58:05,091 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:58:05,192 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:58:05,195 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:58:05 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:58:12 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:58:15 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:58:16 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f773191e510>
INFO 06-30 20:58:17 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:58:17 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:58:19 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-441...
WARNING 06-30 20:58:20 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:58:20 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:58:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.36s/it]

INFO 06-30 20:58:27 [loader.py:458] Loading weights took 7.57 seconds
INFO 06-30 20:58:28 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.830806 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:58:29 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:58:43 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/9c6bff6e45/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:58:43 [backends.py:426] Dynamo bytecode transform time: 7.45 s
INFO 06-30 20:58:47 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:59:12 [backends.py:144] Compiling a graph for general shape takes 28.55 s
INFO 06-30 20:59:26 [monitor.py:33] torch.compile takes 36.00 s in total
INFO 06-30 20:59:26 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:59:26 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 20:59:50 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 20:59:50 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.34 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:59:51 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 20:59:51] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 185.32 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:04, 374.34 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:01<00:03, 456.66 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 834.12 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 941.40 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1217.94 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1452/2108 [00:01<00:00, 1188.74 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1715/2108 [00:01<00:00, 1180.10 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 1230.27 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 912.53 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f646cef4ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 20:59:56.706634351 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-504
INFO 06-30 21:00:07 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:00:09,268 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:00:09,319 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,343 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:09,730 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:00:09,733 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:00:09,738 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:00:09,745 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:09,746 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:10,121 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:00:10,124 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:00:10,333 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:00:10,779 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:00:10,837 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:00:10,837 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:00:10,839 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:00:22 [config.py:689] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.
INFO 06-30 21:00:22 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,354 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:23,355 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:23,698 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:00:23,787 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:00:23,787 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:00:23 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:00:31 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:00:34 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:00:34 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f22f3bdf1d0>
INFO 06-30 21:00:35 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:00:35 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:00:38 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-504...
WARNING 06-30 21:00:38 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:00:38 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:00:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.36s/it]

INFO 06-30 21:00:46 [loader.py:458] Loading weights took 7.57 seconds
INFO 06-30 21:00:46 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.855112 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:00:47 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:01:01 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/cd7ab86862/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:01:01 [backends.py:426] Dynamo bytecode transform time: 7.42 s
INFO 06-30 21:01:05 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:01:30 [backends.py:144] Compiling a graph for general shape takes 28.60 s
INFO 06-30 21:01:44 [monitor.py:33] torch.compile takes 36.02 s in total
INFO 06-30 21:01:45 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:01:45 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:02:08 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:02:08 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.36 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:02:10 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:02:10] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:11, 177.79 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 357.15 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 531.88 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 680.50 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 805.76 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 903.54 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 991.68 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:00, 1057.13 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1105.65 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1145.50 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1359.90 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1433.89 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1385.64 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 901.59 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fe514ae4ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:02:15.268181179 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-567
INFO 06-30 21:02:26 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:02:28,455 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:02:29,053 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,078 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:02:29,468 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:02:29,470 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:02:29,475 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:02:29,482 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,482 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,482 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,483 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,483 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,483 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,483 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:29,483 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:02:29,857 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:02:29,861 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:02:30,070 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:02:30,520 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:02:30,576 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:02:30,576 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:02:30,578 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:02:42 [config.py:689] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.
INFO 06-30 21:02:42 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,054 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:43,055 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:02:43,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:02:43,488 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:02:43,488 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:02:43 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:02:51 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:02:53 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:02:54 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb65e061cd0>
INFO 06-30 21:02:55 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:02:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:02:58 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-567...
WARNING 06-30 21:02:58 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:02:58 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:02:58 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.35s/it]

INFO 06-30 21:03:06 [loader.py:458] Loading weights took 7.55 seconds
INFO 06-30 21:03:06 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.824752 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:03:07 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:03:21 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/854303e6e8/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:03:21 [backends.py:426] Dynamo bytecode transform time: 7.41 s
INFO 06-30 21:03:25 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:03:50 [backends.py:144] Compiling a graph for general shape takes 28.77 s
INFO 06-30 21:04:04 [monitor.py:33] torch.compile takes 36.18 s in total
INFO 06-30 21:04:05 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:04:05 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:04:28 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:04:28 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.54 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:04:29 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:04:29] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 187.99 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 511.69 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 636.05 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 748.97 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 832.00 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:00, 1189.98 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1053.24 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1256.58 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1228.33 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1353.99 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 924.02 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f48c3228ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:04:34.057615628 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-63
INFO 06-30 21:04:45 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:04:47,784 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:04:47,828 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:47,851 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:04:48,235 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:04:48,239 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:04:48,244 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:04:48,251 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:04:48,251 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:04:48,624 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:04:48,627 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:04:48,835 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:04:49,283 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:04:49,335 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:04:49,336 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:04:49,338 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:05:00 [config.py:689] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
INFO 06-30 21:05:00 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,811 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,812 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,813 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,813 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,813 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,813 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:01,813 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:05:02,138 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:05:02,240 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:05:02,243 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:05:02 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:05:09 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:05:12 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:05:13 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f035193ca10>
INFO 06-30 21:05:14 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:05:14 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:05:16 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-63...
WARNING 06-30 21:05:17 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:05:17 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:05:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.30s/it]

INFO 06-30 21:05:24 [loader.py:458] Loading weights took 7.51 seconds
INFO 06-30 21:05:25 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.869035 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:05:26 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:05:40 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/f76142d6b1/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:05:40 [backends.py:426] Dynamo bytecode transform time: 7.39 s
INFO 06-30 21:05:44 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:06:09 [backends.py:144] Compiling a graph for general shape takes 28.73 s
INFO 06-30 21:06:23 [monitor.py:33] torch.compile takes 36.12 s in total
INFO 06-30 21:06:24 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:06:24 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:06:48 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:06:48 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.89 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:06:49 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:06:49] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 183.98 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 365.37 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 542.59 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 802.80 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 887.89 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 950.87 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1011.80 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1062.46 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1452/2108 [00:01<00:00, 1331.63 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1715/2108 [00:01<00:00, 1243.95 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 1081.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1052.36 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 844.43 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7efb81c14ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:06:54.772915019 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-630
INFO 06-30 21:07:05 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:07:07,196 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:07:07,249 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,273 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:07,658 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:07:07,661 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:07:07,666 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:07:07,673 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:07,674 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:08,046 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:07:08,049 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:07:08,259 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:07:08,705 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:07:08,761 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:07:08,762 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:07:08,764 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:07:20 [config.py:689] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.
INFO 06-30 21:07:20 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,268 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:21,269 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:21,609 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:07:21,706 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:07:21,707 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:07:21 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:07:29 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:07:32 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:07:33 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd6d2317050>
INFO 06-30 21:07:33 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:07:33 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:07:36 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_base/checkpoint-630...
WARNING 06-30 21:07:36 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:07:36 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:07:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.76s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.76s/it]

INFO 06-30 21:07:45 [loader.py:458] Loading weights took 8.01 seconds
INFO 06-30 21:07:45 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 8.208659 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:07:47 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:08:01 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/700b140be9/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:08:01 [backends.py:426] Dynamo bytecode transform time: 7.45 s
INFO 06-30 21:08:05 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:08:30 [backends.py:144] Compiling a graph for general shape takes 28.69 s
INFO 06-30 21:08:44 [monitor.py:33] torch.compile takes 36.14 s in total
INFO 06-30 21:08:45 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:08:45 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:09:09 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:09:09 [core.py:163] init engine (profile, create kv cache, warmup model) took 83.76 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:09:10 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:09:10] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 181.15 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 361.01 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 528.80 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 675.73 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 800.79 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:00, 1229.11 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1030.40 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1452/2108 [00:01<00:00, 1216.71 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1715/2108 [00:01<00:00, 1260.40 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 1194.32 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 858.26 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f7d361b8ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:09:15.893289376 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
