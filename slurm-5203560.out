Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Mon Jun 30 20:57:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |
| N/A   40C    P0              65W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Checkpoint directory: saves/qwen2_vl-3b/vindr_sft_def
Output directory: ./evaluate_outputs/train_results/vinder_adkg_def_test__20250630_205743/vindr_sft_def/qwen2_vl-3b
Processing model: qwen2_vl-3b
Running inference on checkpoint: checkpoint-126
INFO 06-30 20:57:55 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 20:57:58,133 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 20:57:58,184 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,221 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:57:58,601 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 20:57:58,603 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 20:57:58,608 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 20:57:58,615 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:57:58,616 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:57:58,989 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 20:57:58,992 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 20:57:59,200 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 20:57:59,641 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 20:57:59,696 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/config.json
[INFO|configuration_utils.py:696] 2025-06-30 20:57:59,697 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/config.json
[INFO|configuration_utils.py:770] 2025-06-30 20:57:59,699 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 20:58:11 [config.py:689] This model supports multiple tasks: {'score', 'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 06-30 20:58:11 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,054 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,054 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,054 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,055 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,055 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,055 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 20:58:13,055 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 20:58:13,400 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 20:58:13,487 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 20:58:13,488 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 20:58:13 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 20:58:21 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 20:58:24 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 20:58:26 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7275068790>
INFO 06-30 20:58:27 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 20:58:27 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:58:30 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-126...
WARNING 06-30 20:58:30 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 20:58:30 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 20:58:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.22s/it]

INFO 06-30 20:58:38 [loader.py:458] Loading weights took 7.43 seconds
INFO 06-30 20:58:38 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.930294 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 20:58:39 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 20:58:58 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/5d5c802e17/rank_0_0 for vLLM's torch.compile
INFO 06-30 20:58:58 [backends.py:426] Dynamo bytecode transform time: 11.58 s
INFO 06-30 20:59:03 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 20:59:28 [backends.py:144] Compiling a graph for general shape takes 29.54 s
INFO 06-30 20:59:42 [monitor.py:33] torch.compile takes 41.12 s in total
INFO 06-30 20:59:42 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 20:59:42 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:00:06 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:00:06 [core.py:163] init engine (profile, create kv cache, warmup model) took 88.58 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:00:08 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:00:08] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2108 examples [00:00, 52305.36 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  25%|██▌       | 528/2108 [00:00<00:00, 5030.33 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 2108/2108 [00:00<00:00, 9124.22 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 184.08 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 361.23 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 529.59 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 808.31 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 964.05 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1005.74 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1023.51 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1060.36 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1384.86 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1319.78 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1370.28 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 914.31 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f4100544ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:00:14.514845719 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-189
INFO 06-30 21:00:26 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:00:28,020 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:00:28,078 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,102 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:28,486 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:00:28,488 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:00:28,494 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:00:28,501 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:28,502 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:28,869 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:00:28,872 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:00:29,077 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:00:29,543 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:00:29,596 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:00:29,596 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:00:29,598 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:00:41 [config.py:689] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 06-30 21:00:41 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,327 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,327 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,328 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,328 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,328 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,328 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:00:42,328 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:00:42,680 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:00:42,781 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:00:42,783 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:00:42 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:00:50 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:00:53 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:00:54 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1e1157b9d0>
INFO 06-30 21:00:54 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:00:54 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:00:57 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-189...
WARNING 06-30 21:00:57 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:00:57 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:00:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.23s/it]

INFO 06-30 21:01:05 [loader.py:458] Loading weights took 7.44 seconds
INFO 06-30 21:01:05 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.693328 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:01:06 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:01:20 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/be8d064123/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:01:20 [backends.py:426] Dynamo bytecode transform time: 7.44 s
INFO 06-30 21:01:24 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:01:49 [backends.py:144] Compiling a graph for general shape takes 28.65 s
INFO 06-30 21:02:03 [monitor.py:33] torch.compile takes 36.10 s in total
INFO 06-30 21:02:04 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:02:04 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:02:28 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:02:28 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.97 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:02:29 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:02:29] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 179.66 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 351.30 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 517.52 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 670.69 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 903.29 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1023.18 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1034.79 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1069.87 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1341.76 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1330.84 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1282.97 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 888.14 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7eff3e584ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:02:34.677174094 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-252
INFO 06-30 21:02:45 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:02:47,640 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:02:47,670 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:47,694 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:02:48,096 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:02:48,099 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:02:48,104 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:02:48,111 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:02:48,112 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:02:48,483 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:02:48,486 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:02:48,697 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:02:49,141 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:02:49,197 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:02:49,197 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:02:49,199 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:03:00 [config.py:689] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 06-30 21:03:00 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:03:01,777 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:03:02,146 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:03:02,234 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:03:02,234 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:03:02 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:03:09 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:03:12 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:03:13 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4edf192050>
INFO 06-30 21:03:13 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:03:13 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:03:16 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-252...
WARNING 06-30 21:03:16 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:03:17 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:03:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.23s/it]

INFO 06-30 21:03:24 [loader.py:458] Loading weights took 7.43 seconds
INFO 06-30 21:03:24 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.704715 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:03:26 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:03:40 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/0bf178a72d/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:03:40 [backends.py:426] Dynamo bytecode transform time: 7.56 s
INFO 06-30 21:03:44 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:04:09 [backends.py:144] Compiling a graph for general shape takes 28.96 s
INFO 06-30 21:04:23 [monitor.py:33] torch.compile takes 36.53 s in total
INFO 06-30 21:04:24 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:04:24 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:04:48 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:04:48 [core.py:163] init engine (profile, create kv cache, warmup model) took 83.14 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:04:49 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:04:49] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 185.30 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 368.65 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 528.02 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 677.59 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 1085.16 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 997.56 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1032.21 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1044.84 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1298.54 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1311.25 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1310.48 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 905.85 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fb181becea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:04:54.355072337 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-315
INFO 06-30 21:05:05 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:05:07,382 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:05:07,408 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,432 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:05:07,831 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:05:07,833 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:05:07,838 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:05:07,845 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:07,846 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:05:08,218 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:05:08,220 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:05:08,431 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:05:08,873 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:05:08,930 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:05:08,930 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:05:08,932 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:05:20 [config.py:689] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.
INFO 06-30 21:05:20 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:05:21,430 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:05:21,802 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:05:21,889 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:05:21,889 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:05:21 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:05:29 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:05:32 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:05:33 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f08cdfdb850>
INFO 06-30 21:05:33 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:05:33 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:05:36 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-315...
WARNING 06-30 21:05:36 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:05:36 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:05:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.18s/it]

INFO 06-30 21:05:44 [loader.py:458] Loading weights took 7.38 seconds
INFO 06-30 21:05:44 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.620131 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:05:45 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:06:00 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/b1c7579c1e/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:06:00 [backends.py:426] Dynamo bytecode transform time: 7.46 s
INFO 06-30 21:06:03 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:06:28 [backends.py:144] Compiling a graph for general shape takes 28.52 s
INFO 06-30 21:06:42 [monitor.py:33] torch.compile takes 35.98 s in total
INFO 06-30 21:06:43 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:06:43 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:07:07 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:07:07 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.31 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:07:08 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:07:08] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 180.84 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 349.95 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 515.79 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 894.33 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 1022.84 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1023.76 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1067.41 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 868.33 examples/s] Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1208.86 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1197.28 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1237.44 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 875.76 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f0172698ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:07:13.316650998 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-378
INFO 06-30 21:07:24 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:07:26,294 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:07:26,360 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,383 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,383 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,383 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,383 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,383 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,384 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,384 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:26,784 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:07:26,786 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:07:26,792 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:07:26,798 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:26,799 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:27,173 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:07:27,176 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:07:27,390 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:07:27,857 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:07:27,918 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:07:27,919 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:07:27,921 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:07:39 [config.py:689] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 06-30 21:07:39 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,514 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:07:40,515 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:07:40,850 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:07:40,943 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:07:40,944 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:07:40 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:07:48 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:07:51 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:07:52 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6f879ed7d0>
INFO 06-30 21:07:52 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:07:52 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:07:55 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-378...
WARNING 06-30 21:07:55 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:07:55 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:07:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.26s/it]

INFO 06-30 21:08:03 [loader.py:458] Loading weights took 7.46 seconds
INFO 06-30 21:08:03 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.700008 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:08:04 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:08:19 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/a5699b0b28/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:08:19 [backends.py:426] Dynamo bytecode transform time: 7.64 s
INFO 06-30 21:08:22 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:08:48 [backends.py:144] Compiling a graph for general shape takes 29.14 s
INFO 06-30 21:09:02 [monitor.py:33] torch.compile takes 36.78 s in total
INFO 06-30 21:09:03 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:09:03 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:09:27 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:09:27 [core.py:163] init engine (profile, create kv cache, warmup model) took 83.48 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:09:28 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:09:28] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:11, 178.03 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 348.40 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 512.18 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 660/2108 [00:01<00:01, 899.32 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 1008.79 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 1012.02 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1043.26 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 895.67 examples/s] Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1255.76 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1118.78 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1275.29 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 877.13 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f4a7e9c0ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:09:33.473887636 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-441
INFO 06-30 21:09:44 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:09:46,462 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:09:46,497 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,521 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,522 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:09:46,923 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:09:46,925 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:09:46,930 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:09:46,937 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,937 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:09:46,938 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:09:47,312 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:09:47,315 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:09:47,526 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:09:47,967 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:09:48,021 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:09:48,021 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:09:48,023 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:09:59 [config.py:689] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 06-30 21:09:59 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:10:00,465 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:10:00,827 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:10:00,921 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:10:00,922 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:10:00 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:10:08 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:10:11 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:10:12 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6dbd0fbb50>
INFO 06-30 21:10:12 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:10:12 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:10:15 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-441...
WARNING 06-30 21:10:15 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:10:17 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:10:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.11s/it]

INFO 06-30 21:10:26 [loader.py:458] Loading weights took 7.32 seconds
INFO 06-30 21:10:26 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 10.410735 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:10:27 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:10:41 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/ea5b8b7d51/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:10:41 [backends.py:426] Dynamo bytecode transform time: 7.55 s
INFO 06-30 21:10:45 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:11:11 [backends.py:144] Compiling a graph for general shape takes 28.95 s
INFO 06-30 21:11:24 [monitor.py:33] torch.compile takes 36.51 s in total
INFO 06-30 21:11:25 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:11:25 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:11:49 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:11:49 [core.py:163] init engine (profile, create kv cache, warmup model) took 83.35 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:11:50 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:11:50] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 183.83 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 359.80 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 532.02 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 684.65 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:00, 1185.67 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 987.19 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1030.26 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1307.59 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:01<00:00, 1341.10 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1292.72 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 913.52 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fbf8eda4ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:11:55.865483456 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-504
INFO 06-30 21:12:07 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:12:09,365 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:12:09,432 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,485 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:12:09,866 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:12:09,868 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:12:09,873 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:12:09,880 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:09,881 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:12:10,255 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:12:10,258 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:12:10,487 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:12:10,932 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:12:10,987 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:12:10,987 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:12:10,990 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:12:22 [config.py:689] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 06-30 21:12:22 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:12:23,746 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:12:24,097 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:12:24,188 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:12:24,189 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:12:24 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:12:31 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:12:34 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:12:35 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f487d936910>
INFO 06-30 21:12:36 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:12:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:12:39 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-504...
WARNING 06-30 21:12:39 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:12:39 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:12:39 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.10s/it]

INFO 06-30 21:12:47 [loader.py:458] Loading weights took 8.31 seconds
INFO 06-30 21:12:47 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 8.538038 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:12:49 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:13:03 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/6426ab010e/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:13:03 [backends.py:426] Dynamo bytecode transform time: 7.75 s
INFO 06-30 21:13:07 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:13:32 [backends.py:144] Compiling a graph for general shape takes 28.72 s
INFO 06-30 21:13:46 [monitor.py:33] torch.compile takes 36.47 s in total
INFO 06-30 21:13:47 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:13:47 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:14:10 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:14:10 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.93 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:14:12 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:14:12] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 182.62 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 358.68 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 530.14 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 686.18 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 1116.66 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 992.10 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1044.59 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1313.33 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1315.61 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1299.71 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 913.36 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f4723d38ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:14:17.331418921 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-567
INFO 06-30 21:14:28 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:14:30,646 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:14:30,691 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:30,716 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:14:31,123 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:14:31,124 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:14:31,130 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:14:31,137 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,138 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,138 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,138 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,138 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,138 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,139 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:31,139 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:14:31,510 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:14:31,512 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:14:31,723 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:14:32,164 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:14:32,216 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:14:32,216 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:14:32,218 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:14:43 [config.py:689] This model supports multiple tasks: {'reward', 'embed', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 06-30 21:14:43 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:14:44,740 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:14:45,092 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:14:45,193 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:14:45,196 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:14:45 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:14:52 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:14:55 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:14:56 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4d14468bd0>
INFO 06-30 21:14:57 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:14:57 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:14:59 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-567...
WARNING 06-30 21:14:59 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:15:00 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:15:00 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.25s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.25s/it]

INFO 06-30 21:15:07 [loader.py:458] Loading weights took 7.46 seconds
INFO 06-30 21:15:07 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.688512 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:15:08 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:15:23 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/2b20d937a3/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:15:23 [backends.py:426] Dynamo bytecode transform time: 7.56 s
INFO 06-30 21:15:27 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:15:52 [backends.py:144] Compiling a graph for general shape takes 28.56 s
INFO 06-30 21:16:06 [monitor.py:33] torch.compile takes 36.12 s in total
INFO 06-30 21:16:06 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:16:06 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:16:30 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:16:30 [core.py:163] init engine (profile, create kv cache, warmup model) took 82.44 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:16:31 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:16:31] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:11, 178.98 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 349.60 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 515.22 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 671.56 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 924/2108 [00:01<00:01, 965.57 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 999.62 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 1014.60 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1053.04 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1353.67 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1346.51 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 1280.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 886.82 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f04a5434ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:16:36.556404340 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-63
INFO 06-30 21:16:47 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:16:49,346 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:16:49,418 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,443 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:16:49,829 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:16:49,831 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:16:49,837 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:16:49,843 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,844 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:16:49,845 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:16:50,216 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:16:50,219 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:16:50,415 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:16:50,856 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:16:50,911 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:16:50,912 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:16:50,914 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:17:02 [config.py:689] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 06-30 21:17:02 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:17:03,462 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:17:03,800 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:17:03,893 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:17:03,893 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:17:03 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:17:11 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:17:14 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:17:14 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa40013e050>
INFO 06-30 21:17:15 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:17:15 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:17:18 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-63...
WARNING 06-30 21:17:18 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:17:18 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:17:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.30s/it]

INFO 06-30 21:17:26 [loader.py:458] Loading weights took 7.51 seconds
INFO 06-30 21:17:26 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 7.725880 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:17:28 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:17:42 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/7837c635ca/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:17:42 [backends.py:426] Dynamo bytecode transform time: 7.43 s
INFO 06-30 21:17:46 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:18:11 [backends.py:144] Compiling a graph for general shape takes 28.58 s
INFO 06-30 21:18:25 [monitor.py:33] torch.compile takes 36.01 s in total
INFO 06-30 21:18:25 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:18:25 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:18:49 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:18:49 [core.py:163] init engine (profile, create kv cache, warmup model) took 83.07 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:18:50 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:18:50] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:10, 179.75 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 352.73 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 519.66 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 678.33 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 1082.32 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:00, 1083.31 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1320/2108 [00:01<00:00, 1158.92 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1584/2108 [00:01<00:00, 1201.05 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1846/2108 [00:02<00:00, 1178.99 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 1082.72 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 972.33 examples/s] Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 837.13 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7fad40184ea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:18:56.219414767 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Running inference on checkpoint: checkpoint-630
INFO 06-30 21:19:07 [__init__.py:239] Automatically detected platform cuda.
[INFO|training_args.py:2135] 2025-06-30 21:19:09,092 >> PyTorch: setting up devices
[INFO|training_args.py:1812] 2025-06-30 21:19:09,135 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,159 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:19:09,557 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-06-30 21:19:09,559 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 21:19:09,565 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 21:19:09,572 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:09,573 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:19:09,948 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:627] 2025-06-30 21:19:09,950 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 21:19:10,162 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:990] 2025-06-30 21:19:10,631 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2VLProcessor"
}

[INFO|configuration_utils.py:696] 2025-06-30 21:19:10,693 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/config.json
[INFO|configuration_utils.py:696] 2025-06-30 21:19:10,693 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/config.json
[INFO|configuration_utils.py:770] 2025-06-30 21:19:10,696 >> Model config Qwen2VLConfig {
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 1536,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 8960,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_vl_text",
    "num_attention_heads": 12,
    "num_hidden_layers": 28,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "tie_word_embeddings": true,
    "torch_dtype": "float32",
    "use_cache": false,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "hidden_act": "quick_gelu",
    "hidden_size": 1536,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "mlp_ratio": 4,
    "model_type": "qwen2_vl",
    "num_heads": 16,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "torch_dtype": "float32"
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

INFO 06-30 21:19:22 [config.py:689] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 06-30 21:19:22 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,236 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 21:19:23,237 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 21:19:23,568 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1088] 2025-06-30 21:19:23,669 >> loading configuration file saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 21:19:23,671 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

WARNING 06-30 21:19:23 [utils.py:2304] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 06-30 21:19:31 [__init__.py:239] Automatically detected platform cuda.
INFO 06-30 21:19:34 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630', speculative_config=None, tokenizer='saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-30 21:19:35 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9dfe94d390>
INFO 06-30 21:19:36 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-30 21:19:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:19:39 [gpu_model_runner.py:1276] Starting to load model saves/qwen2_vl-3b/vindr_sft_def/checkpoint-630...
WARNING 06-30 21:19:39 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 06-30 21:19:39 [config.py:3466] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
WARNING 06-30 21:19:39 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.62s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.63s/it]

INFO 06-30 21:19:48 [loader.py:458] Loading weights took 8.83 seconds
INFO 06-30 21:19:48 [gpu_model_runner.py:1291] Model loading took 4.1513 GiB and 9.111956 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:19:51 [gpu_model_runner.py:1560] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
INFO 06-30 21:20:06 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/e3bdf024ed/rank_0_0 for vLLM's torch.compile
INFO 06-30 21:20:06 [backends.py:426] Dynamo bytecode transform time: 7.69 s
INFO 06-30 21:20:11 [backends.py:132] Cache the graph of shape None for later use
INFO 06-30 21:20:36 [backends.py:144] Compiling a graph for general shape takes 29.21 s
INFO 06-30 21:20:50 [monitor.py:33] torch.compile takes 36.90 s in total
INFO 06-30 21:20:50 [kv_cache_utils.py:634] GPU KV cache size: 1,006,016 tokens
INFO 06-30 21:20:50 [kv_cache_utils.py:637] Maximum concurrency for 6,144 tokens per request: 163.74x
INFO 06-30 21:21:15 [gpu_model_runner.py:1626] Graph capturing finished in 24 secs, took 0.47 GiB
INFO 06-30 21:21:15 [core.py:163] init engine (profile, create kv cache, warmup model) took 86.65 seconds
Unused or unrecognized kwargs: return_tensors.
INFO 06-30 21:21:16 [core_client.py:435] Core engine process 0 ready.
[INFO|2025-06-30 21:21:16] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/vindr/qwen2_vindr_input_definition_test_len_2108.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/2108 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 132/2108 [00:00<00:11, 179.22 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 264/2108 [00:00<00:05, 352.90 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 396/2108 [00:00<00:03, 519.07 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 528/2108 [00:01<00:02, 678.82 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 792/2108 [00:01<00:01, 1077.13 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 1056/2108 [00:01<00:01, 913.77 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 1188/2108 [00:01<00:00, 979.01 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1452/2108 [00:01<00:00, 1245.43 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1715/2108 [00:02<00:00, 1108.46 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1977/2108 [00:02<00:00, 999.66 examples/s] Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 808.88 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 2108/2108 [00:02<00:00, 765.86 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151653, 198, 5598, 30618, 14697, 315, 364, 47, 811, 372, 8767, 269, 706, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 13874, 19324, 9112, 25, 393, 811, 372, 8767, 269, 706, 3363, 6553, 30591, 304, 279, 7100, 4176, 3550, 6825, 264, 12929, 476, 19265, 315, 20622, 19847, 13, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>
Return bounding boxes of 'Pneumothorax' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```

Note: Pneumothorax means Air trapped in the pleural space creating a gap or absence of lung tissue.<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 16, 22, 20, 345, 414, 220, 17, 22, 24, 345, 414, 220, 18, 15, 17, 345, 414, 220, 21, 23, 16, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 811, 372, 8767, 269, 706, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      175,
      279,
      302,
      681
    ],
    "label": "Pneumothorax"
  }
]
</answer><|im_end|>

Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]Processing batched inference:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 199, in <module>
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
  File "/home/june/Code/LLaMA-Factory-Latest/scripts/vllm_infer.py", line 145, in vllm_infer
  File "/home/june/Code/LLaMA-Factory-Latest/src/llamafactory/data/mm_plugin.py", line 255, in _regularize_images
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/PIL/Image.py", line 3505, in open
OSError: [Errno 24] Too many open files: '/home/june/datasets/vindr/images_512/images_512/287422bed1d9d153387361889619abed.png'
Exception ignored in atexit callback: <function shutdown at 0x7f5bdeaccea0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
  File "/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/_private/worker.py", line 1903, in shutdown
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
OSError: [Errno 24] Too many open files: '/root/miniconda3/envs/llamafactory/lib/python3.11/site-packages/ray/dag/__init__.py'
[rank0]:[W630 21:21:22.505853870 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
