Container llamafactory already exists.

=============
== PyTorch ==
=============

NVIDIA Release 24.07 (build 100464919)
PyTorch Version 2.4.0a0+3bcc3cd
Container image Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

ERROR: This container was built for NVIDIA Driver Release 555.42 or later, but
       version 535.247.01 was detected and compatibility mode is UNAVAILABLE.

       [[]]

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

Mon Jun 30 14:14:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.5     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:E3:00.0 Off |                    0 |
| N/A   40C    P0              64W / 500W |      0MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
==============================================
Starting evaluation for model: InternVL3-8B
Model path: /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf
Template: intern_vl
Batch size: 8
==============================================
[2025-06-30 14:16:17,153] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 06-30 14:16:40 [__init__.py:239] Automatically detected platform cuda.
[INFO|2025-06-30 14:16:54] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,345 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,348 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,348 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,349 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,349 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,349 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,349 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 14:16:54,798 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:928] 2025-06-30 14:16:54,802 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|image_processing_base.py:378] 2025-06-30 14:16:54,827 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-06-30 14:16:54,884 >> Image processor GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,887 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,887 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,887 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,887 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,888 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,888 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-30 14:16:54,888 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-30 14:16:55,330 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:328] 2025-06-30 14:16:55,346 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-06-30 14:16:55,346 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|configuration_utils.py:696] 2025-06-30 14:16:55,346 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-30 14:16:55,380 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|video_processing_utils.py:627] 2025-06-30 14:16:55,383 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-06-30 14:16:55,384 >> Video processor InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-06-30 14:16:55,385 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/processor_config.json
[INFO|processing_utils.py:990] 2025-06-30 14:16:55,920 >> Processor InternVLProcessor:
- image_processor: GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf', vocab_size=151643, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151666: AddedToken("</img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151667: AddedToken("<IMG_CONTEXT>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151668: AddedToken("<quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151669: AddedToken("</quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151670: AddedToken("<ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151671: AddedToken("</ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151672: AddedToken("<box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151673: AddedToken("</box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151674: AddedToken("<video>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: InternVLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device"
  ],
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  },
  "size_divisor": null,
  "video_processor_type": "InternVLVideoProcessor"
}


{
  "image_seq_length": 256,
  "processor_class": "InternVLProcessor"
}

[INFO|2025-06-30 14:16:55] llamafactory.data.template:143 >> Using default system message: You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image..
[INFO|2025-06-30 14:16:55] llamafactory.data.template:143 >> Add <|im_end|> to stop words.
[INFO|2025-06-30 14:16:55] llamafactory.data.loader:143 >> Loading dataset ./0_my_dataset/padchest_gt/padchest_input_data_qwen2_test_len_1285.json...
[INFO|configuration_utils.py:696] 2025-06-30 14:16:57,295 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/config.json
[INFO|configuration_utils.py:770] 2025-06-30 14:16:57,302 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "image_seq_length": 256,
  "image_token_id": 151667,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "torch_dtype": "bfloat16",
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

eval example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 11900, 16155, 11163, 311, 13382, 323, 23560, 6457, 2168, 6358, 13, 4615, 3476, 374, 311, 10542, 323, 94416, 11900, 5729, 5904, 315, 3151, 18808, 304, 15138, 1599, 29530, 5335, 13, 1446, 686, 387, 3897, 448, 264, 15138, 1599, 29530, 2168, 323, 264, 8457, 829, 13, 4615, 3383, 374, 311, 29257, 94416, 279, 9760, 5537, 1141, 8, 304, 279, 2168, 13, 151645, 198, 151644, 872, 198, 151665, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151667, 151666, 198, 5598, 30618, 14697, 315, 364, 47, 273, 4176, 81277, 6019, 6, 5671, 438, 4718, 3561, 510, 73594, 2236, 198, 9640, 4913, 58456, 62, 17, 67, 788, 508, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 1125, 330, 1502, 788, 330, 1502, 7115, 9338, 921, 73594, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system
You are an expert radiologist committed to accurate and precise medical image analysis. Your role is to identify and localize radiological evidence of specific diseases in chest X-ray images. You will be provided with a chest X-ray image and a disease name. Your task is to accurately localize the relevant region(s) in the image.<|im_end|>
<|im_start|>user
<img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>
Return bounding boxes of 'Pleural Thickening' areas as JSON format:
```json
[
{"bbox_2d": [x1, y1, x2, y2], "label": "label"},
...
]
```<|im_end|>
<|im_start|>assistant

label_ids:
[27, 9217, 397, 9640, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 17, 22, 21, 345, 414, 220, 16, 19, 16, 345, 414, 220, 19, 23, 19, 345, 414, 220, 17, 16, 23, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 1153, 220, 341, 262, 330, 58456, 62, 17, 67, 788, 2278, 414, 220, 20, 20, 17, 345, 414, 220, 16, 17, 22, 345, 414, 220, 22, 21, 22, 345, 414, 220, 17, 18, 15, 198, 262, 3211, 262, 330, 1502, 788, 330, 47, 273, 4176, 81277, 6019, 698, 220, 456, 921, 522, 9217, 29, 151645, 198]
labels:
<answer>
[
  {
    "bbox_2d": [
      276,
      141,
      484,
      218
    ],
    "label": "Pleural Thickening"
  },
  {
    "bbox_2d": [
      552,
      127,
      767,
      230
    ],
    "label": "Pleural Thickening"
  }
]
</answer><|im_end|>

[INFO|2025-06-30 14:16:57] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1148] 2025-06-30 14:16:58,415 >> loading weights file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-30 14:16:58,423 >> Instantiating InternVLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-30 14:16:58,429 >> Generate config GenerationConfig {}

[INFO|modeling_utils.py:2241] 2025-06-30 14:17:04,064 >> Instantiating InternVLVisionModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2241] 2025-06-30 14:17:04,240 >> Instantiating Qwen2Model model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]
[INFO|modeling_utils.py:5131] 2025-06-30 14:17:19,604 >> All model checkpoint weights were used when initializing InternVLForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-06-30 14:17:19,604 >> All the weights of InternVLForConditionalGeneration were initialized from the model checkpoint at /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-30 14:17:19,610 >> loading configuration file /home/june/Code/new_llamafactory/saves/huggingface_origin/InternVL3-8B-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-30 14:17:19,611 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|2025-06-30 14:17:19] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-30 14:17:19] llamafactory.model.loader:143 >> all params: 7,944,373,760
[WARNING|2025-06-30 14:17:19] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-06-30 14:17:19,747 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-06-30 14:17:19,747 >>   Num examples = 1285
[INFO|trainer.py:4332] 2025-06-30 14:17:19,747 >>   Batch size = 8
  0%|          | 0/161 [00:00<?, ?it/s]  1%|          | 2/161 [00:06<08:27,  3.19s/it]  2%|▏         | 3/161 [00:11<10:46,  4.09s/it]  2%|▏         | 4/161 [00:17<12:09,  4.64s/it]  3%|▎         | 5/161 [00:26<16:00,  6.16s/it]  4%|▎         | 6/161 [00:36<19:02,  7.37s/it]  4%|▍         | 7/161 [00:41<17:39,  6.88s/it]  5%|▍         | 8/161 [00:48<17:33,  6.89s/it]  6%|▌         | 9/161 [00:57<18:46,  7.41s/it]  6%|▌         | 10/161 [01:03<17:40,  7.02s/it]  7%|▋         | 11/161 [01:09<16:44,  6.69s/it]  7%|▋         | 12/161 [01:15<16:16,  6.55s/it]  8%|▊         | 13/161 [01:24<17:59,  7.30s/it]  9%|▊         | 14/161 [01:31<17:38,  7.20s/it]  9%|▉         | 15/161 [01:40<18:37,  7.65s/it] 10%|▉         | 16/161 [01:49<19:20,  8.01s/it]slurmstepd: error: *** JOB 5203157 ON mcml-hgx-a100-009 CANCELLED AT 2025-06-30T14:19:29 ***
